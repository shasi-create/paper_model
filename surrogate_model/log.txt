D:\envs\pytorch\python.exe C:\Users\admin\Desktop\250206\opt_train.py 
Using device:  cuda
Train dataset size: 291
Valid dataset size: 36
Test dataset size: 37
Processing params file: C:/Users/admin/Desktop/241220_paper/outs/05mech_norm_input/norm_mech_02_params.json
Starting training! -- >>
Training -->> Epoch: 1,(no reg loss)standard loss: 0.4704410, inverse loss: 0.0981991
Valid-->> Epoch [1/3000], Standardized Loss: 0.6252303, Inverse Loss: 0.1305096
Valid-->> Lowest loss found at epoch 1, loss: 0.1305096
Epoch 1, Masked params (inverse standardized): tensor([ 31.996437,  44.320480,  -0.209385,  25.912006,  27.278938,  17.524736,
         21.136600,   1.360857, 106.614632,  27.108236,  25.745037,  44.334572,
         24.113203,  14.754033,  84.635368,  28.006075,   5.389900,  29.509096,
         11.687448,  31.743217], device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 2,(no reg loss)standard loss: 0.4055299, inverse loss: 0.0846497
Valid-->> Epoch [2/3000], Standardized Loss: 0.5381823, Inverse Loss: 0.1123393
Valid-->> Lowest loss found at epoch 2, loss: 0.1123393
Epoch 2, Masked params (inverse standardized): tensor([ 32.030327,  44.316273,  -0.173971,  25.944880,  27.311844,  17.559315,
         21.170702,   1.396223, 106.614594,  27.141077,  25.777975,  44.369019,
         24.146671,  14.788807,  84.630554,  28.039196,   5.425241,  29.542603,
         11.722403,  31.777092], device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 3,(no reg loss)standard loss: 0.3422508, inverse loss: 0.0714409
Valid-->> Epoch [3/3000], Standardized Loss: 0.4573235, Inverse Loss: 0.0954610
Valid-->> Lowest loss found at epoch 3, loss: 0.0954610
Epoch 3, Masked params (inverse standardized): tensor([ 32.063145,  44.315506,  -0.140333,  25.977192,  27.344160,  17.592571,
         21.203728,   1.429855, 106.613365,  27.173353,  25.810324,  44.401909,
         24.179338,  14.822181,  84.630066,  28.071625,   5.458803,  29.575241,
         11.755842,  31.809904], device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 4,(no reg loss)standard loss: 0.2847142, inverse loss: 0.0594308
Valid-->> Epoch [4/3000], Standardized Loss: 0.3806762, Inverse Loss: 0.0794618
Valid-->> Lowest loss found at epoch 4, loss: 0.0794618
Epoch 4, Masked params (inverse standardized): tensor([ 32.095425,  44.315277,  -0.108311,  26.009680,  27.376621,  17.624926,
         21.236202,   1.461920, 106.612656,  27.205809,  25.842815,  44.433495,
         24.211857,  14.854520,  84.629364,  28.104065,   5.490704,  29.607639,
         11.788066,  31.842190], device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 5,(no reg loss)standard loss: 0.2342645, inverse loss: 0.0489000
Valid-->> Epoch [5/3000], Standardized Loss: 0.3124510, Inverse Loss: 0.0652205
Valid-->> Lowest loss found at epoch 5, loss: 0.0652205
Epoch 5, Masked params (inverse standardized): tensor([ 3.212563e+01,  4.431350e+01, -7.880020e-02,  2.604038e+01,
         2.740728e+01,  1.765507e+01,  2.126661e+01,  1.491507e+00,
         1.066140e+02,  2.723647e+01,  2.587350e+01,  4.446277e+01,
         2.424245e+01,  1.488461e+01,  8.462780e+01,  2.813464e+01,
         5.520082e+00,  2.963809e+01,  1.181795e+01,  3.187243e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 6,(no reg loss)standard loss: 0.1865728, inverse loss: 0.0389449
Valid-->> Epoch [6/3000], Standardized Loss: 0.2505083, Inverse Loss: 0.0522907
Valid-->> Lowest loss found at epoch 6, loss: 0.0522907
Epoch 6, Masked params (inverse standardized): tensor([ 3.215459e+01,  4.431409e+01, -5.134392e-02,  2.607031e+01,
         2.743714e+01,  1.768371e+01,  2.129575e+01,  1.519106e+00,
         1.066135e+02,  2.726637e+01,  2.590339e+01,  4.449026e+01,
         2.427206e+01,  1.491310e+01,  8.462804e+01,  2.816434e+01,
         5.547373e+00,  2.966748e+01,  1.184610e+01,  3.190142e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 7,(no reg loss)standard loss: 0.1474284, inverse loss: 0.0307740
Valid-->> Epoch [7/3000], Standardized Loss: 0.2013530, Inverse Loss: 0.0420301
Valid-->> Lowest loss found at epoch 7, loss: 0.0420301
Epoch 7, Masked params (inverse standardized): tensor([ 3.217945e+01,  4.431182e+01, -2.761650e-02,  2.609599e+01,
         2.746276e+01,  1.770834e+01,  2.132078e+01,  1.542936e+00,
         1.066133e+02,  2.729201e+01,  2.592904e+01,  4.451394e+01,
         2.429747e+01,  1.493760e+01,  8.462618e+01,  2.818981e+01,
         5.570993e+00,  2.969269e+01,  1.187034e+01,  3.192631e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 8,(no reg loss)standard loss: 0.1163181, inverse loss: 0.0242801
Valid-->> Epoch [8/3000], Standardized Loss: 0.1589719, Inverse Loss: 0.0331835
Valid-->> Lowest loss found at epoch 8, loss: 0.0331835
Epoch 8, Masked params (inverse standardized): tensor([ 3.220254e+01,  4.431110e+01, -6.322861e-03,  2.612037e+01,
         2.748707e+01,  1.773096e+01,  2.134402e+01,  1.564381e+00,
         1.066133e+02,  2.731635e+01,  2.595337e+01,  4.453539e+01,
         2.432136e+01,  1.496000e+01,  8.462524e+01,  2.821386e+01,
         5.592163e+00,  2.971633e+01,  1.189239e+01,  3.194944e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 9,(no reg loss)standard loss: 0.0908419, inverse loss: 0.0189622
Valid-->> Epoch [9/3000], Standardized Loss: 0.1236727, Inverse Loss: 0.0258152
Valid-->> Lowest loss found at epoch 9, loss: 0.0258152
Epoch 9, Masked params (inverse standardized): tensor([3.222360e+01, 4.430998e+01, 1.243973e-02, 2.614314e+01, 2.750974e+01,
        1.775136e+01, 2.136524e+01, 1.583292e+00, 1.066131e+02, 2.733909e+01,
        2.597606e+01, 4.455447e+01, 2.434345e+01, 1.498011e+01, 8.462408e+01,
        2.823622e+01, 5.610777e+00, 2.973811e+01, 1.191206e+01, 3.197054e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 10,(no reg loss)standard loss: 0.0691764, inverse loss: 0.0144398
Valid-->> Epoch [10/3000], Standardized Loss: 0.0959382, Inverse Loss: 0.0200260
Valid-->> Lowest loss found at epoch 10, loss: 0.0200260
Epoch 10, Masked params (inverse standardized): tensor([3.224203e+01, 4.430857e+01, 2.837181e-02, 2.616354e+01, 2.753003e+01,
        1.776903e+01, 2.138380e+01, 1.599384e+00, 1.066124e+02, 2.735946e+01,
        2.599638e+01, 4.457082e+01, 2.436304e+01, 1.499745e+01, 8.462299e+01,
        2.825613e+01, 5.626604e+00, 2.975736e+01, 1.192896e+01, 3.198904e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 11,(no reg loss)standard loss: 0.0513514, inverse loss: 0.0107190
Valid-->> Epoch [11/3000], Standardized Loss: 0.0716639, Inverse Loss: 0.0149590
Valid-->> Lowest loss found at epoch 11, loss: 0.0149590
Epoch 11, Masked params (inverse standardized): tensor([3.226023e+01, 4.430832e+01, 4.298592e-02, 2.618448e+01, 2.755082e+01,
        1.778609e+01, 2.140213e+01, 1.614155e+00, 1.066127e+02, 2.738036e+01,
        2.601719e+01, 4.458613e+01, 2.438282e+01, 1.501403e+01, 8.462212e+01,
        2.827641e+01, 5.640974e+00, 2.977670e+01, 1.194492e+01, 3.200732e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 12,(no reg loss)standard loss: 0.0385759, inverse loss: 0.0080523
Valid-->> Epoch [12/3000], Standardized Loss: 0.0551823, Inverse Loss: 0.0115187
Valid-->> Lowest loss found at epoch 12, loss: 0.0115187
Epoch 12, Masked params (inverse standardized): tensor([3.227451e+01, 4.430605e+01, 5.415726e-02, 2.620147e+01, 2.756766e+01,
        1.779930e+01, 2.141651e+01, 1.625504e+00, 1.066124e+02, 2.739733e+01,
        2.603405e+01, 4.459790e+01, 2.439863e+01, 1.502684e+01, 8.462058e+01,
        2.829271e+01, 5.652065e+00, 2.979208e+01, 1.195718e+01, 3.202168e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 13,(no reg loss)standard loss: 0.0290919, inverse loss: 0.0060726
Valid-->> Epoch [13/3000], Standardized Loss: 0.0440449, Inverse Loss: 0.0091939
Valid-->> Lowest loss found at epoch 13, loss: 0.0091939
Epoch 13, Masked params (inverse standardized): tensor([3.228550e+01, 4.430635e+01, 6.247902e-02, 2.621521e+01, 2.758124e+01,
        1.780930e+01, 2.142760e+01, 1.633945e+00, 1.066124e+02, 2.741104e+01,
        2.604766e+01, 4.460667e+01, 2.441113e+01, 1.503643e+01, 8.462070e+01,
        2.830574e+01, 5.660303e+00, 2.980415e+01, 1.196634e+01, 3.203275e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 14,(no reg loss)standard loss: 0.0223180, inverse loss: 0.0046586
Valid-->> Epoch [14/3000], Standardized Loss: 0.0360552, Inverse Loss: 0.0075261
Valid-->> Lowest loss found at epoch 14, loss: 0.0075261
Epoch 14, Masked params (inverse standardized): tensor([3.229436e+01, 4.430524e+01, 6.882095e-02, 2.622700e+01, 2.759287e+01,
        1.781715e+01, 2.143654e+01, 1.640343e+00, 1.066126e+02, 2.742281e+01,
        2.605931e+01, 4.461340e+01, 2.442159e+01, 1.504387e+01, 8.461969e+01,
        2.831679e+01, 5.666513e+00, 2.981414e+01, 1.197340e+01, 3.204169e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 15,(no reg loss)standard loss: 0.0173012, inverse loss: 0.0036114
Valid-->> Epoch [15/3000], Standardized Loss: 0.0298595, Inverse Loss: 0.0062328
Valid-->> Lowest loss found at epoch 15, loss: 0.0062328
Epoch 15, Masked params (inverse standardized): tensor([3.230213e+01, 4.430473e+01, 7.386971e-02, 2.623805e+01, 2.760374e+01,
        1.782380e+01, 2.144437e+01, 1.645468e+00, 1.066126e+02, 2.743384e+01,
        2.607019e+01, 4.461890e+01, 2.443110e+01, 1.505008e+01, 8.461908e+01,
        2.832697e+01, 5.671442e+00, 2.982315e+01, 1.197920e+01, 3.204952e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 16,(no reg loss)standard loss: 0.0133478, inverse loss: 0.0027862
Valid-->> Epoch [16/3000], Standardized Loss: 0.0246696, Inverse Loss: 0.0051495
Valid-->> Lowest loss found at epoch 16, loss: 0.0051495
Epoch 16, Masked params (inverse standardized): tensor([3.230979e+01, 4.430370e+01, 7.831001e-02, 2.624952e+01, 2.761500e+01,
        1.783014e+01, 2.145209e+01, 1.649981e+00, 1.066130e+02, 2.744530e+01,
        2.608147e+01, 4.462395e+01, 2.444078e+01, 1.505591e+01, 8.461797e+01,
        2.833745e+01, 5.675718e+00, 2.983225e+01, 1.198456e+01, 3.205727e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 17,(no reg loss)standard loss: 0.0105133, inverse loss: 0.0021945
Valid-->> Epoch [17/3000], Standardized Loss: 0.0218481, Inverse Loss: 0.0045605
Valid-->> Lowest loss found at epoch 17, loss: 0.0045605
Epoch 17, Masked params (inverse standardized): tensor([3.231414e+01, 4.430334e+01, 8.045006e-02, 2.625748e+01, 2.762275e+01,
        1.783342e+01, 2.145648e+01, 1.652136e+00, 1.066124e+02, 2.745325e+01,
        2.608924e+01, 4.462639e+01, 2.444697e+01, 1.505881e+01, 8.461793e+01,
        2.834442e+01, 5.677780e+00, 2.983790e+01, 1.198715e+01, 3.206171e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 18,(no reg loss)standard loss: 0.0087575, inverse loss: 0.0018280
Valid-->> Epoch [18/3000], Standardized Loss: 0.0200370, Inverse Loss: 0.0041825
Valid-->> Lowest loss found at epoch 18, loss: 0.0041825
Epoch 18, Masked params (inverse standardized): tensor([3.231683e+01, 4.430312e+01, 8.141136e-02, 2.626381e+01, 2.762886e+01,
        1.783515e+01, 2.145921e+01, 1.653078e+00, 1.066117e+02, 2.745958e+01,
        2.609537e+01, 4.462751e+01, 2.445146e+01, 1.506021e+01, 8.461757e+01,
        2.834973e+01, 5.678692e+00, 2.984186e+01, 1.198835e+01, 3.206448e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 19,(no reg loss)standard loss: 0.0075764, inverse loss: 0.0015815
Valid-->> Epoch [19/3000], Standardized Loss: 0.0187171, Inverse Loss: 0.0039070
Valid-->> Lowest loss found at epoch 19, loss: 0.0039070
Epoch 19, Masked params (inverse standardized): tensor([3.231873e+01, 4.430302e+01, 8.179474e-02, 2.626943e+01, 2.763425e+01,
        1.783613e+01, 2.146112e+01, 1.653444e+00, 1.066113e+02, 2.746519e+01,
        2.610077e+01, 4.462803e+01, 2.445515e+01, 1.506091e+01, 8.461740e+01,
        2.835426e+01, 5.679049e+00, 2.984500e+01, 1.198887e+01, 3.206646e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 20,(no reg loss)standard loss: 0.0066908, inverse loss: 0.0013966
Valid-->> Epoch [20/3000], Standardized Loss: 0.0177863, Inverse Loss: 0.0037127
Valid-->> Lowest loss found at epoch 20, loss: 0.0037127
Epoch 20, Masked params (inverse standardized): tensor([3.231982e+01, 4.430241e+01, 8.168983e-02, 2.627429e+01, 2.763887e+01,
        1.783640e+01, 2.146223e+01, 1.653309e+00, 1.066114e+02, 2.747006e+01,
        2.610541e+01, 4.462799e+01, 2.445801e+01, 1.506094e+01, 8.461696e+01,
        2.835799e+01, 5.678936e+00, 2.984731e+01, 1.198879e+01, 3.206762e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 21,(no reg loss)standard loss: 0.0060542, inverse loss: 0.0012637
Valid-->> Epoch [21/3000], Standardized Loss: 0.0171297, Inverse Loss: 0.0035756
Valid-->> Lowest loss found at epoch 21, loss: 0.0035756
Epoch 21, Masked params (inverse standardized): tensor([3.232026e+01, 4.430246e+01, 8.124161e-02, 2.627850e+01, 2.764283e+01,
        1.783611e+01, 2.146268e+01, 1.652798e+00, 1.066124e+02, 2.747427e+01,
        2.610938e+01, 4.462753e+01, 2.446015e+01, 1.506045e+01, 8.461688e+01,
        2.836103e+01, 5.678490e+00, 2.984891e+01, 1.198825e+01, 3.206813e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 22,(no reg loss)standard loss: 0.0056017, inverse loss: 0.0011693
Valid-->> Epoch [22/3000], Standardized Loss: 0.0166986, Inverse Loss: 0.0034856
Valid-->> Lowest loss found at epoch 22, loss: 0.0034856
Epoch 22, Masked params (inverse standardized): tensor([3.231998e+01, 4.430231e+01, 8.045387e-02, 2.628193e+01, 2.764600e+01,
        1.783523e+01, 2.146240e+01, 1.651951e+00, 1.066124e+02, 2.747771e+01,
        2.611257e+01, 4.462667e+01, 2.446148e+01, 1.505943e+01, 8.461685e+01,
        2.836326e+01, 5.677734e+00, 2.984972e+01, 1.198724e+01, 3.206791e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 23,(no reg loss)standard loss: 0.0052840, inverse loss: 0.0011030
Valid-->> Epoch [23/3000], Standardized Loss: 0.0163931, Inverse Loss: 0.0034219
Valid-->> Lowest loss found at epoch 23, loss: 0.0034219
Epoch 23, Masked params (inverse standardized): tensor([3.231928e+01, 4.430187e+01, 7.952881e-02, 2.628489e+01, 2.764869e+01,
        1.783403e+01, 2.146169e+01, 1.650969e+00, 1.066131e+02, 2.748067e+01,
        2.611527e+01, 4.462564e+01, 2.446230e+01, 1.505816e+01, 8.461641e+01,
        2.836498e+01, 5.676855e+00, 2.985003e+01, 1.198602e+01, 3.206726e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 24,(no reg loss)standard loss: 0.0050389, inverse loss: 0.0010518
Valid-->> Epoch [24/3000], Standardized Loss: 0.0161475, Inverse Loss: 0.0033706
Valid-->> Lowest loss found at epoch 24, loss: 0.0033706
Epoch 24, Masked params (inverse standardized): tensor([3.231845e+01, 4.430212e+01, 7.866669e-02, 2.628769e+01, 2.765122e+01,
        1.783280e+01, 2.146085e+01, 1.650032e+00, 1.066128e+02, 2.748348e+01,
        2.611781e+01, 4.462463e+01, 2.446292e+01, 1.505687e+01, 8.461649e+01,
        2.836651e+01, 5.676035e+00, 2.985015e+01, 1.198482e+01, 3.206648e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 25,(no reg loss)standard loss: 0.0048344, inverse loss: 0.0010091
Valid-->> Epoch [25/3000], Standardized Loss: 0.0159401, Inverse Loss: 0.0033273
Valid-->> Lowest loss found at epoch 25, loss: 0.0033273
Epoch 25, Masked params (inverse standardized): tensor([3.231761e+01, 4.430170e+01, 7.789040e-02, 2.629045e+01, 2.765370e+01,
        1.783162e+01, 2.145999e+01, 1.649210e+00, 1.066118e+02, 2.748625e+01,
        2.612029e+01, 4.462374e+01, 2.446346e+01, 1.505568e+01, 8.461627e+01,
        2.836797e+01, 5.675331e+00, 2.985019e+01, 1.198372e+01, 3.206568e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 26,(no reg loss)standard loss: 0.0046950, inverse loss: 0.0009800
Valid-->> Epoch [26/3000], Standardized Loss: 0.0158367, Inverse Loss: 0.0033057
Valid-->> Lowest loss found at epoch 26, loss: 0.0033057
Epoch 26, Masked params (inverse standardized): tensor([3.231612e+01, 4.430219e+01, 7.687759e-02, 2.629238e+01, 2.765533e+01,
        1.782995e+01, 2.145848e+01, 1.648121e+00, 1.066120e+02, 2.748819e+01,
        2.612194e+01, 4.462252e+01, 2.446318e+01, 1.505405e+01, 8.461673e+01,
        2.836858e+01, 5.674379e+00, 2.984946e+01, 1.198226e+01, 3.206422e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 27,(no reg loss)standard loss: 0.0046081, inverse loss: 0.0009619
Valid-->> Epoch [27/3000], Standardized Loss: 0.0157515, Inverse Loss: 0.0032879
Valid-->> Lowest loss found at epoch 27, loss: 0.0032879
Epoch 27, Masked params (inverse standardized): tensor([3.231462e+01, 4.430148e+01, 7.597923e-02, 2.629419e+01, 2.765685e+01,
        1.782835e+01, 2.145696e+01, 1.647148e+00, 1.066126e+02, 2.749002e+01,
        2.612346e+01, 4.462142e+01, 2.446278e+01, 1.505251e+01, 8.461607e+01,
        2.836907e+01, 5.673534e+00, 2.984863e+01, 1.198091e+01, 3.206274e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 28,(no reg loss)standard loss: 0.0045513, inverse loss: 0.0009500
Valid-->> Epoch [28/3000], Standardized Loss: 0.0157102, Inverse Loss: 0.0032793
Valid-->> Lowest loss found at epoch 28, loss: 0.0032793
Epoch 28, Masked params (inverse standardized): tensor([3.231281e+01, 4.430199e+01, 7.501030e-02, 2.629552e+01, 2.765788e+01,
        1.782656e+01, 2.145511e+01, 1.646112e+00, 1.066119e+02, 2.749135e+01,
        2.612449e+01, 4.462023e+01, 2.446191e+01, 1.505084e+01, 8.461652e+01,
        2.836906e+01, 5.672663e+00, 2.984737e+01, 1.197945e+01, 3.206094e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 29,(no reg loss)standard loss: 0.0045086, inverse loss: 0.0009411
Valid-->> Epoch [29/3000], Standardized Loss: 0.0156683, Inverse Loss: 0.0032706
Valid-->> Lowest loss found at epoch 29, loss: 0.0032706
Epoch 29, Masked params (inverse standardized): tensor([3.231112e+01, 4.430144e+01, 7.423210e-02, 2.629686e+01, 2.765892e+01,
        1.782497e+01, 2.145340e+01, 1.645269e+00, 1.066122e+02, 2.749271e+01,
        2.612554e+01, 4.461926e+01, 2.446108e+01, 1.504938e+01, 8.461603e+01,
        2.836906e+01, 5.671963e+00, 2.984616e+01, 1.197820e+01, 3.205927e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 30,(no reg loss)standard loss: 0.0044764, inverse loss: 0.0009344
Valid-->> Epoch [30/3000], Standardized Loss: 0.0156394, Inverse Loss: 0.0032645
Valid-->> Lowest loss found at epoch 30, loss: 0.0032645
Epoch 30, Masked params (inverse standardized): tensor([3.230942e+01, 4.430182e+01, 7.354164e-02, 2.629803e+01, 2.765978e+01,
        1.782344e+01, 2.145166e+01, 1.644506e+00, 1.066121e+02, 2.749390e+01,
        2.612640e+01, 4.461835e+01, 2.446009e+01, 1.504800e+01, 8.461637e+01,
        2.836889e+01, 5.671322e+00, 2.984484e+01, 1.197704e+01, 3.205756e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 31,(no reg loss)standard loss: 0.0044554, inverse loss: 0.0009300
Valid-->> Epoch [31/3000], Standardized Loss: 0.0156223, Inverse Loss: 0.0032610
Valid-->> Lowest loss found at epoch 31, loss: 0.0032610
Epoch 31, Masked params (inverse standardized): tensor([3.230769e+01, 4.430199e+01, 7.289505e-02, 2.629901e+01, 2.766045e+01,
        1.782195e+01, 2.144991e+01, 1.643810e+00, 1.066125e+02, 2.749490e+01,
        2.612707e+01, 4.461752e+01, 2.445895e+01, 1.504668e+01, 8.461654e+01,
        2.836852e+01, 5.670742e+00, 2.984339e+01, 1.197596e+01, 3.205583e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 32,(no reg loss)standard loss: 0.0044343, inverse loss: 0.0009256
Valid-->> Epoch [32/3000], Standardized Loss: 0.0155988, Inverse Loss: 0.0032561
Valid-->> Lowest loss found at epoch 32, loss: 0.0032561
Epoch 32, Masked params (inverse standardized): tensor([3.230617e+01, 4.430166e+01, 7.242203e-02, 2.630009e+01, 2.766121e+01,
        1.782071e+01, 2.144835e+01, 1.643295e+00, 1.066123e+02, 2.749599e+01,
        2.612783e+01, 4.461689e+01, 2.445792e+01, 1.504560e+01, 8.461631e+01,
        2.836825e+01, 5.670334e+00, 2.984208e+01, 1.197509e+01, 3.205431e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 33,(no reg loss)standard loss: 0.0044055, inverse loss: 0.0009196
Valid-->> Epoch [33/3000], Standardized Loss: 0.0155723, Inverse Loss: 0.0032505
Valid-->> Lowest loss found at epoch 33, loss: 0.0032505
Epoch 33, Masked params (inverse standardized): tensor([3.230481e+01, 4.430154e+01, 7.206345e-02, 2.630122e+01, 2.766201e+01,
        1.781964e+01, 2.144695e+01, 1.642899e+00, 1.066124e+02, 2.749714e+01,
        2.612864e+01, 4.461641e+01, 2.445697e+01, 1.504470e+01, 8.461591e+01,
        2.836802e+01, 5.670040e+00, 2.984087e+01, 1.197440e+01, 3.205294e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 34,(no reg loss)standard loss: 0.0043943, inverse loss: 0.0009173
Valid-->> Epoch [34/3000], Standardized Loss: 0.0155712, Inverse Loss: 0.0032503
Valid-->> Lowest loss found at epoch 34, loss: 0.0032503
Epoch 34, Masked params (inverse standardized): tensor([3.230320e+01, 4.430171e+01, 7.159424e-02, 2.630189e+01, 2.766237e+01,
        1.781841e+01, 2.144531e+01, 1.642380e+00, 1.066123e+02, 2.749784e+01,
        2.612900e+01, 4.461578e+01, 2.445563e+01, 1.504364e+01, 8.461639e+01,
        2.836736e+01, 5.669598e+00, 2.983931e+01, 1.197356e+01, 3.205131e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 35,(no reg loss)standard loss: 0.0043944, inverse loss: 0.0009173
Valid-->> Epoch [35/3000], Standardized Loss: 0.0155612, Inverse Loss: 0.0032482
Valid-->> Lowest loss found at epoch 35, loss: 0.0032482
Epoch 35, Masked params (inverse standardized): tensor([3.230184e+01, 4.430178e+01, 7.127571e-02, 2.630271e+01, 2.766286e+01,
        1.781742e+01, 2.144391e+01, 1.642035e+00, 1.066121e+02, 2.749867e+01,
        2.612949e+01, 4.461537e+01, 2.445445e+01, 1.504283e+01, 8.461630e+01,
        2.836684e+01, 5.669336e+00, 2.983793e+01, 1.197294e+01, 3.204993e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 36,(no reg loss)standard loss: 0.0043877, inverse loss: 0.0009159
Valid-->> Epoch [36/3000], Standardized Loss: 0.0155591, Inverse Loss: 0.0032478
Valid-->> Lowest loss found at epoch 36, loss: 0.0032478
Epoch 36, Masked params (inverse standardized): tensor([3.230046e+01, 4.430202e+01, 7.095528e-02, 2.630336e+01, 2.766319e+01,
        1.781646e+01, 2.144250e+01, 1.641687e+00, 1.066119e+02, 2.749934e+01,
        2.612981e+01, 4.461495e+01, 2.445316e+01, 1.504204e+01, 8.461656e+01,
        2.836617e+01, 5.669075e+00, 2.983648e+01, 1.197233e+01, 3.204853e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 37,(no reg loss)standard loss: 0.0043639, inverse loss: 0.0009109
Valid-->> Epoch [37/3000], Standardized Loss: 0.0155283, Inverse Loss: 0.0032414
Valid-->> Lowest loss found at epoch 37, loss: 0.0032414
Epoch 37, Masked params (inverse standardized): tensor([3.229963e+01, 4.430122e+01, 7.093430e-02, 2.630452e+01, 2.766402e+01,
        1.781598e+01, 2.144163e+01, 1.641640e+00, 1.066120e+02, 2.750053e+01,
        2.613065e+01, 4.461490e+01, 2.445239e+01, 1.504170e+01, 8.461594e+01,
        2.836600e+01, 5.669060e+00, 2.983556e+01, 1.197213e+01, 3.204767e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 38,(no reg loss)standard loss: 0.0043405, inverse loss: 0.0009060
Valid-->> Epoch [38/3000], Standardized Loss: 0.0155141, Inverse Loss: 0.0032384
Valid-->> Lowest loss found at epoch 38, loss: 0.0032384
Epoch 38, Masked params (inverse standardized): tensor([3.229863e+01, 4.430143e+01, 7.077217e-02, 2.630539e+01, 2.766455e+01,
        1.781535e+01, 2.144059e+01, 1.641466e+00, 1.066119e+02, 2.750142e+01,
        2.613118e+01, 4.461471e+01, 2.445137e+01, 1.504123e+01, 8.461572e+01,
        2.836554e+01, 5.668955e+00, 2.983442e+01, 1.197179e+01, 3.204666e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 39,(no reg loss)standard loss: 0.0043392, inverse loss: 0.0009058
Valid-->> Epoch [39/3000], Standardized Loss: 0.0155193, Inverse Loss: 0.0032395
Training -->> Epoch: 40,(no reg loss)standard loss: 0.0043463, inverse loss: 0.0009072
Valid-->> Epoch [40/3000], Standardized Loss: 0.0155201, Inverse Loss: 0.0032396
Training -->> Epoch: 41,(no reg loss)standard loss: 0.0043513, inverse loss: 0.0009083
Valid-->> Epoch [41/3000], Standardized Loss: 0.0155207, Inverse Loss: 0.0032398
Training -->> Epoch: 42,(no reg loss)standard loss: 0.0043260, inverse loss: 0.0009030
Valid-->> Epoch [42/3000], Standardized Loss: 0.0154866, Inverse Loss: 0.0032326
Valid-->> Lowest loss found at epoch 42, loss: 0.0032326
Epoch 42, Masked params (inverse standardized): tensor([3.229510e+01, 4.430109e+01, 7.026291e-02, 2.630808e+01, 2.766591e+01,
        1.781334e+01, 2.143693e+01, 1.640907e+00, 1.066120e+02, 2.750421e+01,
        2.613253e+01, 4.461415e+01, 2.444707e+01, 1.503971e+01, 8.461574e+01,
        2.836312e+01, 5.668526e+00, 2.982988e+01, 1.197073e+01, 3.204301e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 43,(no reg loss)standard loss: 0.0043027, inverse loss: 0.0008981
Valid-->> Epoch [43/3000], Standardized Loss: 0.0154785, Inverse Loss: 0.0032310
Valid-->> Lowest loss found at epoch 43, loss: 0.0032310
Epoch 43, Masked params (inverse standardized): tensor([3.229442e+01, 4.430159e+01, 7.015610e-02, 2.630876e+01, 2.766625e+01,
        1.781298e+01, 2.143621e+01, 1.640795e+00, 1.066121e+02, 2.750491e+01,
        2.613286e+01, 4.461406e+01, 2.444609e+01, 1.503945e+01, 8.461594e+01,
        2.836255e+01, 5.668434e+00, 2.982889e+01, 1.197054e+01, 3.204231e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 44,(no reg loss)standard loss: 0.0043050, inverse loss: 0.0008986
Valid-->> Epoch [44/3000], Standardized Loss: 0.0154797, Inverse Loss: 0.0032312
Training -->> Epoch: 45,(no reg loss)standard loss: 0.0043137, inverse loss: 0.0009004
Valid-->> Epoch [45/3000], Standardized Loss: 0.0154862, Inverse Loss: 0.0032326
Training -->> Epoch: 46,(no reg loss)standard loss: 0.0043147, inverse loss: 0.0009006
Valid-->> Epoch [46/3000], Standardized Loss: 0.0154839, Inverse Loss: 0.0032321
Training -->> Epoch: 47,(no reg loss)standard loss: 0.0043178, inverse loss: 0.0009013
Valid-->> Epoch [47/3000], Standardized Loss: 0.0154870, Inverse Loss: 0.0032327
Training -->> Epoch: 48,(no reg loss)standard loss: 0.0043228, inverse loss: 0.0009023
Valid-->> Epoch [48/3000], Standardized Loss: 0.0154896, Inverse Loss: 0.0032333
Training -->> Epoch: 49,(no reg loss)standard loss: 0.0043138, inverse loss: 0.0009005
Valid-->> Epoch [49/3000], Standardized Loss: 0.0154667, Inverse Loss: 0.0032285
Valid-->> Lowest loss found at epoch 49, loss: 0.0032285
Epoch 49, Masked params (inverse standardized): tensor([3.229106e+01, 4.430107e+01, 6.964684e-02, 2.631160e+01, 2.766710e+01,
        1.781133e+01, 2.143266e+01, 1.640263e+00, 1.066119e+02, 2.750793e+01,
        2.613367e+01, 4.461365e+01, 2.444027e+01, 1.503823e+01, 8.461554e+01,
        2.835839e+01, 5.668015e+00, 2.982335e+01, 1.196967e+01, 3.203877e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 50,(no reg loss)standard loss: 0.0043126, inverse loss: 0.0009002
Valid-->> Epoch [50/3000], Standardized Loss: 0.0154821, Inverse Loss: 0.0032317
Training -->> Epoch: 51,(no reg loss)standard loss: 0.0043180, inverse loss: 0.0009013
Valid-->> Epoch [51/3000], Standardized Loss: 0.0154786, Inverse Loss: 0.0032310
Training -->> Epoch: 52,(no reg loss)standard loss: 0.0043261, inverse loss: 0.0009030
Valid-->> Epoch [52/3000], Standardized Loss: 0.0154845, Inverse Loss: 0.0032322
Training -->> Epoch: 53,(no reg loss)standard loss: 0.0043182, inverse loss: 0.0009014
Valid-->> Epoch [53/3000], Standardized Loss: 0.0154702, Inverse Loss: 0.0032292
Training -->> Epoch: 54,(no reg loss)standard loss: 0.0043010, inverse loss: 0.0008978
Valid-->> Epoch [54/3000], Standardized Loss: 0.0154571, Inverse Loss: 0.0032265
Valid-->> Lowest loss found at epoch 54, loss: 0.0032265
Epoch 54, Masked params (inverse standardized): tensor([3.228954e+01, 4.430138e+01, 6.943512e-02, 2.631356e+01, 2.766744e+01,
        1.781070e+01, 2.143100e+01, 1.640030e+00, 1.066122e+02, 2.751004e+01,
        2.613397e+01, 4.461358e+01, 2.443641e+01, 1.503778e+01, 8.461586e+01,
        2.835516e+01, 5.667830e+00, 2.982004e+01, 1.196935e+01, 3.203715e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 55,(no reg loss)standard loss: 0.0042944, inverse loss: 0.0008964
Valid-->> Epoch [55/3000], Standardized Loss: 0.0154555, Inverse Loss: 0.0032262
Valid-->> Lowest loss found at epoch 55, loss: 0.0032262
Epoch 55, Masked params (inverse standardized): tensor([3.228930e+01, 4.430138e+01, 6.934166e-02, 2.631392e+01, 2.766748e+01,
        1.781057e+01, 2.143073e+01, 1.639944e+00, 1.066121e+02, 2.751043e+01,
        2.613400e+01, 4.461352e+01, 2.443573e+01, 1.503767e+01, 8.461582e+01,
        2.835455e+01, 5.667749e+00, 2.981948e+01, 1.196924e+01, 3.203689e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 56,(no reg loss)standard loss: 0.0042942, inverse loss: 0.0008964
Valid-->> Epoch [56/3000], Standardized Loss: 0.0154531, Inverse Loss: 0.0032257
Valid-->> Lowest loss found at epoch 56, loss: 0.0032257
Epoch 56, Masked params (inverse standardized): tensor([3.228909e+01, 4.430128e+01, 6.928444e-02, 2.631428e+01, 2.766753e+01,
        1.781047e+01, 2.143050e+01, 1.639875e+00, 1.066124e+02, 2.751083e+01,
        2.613404e+01, 4.461348e+01, 2.443510e+01, 1.503758e+01, 8.461591e+01,
        2.835397e+01, 5.667679e+00, 2.981897e+01, 1.196917e+01, 3.203666e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 57,(no reg loss)standard loss: 0.0043093, inverse loss: 0.0008995
Valid-->> Epoch [57/3000], Standardized Loss: 0.0154624, Inverse Loss: 0.0032276
Training -->> Epoch: 58,(no reg loss)standard loss: 0.0042976, inverse loss: 0.0008971
Valid-->> Epoch [58/3000], Standardized Loss: 0.0154449, Inverse Loss: 0.0032239
Valid-->> Lowest loss found at epoch 58, loss: 0.0032239
Epoch 58, Masked params (inverse standardized): tensor([3.228884e+01, 4.430145e+01, 6.926346e-02, 2.631505e+01, 2.766768e+01,
        1.781039e+01, 2.143019e+01, 1.639853e+00, 1.066122e+02, 2.751167e+01,
        2.613416e+01, 4.461352e+01, 2.443402e+01, 1.503753e+01, 8.461599e+01,
        2.835292e+01, 5.667679e+00, 2.981815e+01, 1.196914e+01, 3.203638e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 59,(no reg loss)standard loss: 0.0042925, inverse loss: 0.0008960
Valid-->> Epoch [59/3000], Standardized Loss: 0.0154478, Inverse Loss: 0.0032246
Training -->> Epoch: 60,(no reg loss)standard loss: 0.0043005, inverse loss: 0.0008977
Valid-->> Epoch [60/3000], Standardized Loss: 0.0154476, Inverse Loss: 0.0032245
Training -->> Epoch: 61,(no reg loss)standard loss: 0.0042949, inverse loss: 0.0008965
Valid-->> Epoch [61/3000], Standardized Loss: 0.0154410, Inverse Loss: 0.0032231
Valid-->> Lowest loss found at epoch 61, loss: 0.0032231
Epoch 61, Masked params (inverse standardized): tensor([3.228841e+01, 4.430138e+01, 6.912231e-02, 2.631593e+01, 2.766765e+01,
        1.781015e+01, 2.142969e+01, 1.639700e+00, 1.066120e+02, 2.751265e+01,
        2.613410e+01, 4.461346e+01, 2.443243e+01, 1.503733e+01, 8.461589e+01,
        2.835126e+01, 5.667562e+00, 2.981695e+01, 1.196896e+01, 3.203592e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 62,(no reg loss)standard loss: 0.0043005, inverse loss: 0.0008977
Valid-->> Epoch [62/3000], Standardized Loss: 0.0154466, Inverse Loss: 0.0032243
Training -->> Epoch: 63,(no reg loss)standard loss: 0.0043013, inverse loss: 0.0008979
Valid-->> Epoch [63/3000], Standardized Loss: 0.0154361, Inverse Loss: 0.0032221
Valid-->> Lowest loss found at epoch 63, loss: 0.0032221
Epoch 63, Masked params (inverse standardized): tensor([3.228824e+01, 4.430199e+01, 6.910896e-02, 2.631652e+01, 2.766765e+01,
        1.781009e+01, 2.142949e+01, 1.639675e+00, 1.066120e+02, 2.751331e+01,
        2.613408e+01, 4.461348e+01, 2.443156e+01, 1.503728e+01, 8.461650e+01,
        2.835030e+01, 5.667522e+00, 2.981635e+01, 1.196893e+01, 3.203574e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 64,(no reg loss)standard loss: 0.0042914, inverse loss: 0.0008958
Valid-->> Epoch [64/3000], Standardized Loss: 0.0154327, Inverse Loss: 0.0032214
Valid-->> Lowest loss found at epoch 64, loss: 0.0032214
Epoch 64, Masked params (inverse standardized): tensor([3.228819e+01, 4.430138e+01, 6.906128e-02, 2.631683e+01, 2.766767e+01,
        1.781006e+01, 2.142941e+01, 1.639656e+00, 1.066119e+02, 2.751365e+01,
        2.613409e+01, 4.461350e+01, 2.443119e+01, 1.503726e+01, 8.461591e+01,
        2.834985e+01, 5.667522e+00, 2.981610e+01, 1.196890e+01, 3.203568e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 65,(no reg loss)standard loss: 0.0042939, inverse loss: 0.0008963
Valid-->> Epoch [65/3000], Standardized Loss: 0.0154384, Inverse Loss: 0.0032226
Training -->> Epoch: 66,(no reg loss)standard loss: 0.0042978, inverse loss: 0.0008971
Valid-->> Epoch [66/3000], Standardized Loss: 0.0154304, Inverse Loss: 0.0032209
Valid-->> Lowest loss found at epoch 66, loss: 0.0032209
Epoch 66, Masked params (inverse standardized): tensor([3.228802e+01, 4.430144e+01, 6.894302e-02, 2.631732e+01, 2.766759e+01,
        1.780994e+01, 2.142921e+01, 1.639574e+00, 1.066124e+02, 2.751420e+01,
        2.613398e+01, 4.461349e+01, 2.443040e+01, 1.503716e+01, 8.461620e+01,
        2.834891e+01, 5.667471e+00, 2.981556e+01, 1.196880e+01, 3.203549e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 67,(no reg loss)standard loss: 0.0042850, inverse loss: 0.0008944
Valid-->> Epoch [67/3000], Standardized Loss: 0.0154220, Inverse Loss: 0.0032192
Valid-->> Lowest loss found at epoch 67, loss: 0.0032192
Epoch 67, Masked params (inverse standardized): tensor([3.228808e+01, 4.430138e+01, 6.899834e-02, 2.631772e+01, 2.766772e+01,
        1.781001e+01, 2.142924e+01, 1.639606e+00, 1.066122e+02, 2.751464e+01,
        2.613410e+01, 4.461355e+01, 2.443021e+01, 1.503722e+01, 8.461588e+01,
        2.834863e+01, 5.667517e+00, 2.981548e+01, 1.196886e+01, 3.203556e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 68,(no reg loss)standard loss: 0.0042929, inverse loss: 0.0008961
Valid-->> Epoch [68/3000], Standardized Loss: 0.0154232, Inverse Loss: 0.0032194
Training -->> Epoch: 69,(no reg loss)standard loss: 0.0042975, inverse loss: 0.0008971
Valid-->> Epoch [69/3000], Standardized Loss: 0.0154264, Inverse Loss: 0.0032201
Training -->> Epoch: 70,(no reg loss)standard loss: 0.0042960, inverse loss: 0.0008967
Valid-->> Epoch [70/3000], Standardized Loss: 0.0154237, Inverse Loss: 0.0032195
Training -->> Epoch: 71,(no reg loss)standard loss: 0.0042886, inverse loss: 0.0008952
Valid-->> Epoch [71/3000], Standardized Loss: 0.0154171, Inverse Loss: 0.0032181
Valid-->> Lowest loss found at epoch 71, loss: 0.0032181
Epoch 71, Masked params (inverse standardized): tensor([3.228786e+01, 4.430126e+01, 6.885338e-02, 2.631857e+01, 2.766750e+01,
        1.780983e+01, 2.142893e+01, 1.639482e+00, 1.066116e+02, 2.751562e+01,
        2.613383e+01, 4.461353e+01, 2.442899e+01, 1.503706e+01, 8.461589e+01,
        2.834700e+01, 5.667381e+00, 2.981470e+01, 1.196871e+01, 3.203531e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 72,(no reg loss)standard loss: 0.0043014, inverse loss: 0.0008979
Valid-->> Epoch [72/3000], Standardized Loss: 0.0154235, Inverse Loss: 0.0032195
Training -->> Epoch: 73,(no reg loss)standard loss: 0.0042957, inverse loss: 0.0008967
Valid-->> Epoch [73/3000], Standardized Loss: 0.0154204, Inverse Loss: 0.0032188
Training -->> Epoch: 74,(no reg loss)standard loss: 0.0043076, inverse loss: 0.0008992
Valid-->> Epoch [74/3000], Standardized Loss: 0.0154229, Inverse Loss: 0.0032194
Training -->> Epoch: 75,(no reg loss)standard loss: 0.0042931, inverse loss: 0.0008961
Valid-->> Epoch [75/3000], Standardized Loss: 0.0154048, Inverse Loss: 0.0032156
Valid-->> Lowest loss found at epoch 75, loss: 0.0032156
Epoch 75, Masked params (inverse standardized): tensor([3.228788e+01, 4.430170e+01, 6.889915e-02, 2.631947e+01, 2.766741e+01,
        1.780987e+01, 2.142890e+01, 1.639496e+00, 1.066130e+02, 2.751666e+01,
        2.613369e+01, 4.461365e+01, 2.442824e+01, 1.503710e+01, 8.461644e+01,
        2.834580e+01, 5.667400e+00, 2.981433e+01, 1.196875e+01, 3.203531e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 76,(no reg loss)standard loss: 0.0042844, inverse loss: 0.0008943
Valid-->> Epoch [76/3000], Standardized Loss: 0.0154027, Inverse Loss: 0.0032151
Valid-->> Lowest loss found at epoch 76, loss: 0.0032151
Epoch 76, Masked params (inverse standardized): tensor([3.228785e+01, 4.430141e+01, 6.885338e-02, 2.631966e+01, 2.766737e+01,
        1.780982e+01, 2.142885e+01, 1.639444e+00, 1.066119e+02, 2.751689e+01,
        2.613363e+01, 4.461361e+01, 2.442806e+01, 1.503706e+01, 8.461600e+01,
        2.834551e+01, 5.667368e+00, 2.981424e+01, 1.196871e+01, 3.203528e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 77,(no reg loss)standard loss: 0.0042866, inverse loss: 0.0008948
Valid-->> Epoch [77/3000], Standardized Loss: 0.0153992, Inverse Loss: 0.0032144
Valid-->> Lowest loss found at epoch 77, loss: 0.0032144
Epoch 77, Masked params (inverse standardized): tensor([3.228783e+01, 4.430113e+01, 6.881332e-02, 2.631989e+01, 2.766736e+01,
        1.780979e+01, 2.142883e+01, 1.639406e+00, 1.066111e+02, 2.751715e+01,
        2.613361e+01, 4.461360e+01, 2.442792e+01, 1.503703e+01, 8.461548e+01,
        2.834527e+01, 5.667347e+00, 2.981417e+01, 1.196868e+01, 3.203527e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 78,(no reg loss)standard loss: 0.0042841, inverse loss: 0.0008943
Valid-->> Epoch [78/3000], Standardized Loss: 0.0154010, Inverse Loss: 0.0032148
Training -->> Epoch: 79,(no reg loss)standard loss: 0.0042993, inverse loss: 0.0008974
Valid-->> Epoch [79/3000], Standardized Loss: 0.0154130, Inverse Loss: 0.0032173
Training -->> Epoch: 80,(no reg loss)standard loss: 0.0042929, inverse loss: 0.0008961
Valid-->> Epoch [80/3000], Standardized Loss: 0.0153976, Inverse Loss: 0.0032141
Valid-->> Lowest loss found at epoch 80, loss: 0.0032141
Epoch 80, Masked params (inverse standardized): tensor([3.228770e+01, 4.430146e+01, 6.871414e-02, 2.632030e+01, 2.766710e+01,
        1.780966e+01, 2.142865e+01, 1.639322e+00, 1.066124e+02, 2.751766e+01,
        2.613331e+01, 4.461358e+01, 2.442739e+01, 1.503691e+01, 8.461611e+01,
        2.834440e+01, 5.667278e+00, 2.981385e+01, 1.196856e+01, 3.203513e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 81,(no reg loss)standard loss: 0.0042861, inverse loss: 0.0008947
Valid-->> Epoch [81/3000], Standardized Loss: 0.0153924, Inverse Loss: 0.0032130
Valid-->> Lowest loss found at epoch 81, loss: 0.0032130
Epoch 81, Masked params (inverse standardized): tensor([3.228775e+01, 4.430149e+01, 6.874657e-02, 2.632054e+01, 2.766712e+01,
        1.780969e+01, 2.142868e+01, 1.639328e+00, 1.066118e+02, 2.751793e+01,
        2.613332e+01, 4.461360e+01, 2.442733e+01, 1.503693e+01, 8.461612e+01,
        2.834425e+01, 5.667259e+00, 2.981386e+01, 1.196859e+01, 3.203517e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 82,(no reg loss)standard loss: 0.0042842, inverse loss: 0.0008943
Valid-->> Epoch [82/3000], Standardized Loss: 0.0153861, Inverse Loss: 0.0032117
Valid-->> Lowest loss found at epoch 82, loss: 0.0032117
Epoch 82, Masked params (inverse standardized): tensor([3.228781e+01, 4.430158e+01, 6.876945e-02, 2.632080e+01, 2.766718e+01,
        1.780974e+01, 2.142874e+01, 1.639343e+00, 1.066126e+02, 2.751822e+01,
        2.613336e+01, 4.461364e+01, 2.442732e+01, 1.503696e+01, 8.461626e+01,
        2.834414e+01, 5.667271e+00, 2.981390e+01, 1.196863e+01, 3.203523e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 83,(no reg loss)standard loss: 0.0042780, inverse loss: 0.0008930
Valid-->> Epoch [83/3000], Standardized Loss: 0.0153832, Inverse Loss: 0.0032111
Valid-->> Lowest loss found at epoch 83, loss: 0.0032111
Epoch 83, Masked params (inverse standardized): tensor([3.228780e+01, 4.430162e+01, 6.872559e-02, 2.632098e+01, 2.766715e+01,
        1.780970e+01, 2.142871e+01, 1.639299e+00, 1.066122e+02, 2.751843e+01,
        2.613333e+01, 4.461362e+01, 2.442723e+01, 1.503692e+01, 8.461632e+01,
        2.834396e+01, 5.667255e+00, 2.981386e+01, 1.196859e+01, 3.203522e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 84,(no reg loss)standard loss: 0.0042898, inverse loss: 0.0008955
Valid-->> Epoch [84/3000], Standardized Loss: 0.0153784, Inverse Loss: 0.0032101
Valid-->> Lowest loss found at epoch 84, loss: 0.0032101
Epoch 84, Masked params (inverse standardized): tensor([3.228783e+01, 4.430179e+01, 6.871796e-02, 2.632120e+01, 2.766717e+01,
        1.780972e+01, 2.142873e+01, 1.639324e+00, 1.066123e+02, 2.751868e+01,
        2.613333e+01, 4.461367e+01, 2.442719e+01, 1.503695e+01, 8.461646e+01,
        2.834382e+01, 5.667286e+00, 2.981387e+01, 1.196861e+01, 3.203526e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 85,(no reg loss)standard loss: 0.0042747, inverse loss: 0.0008923
Valid-->> Epoch [85/3000], Standardized Loss: 0.0153733, Inverse Loss: 0.0032090
Valid-->> Lowest loss found at epoch 85, loss: 0.0032090
Epoch 85, Masked params (inverse standardized): tensor([3.228786e+01, 4.430186e+01, 6.870079e-02, 2.632142e+01, 2.766720e+01,
        1.780974e+01, 2.142875e+01, 1.639343e+00, 1.066125e+02, 2.751894e+01,
        2.613335e+01, 4.461371e+01, 2.442715e+01, 1.503697e+01, 8.461629e+01,
        2.834371e+01, 5.667305e+00, 2.981388e+01, 1.196862e+01, 3.203530e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 86,(no reg loss)standard loss: 0.0042773, inverse loss: 0.0008928
Valid-->> Epoch [86/3000], Standardized Loss: 0.0153762, Inverse Loss: 0.0032096
Training -->> Epoch: 87,(no reg loss)standard loss: 0.0042932, inverse loss: 0.0008962
Valid-->> Epoch [87/3000], Standardized Loss: 0.0153878, Inverse Loss: 0.0032120
Training -->> Epoch: 88,(no reg loss)standard loss: 0.0042890, inverse loss: 0.0008953
Valid-->> Epoch [88/3000], Standardized Loss: 0.0153740, Inverse Loss: 0.0032091
Training -->> Epoch: 89,(no reg loss)standard loss: 0.0042933, inverse loss: 0.0008962
Valid-->> Epoch [89/3000], Standardized Loss: 0.0153797, Inverse Loss: 0.0032103
Training -->> Epoch: 90,(no reg loss)standard loss: 0.0042842, inverse loss: 0.0008943
Valid-->> Epoch [90/3000], Standardized Loss: 0.0153649, Inverse Loss: 0.0032072
Valid-->> Lowest loss found at epoch 90, loss: 0.0032072
Epoch 90, Masked params (inverse standardized): tensor([3.228782e+01, 4.430119e+01, 6.867218e-02, 2.632202e+01, 2.766690e+01,
        1.780967e+01, 2.142864e+01, 1.639282e+00, 1.066119e+02, 2.751969e+01,
        2.613299e+01, 4.461375e+01, 2.442675e+01, 1.503693e+01, 8.461560e+01,
        2.834289e+01, 5.667278e+00, 2.981370e+01, 1.196857e+01, 3.203525e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 91,(no reg loss)standard loss: 0.0043165, inverse loss: 0.0009010
Valid-->> Epoch [91/3000], Standardized Loss: 0.0154026, Inverse Loss: 0.0032151
Training -->> Epoch: 92,(no reg loss)standard loss: 0.0043113, inverse loss: 0.0008999
Valid-->> Epoch [92/3000], Standardized Loss: 0.0153764, Inverse Loss: 0.0032096
Training -->> Epoch: 93,(no reg loss)standard loss: 0.0042788, inverse loss: 0.0008932
Valid-->> Epoch [93/3000], Standardized Loss: 0.0153528, Inverse Loss: 0.0032047
Valid-->> Lowest loss found at epoch 93, loss: 0.0032047
Epoch 93, Masked params (inverse standardized): tensor([3.228802e+01, 4.430145e+01, 6.881523e-02, 2.632248e+01, 2.766689e+01,
        1.780986e+01, 2.142882e+01, 1.639400e+00, 1.066125e+02, 2.752024e+01,
        2.613294e+01, 4.461392e+01, 2.442679e+01, 1.503708e+01, 8.461611e+01,
        2.834270e+01, 5.667376e+00, 2.981384e+01, 1.196875e+01, 3.203544e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 94,(no reg loss)standard loss: 0.0042941, inverse loss: 0.0008963
Valid-->> Epoch [94/3000], Standardized Loss: 0.0153765, Inverse Loss: 0.0032097
Training -->> Epoch: 95,(no reg loss)standard loss: 0.0043148, inverse loss: 0.0009007
Valid-->> Epoch [95/3000], Standardized Loss: 0.0153805, Inverse Loss: 0.0032105
Training -->> Epoch: 96,(no reg loss)standard loss: 0.0043055, inverse loss: 0.0008987
Valid-->> Epoch [96/3000], Standardized Loss: 0.0153672, Inverse Loss: 0.0032077
Training -->> Epoch: 97,(no reg loss)standard loss: 0.0042847, inverse loss: 0.0008944
Valid-->> Epoch [97/3000], Standardized Loss: 0.0153508, Inverse Loss: 0.0032043
Valid-->> Lowest loss found at epoch 97, loss: 0.0032043
Epoch 97, Masked params (inverse standardized): tensor([3.228792e+01, 4.430151e+01, 6.866264e-02, 2.632271e+01, 2.766655e+01,
        1.780973e+01, 2.142868e+01, 1.639267e+00, 1.066118e+02, 2.752059e+01,
        2.613256e+01, 4.461388e+01, 2.442653e+01, 1.503696e+01, 8.461592e+01,
        2.834219e+01, 5.667274e+00, 2.981369e+01, 1.196860e+01, 3.203535e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 98,(no reg loss)standard loss: 0.0042783, inverse loss: 0.0008930
Valid-->> Epoch [98/3000], Standardized Loss: 0.0153477, Inverse Loss: 0.0032037
Valid-->> Lowest loss found at epoch 98, loss: 0.0032037
Epoch 98, Masked params (inverse standardized): tensor([3.228790e+01, 4.430183e+01, 6.859970e-02, 2.632284e+01, 2.766655e+01,
        1.780967e+01, 2.142864e+01, 1.639164e+00, 1.066125e+02, 2.752076e+01,
        2.613255e+01, 4.461379e+01, 2.442651e+01, 1.503688e+01, 8.461658e+01,
        2.834216e+01, 5.667137e+00, 2.981369e+01, 1.196852e+01, 3.203532e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 99,(no reg loss)standard loss: 0.0042931, inverse loss: 0.0008961
Valid-->> Epoch [99/3000], Standardized Loss: 0.0153573, Inverse Loss: 0.0032057
Training -->> Epoch: 100,(no reg loss)standard loss: 0.0042839, inverse loss: 0.0008942
Valid-->> Epoch [100/3000], Standardized Loss: 0.0153371, Inverse Loss: 0.0032014
Valid-->> Lowest loss found at epoch 100, loss: 0.0032014
Epoch 100, Masked params (inverse standardized): tensor([3.228799e+01, 4.430206e+01, 6.862640e-02, 2.632320e+01, 2.766665e+01,
        1.780973e+01, 2.142872e+01, 1.639235e+00, 1.066122e+02, 2.752117e+01,
        2.613262e+01, 4.461388e+01, 2.442660e+01, 1.503694e+01, 8.461685e+01,
        2.834217e+01, 5.667208e+00, 2.981380e+01, 1.196857e+01, 3.203542e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 101,(no reg loss)standard loss: 0.0042668, inverse loss: 0.0008906
Valid-->> Epoch [101/3000], Standardized Loss: 0.0153354, Inverse Loss: 0.0032011
Valid-->> Lowest loss found at epoch 101, loss: 0.0032011
Epoch 101, Masked params (inverse standardized): tensor([3.228793e+01, 4.430152e+01, 6.851387e-02, 2.632330e+01, 2.766662e+01,
        1.780963e+01, 2.142864e+01, 1.639160e+00, 1.066125e+02, 2.752129e+01,
        2.613258e+01, 4.461383e+01, 2.442654e+01, 1.503687e+01, 8.461610e+01,
        2.834208e+01, 5.667181e+00, 2.981376e+01, 1.196848e+01, 3.203537e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 102,(no reg loss)standard loss: 0.0043039, inverse loss: 0.0008984
Valid-->> Epoch [102/3000], Standardized Loss: 0.0153635, Inverse Loss: 0.0032069
Training -->> Epoch: 103,(no reg loss)standard loss: 0.0043160, inverse loss: 0.0009009
Valid-->> Epoch [103/3000], Standardized Loss: 0.0153583, Inverse Loss: 0.0032059
Training -->> Epoch: 104,(no reg loss)standard loss: 0.0042956, inverse loss: 0.0008967
Valid-->> Epoch [104/3000], Standardized Loss: 0.0153367, Inverse Loss: 0.0032014
Training -->> Epoch: 105,(no reg loss)standard loss: 0.0042704, inverse loss: 0.0008914
Valid-->> Epoch [105/3000], Standardized Loss: 0.0153187, Inverse Loss: 0.0031976
Valid-->> Lowest loss found at epoch 105, loss: 0.0031976
Epoch 105, Masked params (inverse standardized): tensor([3.228817e+01, 4.430196e+01, 6.870270e-02, 2.632381e+01, 2.766669e+01,
        1.780986e+01, 2.142885e+01, 1.639305e+00, 1.066125e+02, 2.752191e+01,
        2.613261e+01, 4.461404e+01, 2.442669e+01, 1.503706e+01, 8.461679e+01,
        2.834211e+01, 5.667278e+00, 2.981397e+01, 1.196870e+01, 3.203560e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 106,(no reg loss)standard loss: 0.0042571, inverse loss: 0.0008886
Valid-->> Epoch [106/3000], Standardized Loss: 0.0153146, Inverse Loss: 0.0031968
Valid-->> Lowest loss found at epoch 106, loss: 0.0031968
Epoch 106, Masked params (inverse standardized): tensor([3.228812e+01, 4.430175e+01, 6.854439e-02, 2.632397e+01, 2.766674e+01,
        1.780975e+01, 2.142879e+01, 1.639185e+00, 1.066119e+02, 2.752209e+01,
        2.613265e+01, 4.461395e+01, 2.442669e+01, 1.503695e+01, 8.461642e+01,
        2.834210e+01, 5.667177e+00, 2.981396e+01, 1.196856e+01, 3.203556e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 107,(no reg loss)standard loss: 0.0042606, inverse loss: 0.0008894
Valid-->> Epoch [107/3000], Standardized Loss: 0.0153078, Inverse Loss: 0.0031953
Valid-->> Lowest loss found at epoch 107, loss: 0.0031953
Epoch 107, Masked params (inverse standardized): tensor([3.228814e+01, 4.430151e+01, 6.852341e-02, 2.632419e+01, 2.766686e+01,
        1.780973e+01, 2.142881e+01, 1.639132e+00, 1.066121e+02, 2.752234e+01,
        2.613276e+01, 4.461392e+01, 2.442676e+01, 1.503692e+01, 8.461596e+01,
        2.834219e+01, 5.667131e+00, 2.981403e+01, 1.196853e+01, 3.203559e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 108,(no reg loss)standard loss: 0.0042708, inverse loss: 0.0008915
Valid-->> Epoch [108/3000], Standardized Loss: 0.0153284, Inverse Loss: 0.0031996
Training -->> Epoch: 109,(no reg loss)standard loss: 0.0042795, inverse loss: 0.0008933
Valid-->> Epoch [109/3000], Standardized Loss: 0.0153250, Inverse Loss: 0.0031989
Training -->> Epoch: 110,(no reg loss)standard loss: 0.0042711, inverse loss: 0.0008915
Valid-->> Epoch [110/3000], Standardized Loss: 0.0153109, Inverse Loss: 0.0031960
Training -->> Epoch: 111,(no reg loss)standard loss: 0.0042901, inverse loss: 0.0008955
Valid-->> Epoch [111/3000], Standardized Loss: 0.0153387, Inverse Loss: 0.0032018
Training -->> Epoch: 112,(no reg loss)standard loss: 0.0042959, inverse loss: 0.0008967
Valid-->> Epoch [112/3000], Standardized Loss: 0.0153133, Inverse Loss: 0.0031965
Training -->> Epoch: 113,(no reg loss)standard loss: 0.0042950, inverse loss: 0.0008965
Valid-->> Epoch [113/3000], Standardized Loss: 0.0153183, Inverse Loss: 0.0031975
Training -->> Epoch: 114,(no reg loss)standard loss: 0.0043066, inverse loss: 0.0008990
Valid-->> Epoch [114/3000], Standardized Loss: 0.0153340, Inverse Loss: 0.0032008
Training -->> Epoch: 115,(no reg loss)standard loss: 0.0043035, inverse loss: 0.0008983
Valid-->> Epoch [115/3000], Standardized Loss: 0.0153245, Inverse Loss: 0.0031988
Training -->> Epoch: 116,(no reg loss)standard loss: 0.0043111, inverse loss: 0.0008999
Valid-->> Epoch [116/3000], Standardized Loss: 0.0153338, Inverse Loss: 0.0032008
Training -->> Epoch: 117,(no reg loss)standard loss: 0.0042990, inverse loss: 0.0008974
Valid-->> Epoch [117/3000], Standardized Loss: 0.0153083, Inverse Loss: 0.0031954
Training -->> Epoch: 118,(no reg loss)standard loss: 0.0043097, inverse loss: 0.0008996
Valid-->> Epoch [118/3000], Standardized Loss: 0.0153186, Inverse Loss: 0.0031976
Training -->> Epoch: 119,(no reg loss)standard loss: 0.0042906, inverse loss: 0.0008956
Valid-->> Epoch [119/3000], Standardized Loss: 0.0152882, Inverse Loss: 0.0031912
Valid-->> Lowest loss found at epoch 119, loss: 0.0031912
Epoch 119, Masked params (inverse standardized): tensor([3.228828e+01, 4.430221e+01, 6.865120e-02, 2.632470e+01, 2.766646e+01,
        1.780986e+01, 2.142884e+01, 1.639254e+00, 1.066118e+02, 2.752310e+01,
        2.613224e+01, 4.461419e+01, 2.442659e+01, 1.503706e+01, 8.461679e+01,
        2.834181e+01, 5.667259e+00, 2.981403e+01, 1.196867e+01, 3.203572e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 120,(no reg loss)standard loss: 0.0042344, inverse loss: 0.0008839
Valid-->> Epoch [120/3000], Standardized Loss: 0.0152563, Inverse Loss: 0.0031846
Valid-->> Lowest loss found at epoch 120, loss: 0.0031846
Epoch 120, Masked params (inverse standardized): tensor([3.228883e+01, 4.430167e+01, 6.874657e-02, 2.632566e+01, 2.766734e+01,
        1.781023e+01, 2.142938e+01, 1.639391e+00, 1.066118e+02, 2.752407e+01,
        2.613311e+01, 4.461443e+01, 2.442734e+01, 1.503737e+01, 8.461623e+01,
        2.834262e+01, 5.667377e+00, 2.981473e+01, 1.196892e+01, 3.203627e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 121,(no reg loss)standard loss: 0.0042504, inverse loss: 0.0008872
Valid-->> Epoch [121/3000], Standardized Loss: 0.0152876, Inverse Loss: 0.0031911
Training -->> Epoch: 122,(no reg loss)standard loss: 0.0042769, inverse loss: 0.0008927
Valid-->> Epoch [122/3000], Standardized Loss: 0.0152995, Inverse Loss: 0.0031936
Training -->> Epoch: 123,(no reg loss)standard loss: 0.0042949, inverse loss: 0.0008965
Valid-->> Epoch [123/3000], Standardized Loss: 0.0153074, Inverse Loss: 0.0031952
Training -->> Epoch: 124,(no reg loss)standard loss: 0.0043342, inverse loss: 0.0009047
Valid-->> Epoch [124/3000], Standardized Loss: 0.0153218, Inverse Loss: 0.0031983
Training -->> Epoch: 125,(no reg loss)standard loss: 0.0043090, inverse loss: 0.0008994
Valid-->> Epoch [125/3000], Standardized Loss: 0.0152902, Inverse Loss: 0.0031917
Training -->> Epoch: 126,(no reg loss)standard loss: 0.0042894, inverse loss: 0.0008954
Valid-->> Epoch [126/3000], Standardized Loss: 0.0152758, Inverse Loss: 0.0031886
Training -->> Epoch: 127,(no reg loss)standard loss: 0.0042722, inverse loss: 0.0008918
Valid-->> Epoch [127/3000], Standardized Loss: 0.0152713, Inverse Loss: 0.0031877
Training -->> Epoch: 128,(no reg loss)standard loss: 0.0042925, inverse loss: 0.0008960
Valid-->> Epoch [128/3000], Standardized Loss: 0.0152921, Inverse Loss: 0.0031920
Training -->> Epoch: 129,(no reg loss)standard loss: 0.0043422, inverse loss: 0.0009064
Valid-->> Epoch [129/3000], Standardized Loss: 0.0153167, Inverse Loss: 0.0031972
Training -->> Epoch: 130,(no reg loss)standard loss: 0.0042931, inverse loss: 0.0008961
Valid-->> Epoch [130/3000], Standardized Loss: 0.0152614, Inverse Loss: 0.0031856
Training -->> Epoch: 131,(no reg loss)standard loss: 0.0042842, inverse loss: 0.0008943
Valid-->> Epoch [131/3000], Standardized Loss: 0.0152651, Inverse Loss: 0.0031864
Training -->> Epoch: 132,(no reg loss)standard loss: 0.0042365, inverse loss: 0.0008843
Valid-->> Epoch [132/3000], Standardized Loss: 0.0152204, Inverse Loss: 0.0031771
Valid-->> Lowest loss found at epoch 132, loss: 0.0031771
Epoch 132, Masked params (inverse standardized): tensor([3.228908e+01, 4.430184e+01, 6.880188e-02, 2.632652e+01, 2.766768e+01,
        1.781033e+01, 2.142954e+01, 1.639406e+00, 1.066123e+02, 2.752512e+01,
        2.613335e+01, 4.461460e+01, 2.442759e+01, 1.503750e+01, 8.461636e+01,
        2.834295e+01, 5.667416e+00, 2.981504e+01, 1.196898e+01, 3.203653e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 133,(no reg loss)standard loss: 0.0042118, inverse loss: 0.0008792
Valid-->> Epoch [133/3000], Standardized Loss: 0.0152203, Inverse Loss: 0.0031771
Valid-->> Lowest loss found at epoch 133, loss: 0.0031771
Epoch 133, Masked params (inverse standardized): tensor([3.228875e+01, 4.430140e+01, 6.838036e-02, 2.632659e+01, 2.766768e+01,
        1.780992e+01, 2.142921e+01, 1.639006e+00, 1.066124e+02, 2.752520e+01,
        2.613335e+01, 4.461420e+01, 2.442743e+01, 1.503706e+01, 8.461607e+01,
        2.834287e+01, 5.667044e+00, 2.981484e+01, 1.196855e+01, 3.203620e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 134,(no reg loss)standard loss: 0.0042708, inverse loss: 0.0008915
Valid-->> Epoch [134/3000], Standardized Loss: 0.0152730, Inverse Loss: 0.0031881
Training -->> Epoch: 135,(no reg loss)standard loss: 0.0042994, inverse loss: 0.0008974
Valid-->> Epoch [135/3000], Standardized Loss: 0.0152640, Inverse Loss: 0.0031862
Training -->> Epoch: 136,(no reg loss)standard loss: 0.0043269, inverse loss: 0.0009032
Valid-->> Epoch [136/3000], Standardized Loss: 0.0152920, Inverse Loss: 0.0031920
Training -->> Epoch: 137,(no reg loss)standard loss: 0.0043138, inverse loss: 0.0009005
Valid-->> Epoch [137/3000], Standardized Loss: 0.0152650, Inverse Loss: 0.0031864
Training -->> Epoch: 138,(no reg loss)standard loss: 0.0042826, inverse loss: 0.0008939
Valid-->> Epoch [138/3000], Standardized Loss: 0.0152592, Inverse Loss: 0.0031852
Training -->> Epoch: 139,(no reg loss)standard loss: 0.0043000, inverse loss: 0.0008976
Valid-->> Epoch [139/3000], Standardized Loss: 0.0152775, Inverse Loss: 0.0031890
Training -->> Epoch: 140,(no reg loss)standard loss: 0.0042808, inverse loss: 0.0008936
Valid-->> Epoch [140/3000], Standardized Loss: 0.0152384, Inverse Loss: 0.0031808
Training -->> Epoch: 141,(no reg loss)standard loss: 0.0042927, inverse loss: 0.0008961
Valid-->> Epoch [141/3000], Standardized Loss: 0.0152647, Inverse Loss: 0.0031863
Training -->> Epoch: 142,(no reg loss)standard loss: 0.0042853, inverse loss: 0.0008945
Valid-->> Epoch [142/3000], Standardized Loss: 0.0152429, Inverse Loss: 0.0031818
Training -->> Epoch: 143,(no reg loss)standard loss: 0.0042812, inverse loss: 0.0008936
Valid-->> Epoch [143/3000], Standardized Loss: 0.0152416, Inverse Loss: 0.0031815
Training -->> Epoch: 144,(no reg loss)standard loss: 0.0042723, inverse loss: 0.0008918
Valid-->> Epoch [144/3000], Standardized Loss: 0.0152245, Inverse Loss: 0.0031779
Training -->> Epoch: 145,(no reg loss)standard loss: 0.0042167, inverse loss: 0.0008802
Valid-->> Epoch [145/3000], Standardized Loss: 0.0151800, Inverse Loss: 0.0031686
Valid-->> Lowest loss found at epoch 145, loss: 0.0031686
Epoch 145, Masked params (inverse standardized): tensor([3.228912e+01, 4.430230e+01, 6.844711e-02, 2.632758e+01, 2.766833e+01,
        1.781018e+01, 2.142950e+01, 1.639343e+00, 1.066105e+02, 2.752626e+01,
        2.613393e+01, 4.461465e+01, 2.442779e+01, 1.503743e+01, 8.461700e+01,
        2.834330e+01, 5.667582e+00, 2.981523e+01, 1.196883e+01, 3.203671e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 146,(no reg loss)standard loss: 0.0042159, inverse loss: 0.0008800
Valid-->> Epoch [146/3000], Standardized Loss: 0.0152112, Inverse Loss: 0.0031752
Training -->> Epoch: 147,(no reg loss)standard loss: 0.0042910, inverse loss: 0.0008957
Valid-->> Epoch [147/3000], Standardized Loss: 0.0152544, Inverse Loss: 0.0031842
Training -->> Epoch: 148,(no reg loss)standard loss: 0.0042812, inverse loss: 0.0008937
Valid-->> Epoch [148/3000], Standardized Loss: 0.0152159, Inverse Loss: 0.0031761
Training -->> Epoch: 149,(no reg loss)standard loss: 0.0043222, inverse loss: 0.0009022
Valid-->> Epoch [149/3000], Standardized Loss: 0.0152627, Inverse Loss: 0.0031859
Training -->> Epoch: 150,(no reg loss)standard loss: 0.0042905, inverse loss: 0.0008956
Valid-->> Epoch [150/3000], Standardized Loss: 0.0152165, Inverse Loss: 0.0031763
Training -->> Epoch: 151,(no reg loss)standard loss: 0.0042852, inverse loss: 0.0008945
Valid-->> Epoch [151/3000], Standardized Loss: 0.0152397, Inverse Loss: 0.0031811
Training -->> Epoch: 152,(no reg loss)standard loss: 0.0043084, inverse loss: 0.0008993
Valid-->> Epoch [152/3000], Standardized Loss: 0.0152314, Inverse Loss: 0.0031794
Training -->> Epoch: 153,(no reg loss)standard loss: 0.0043019, inverse loss: 0.0008980
Valid-->> Epoch [153/3000], Standardized Loss: 0.0152305, Inverse Loss: 0.0031792
Training -->> Epoch: 154,(no reg loss)standard loss: 0.0043035, inverse loss: 0.0008983
Valid-->> Epoch [154/3000], Standardized Loss: 0.0152279, Inverse Loss: 0.0031787
Training -->> Epoch: 155,(no reg loss)standard loss: 0.0042934, inverse loss: 0.0008962
Valid-->> Epoch [155/3000], Standardized Loss: 0.0152175, Inverse Loss: 0.0031765
Training -->> Epoch: 156,(no reg loss)standard loss: 0.0042877, inverse loss: 0.0008950
Valid-->> Epoch [156/3000], Standardized Loss: 0.0152165, Inverse Loss: 0.0031763
Training -->> Epoch: 157,(no reg loss)standard loss: 0.0043082, inverse loss: 0.0008993
Valid-->> Epoch [157/3000], Standardized Loss: 0.0152257, Inverse Loss: 0.0031782
Training -->> Epoch: 158,(no reg loss)standard loss: 0.0042846, inverse loss: 0.0008944
Valid-->> Epoch [158/3000], Standardized Loss: 0.0152083, Inverse Loss: 0.0031746
Training -->> Epoch: 159,(no reg loss)standard loss: 0.0042719, inverse loss: 0.0008917
Valid-->> Epoch [159/3000], Standardized Loss: 0.0151883, Inverse Loss: 0.0031704
Training -->> Epoch: 160,(no reg loss)standard loss: 0.0042706, inverse loss: 0.0008914
Valid-->> Epoch [160/3000], Standardized Loss: 0.0151927, Inverse Loss: 0.0031713
Training -->> Epoch: 161,(no reg loss)standard loss: 0.0043104, inverse loss: 0.0008997
Valid-->> Epoch [161/3000], Standardized Loss: 0.0152344, Inverse Loss: 0.0031800
Training -->> Epoch: 162,(no reg loss)standard loss: 0.0043204, inverse loss: 0.0009018
Valid-->> Epoch [162/3000], Standardized Loss: 0.0152174, Inverse Loss: 0.0031764
Training -->> Epoch: 163,(no reg loss)standard loss: 0.0042869, inverse loss: 0.0008948
Valid-->> Epoch [163/3000], Standardized Loss: 0.0151782, Inverse Loss: 0.0031683
Valid-->> Lowest loss found at epoch 163, loss: 0.0031683
Epoch 163, Masked params (inverse standardized): tensor([3.228865e+01, 4.430119e+01, 6.853485e-02, 2.632681e+01, 2.766740e+01,
        1.780994e+01, 2.142899e+01, 1.639177e+00, 1.066121e+02, 2.752567e+01,
        2.613291e+01, 4.461461e+01, 2.442691e+01, 1.503715e+01, 8.461595e+01,
        2.834245e+01, 5.667276e+00, 2.981451e+01, 1.196871e+01, 3.203609e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 164,(no reg loss)standard loss: 0.0042668, inverse loss: 0.0008906
Valid-->> Epoch [164/3000], Standardized Loss: 0.0151884, Inverse Loss: 0.0031704
Training -->> Epoch: 165,(no reg loss)standard loss: 0.0042832, inverse loss: 0.0008941
Valid-->> Epoch [165/3000], Standardized Loss: 0.0151902, Inverse Loss: 0.0031708
Training -->> Epoch: 166,(no reg loss)standard loss: 0.0042934, inverse loss: 0.0008962
Valid-->> Epoch [166/3000], Standardized Loss: 0.0152066, Inverse Loss: 0.0031742
Training -->> Epoch: 167,(no reg loss)standard loss: 0.0043265, inverse loss: 0.0009031
Valid-->> Epoch [167/3000], Standardized Loss: 0.0152150, Inverse Loss: 0.0031759
Training -->> Epoch: 168,(no reg loss)standard loss: 0.0042827, inverse loss: 0.0008940
Valid-->> Epoch [168/3000], Standardized Loss: 0.0151638, Inverse Loss: 0.0031653
Valid-->> Lowest loss found at epoch 168, loss: 0.0031653
Epoch 168, Masked params (inverse standardized): tensor([3.228873e+01, 4.430106e+01, 6.852913e-02, 2.632705e+01, 2.766762e+01,
        1.780996e+01, 2.142904e+01, 1.639168e+00, 1.066122e+02, 2.752595e+01,
        2.613311e+01, 4.461462e+01, 2.442702e+01, 1.503716e+01, 8.461580e+01,
        2.834261e+01, 5.667286e+00, 2.981462e+01, 1.196872e+01, 3.203621e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 169,(no reg loss)standard loss: 0.0042764, inverse loss: 0.0008926
Valid-->> Epoch [169/3000], Standardized Loss: 0.0151809, Inverse Loss: 0.0031688
Training -->> Epoch: 170,(no reg loss)standard loss: 0.0042828, inverse loss: 0.0008940
Valid-->> Epoch [170/3000], Standardized Loss: 0.0151905, Inverse Loss: 0.0031708
Training -->> Epoch: 171,(no reg loss)standard loss: 0.0043780, inverse loss: 0.0009139
Valid-->> Epoch [171/3000], Standardized Loss: 0.0152504, Inverse Loss: 0.0031833
Training -->> Epoch: 172,(no reg loss)standard loss: 0.0043273, inverse loss: 0.0009033
Valid-->> Epoch [172/3000], Standardized Loss: 0.0151746, Inverse Loss: 0.0031675
Training -->> Epoch: 173,(no reg loss)standard loss: 0.0043168, inverse loss: 0.0009011
Valid-->> Epoch [173/3000], Standardized Loss: 0.0151970, Inverse Loss: 0.0031722
Training -->> Epoch: 174,(no reg loss)standard loss: 0.0042757, inverse loss: 0.0008925
Valid-->> Epoch [174/3000], Standardized Loss: 0.0151454, Inverse Loss: 0.0031614
Valid-->> Lowest loss found at epoch 174, loss: 0.0031614
Epoch 174, Masked params (inverse standardized): tensor([3.228896e+01, 4.430112e+01, 6.836510e-02, 2.632730e+01, 2.766788e+01,
        1.781002e+01, 2.142924e+01, 1.639091e+00, 1.066119e+02, 2.752626e+01,
        2.613337e+01, 4.461460e+01, 2.442732e+01, 1.503717e+01, 8.461557e+01,
        2.834294e+01, 5.667198e+00, 2.981493e+01, 1.196866e+01, 3.203645e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 175,(no reg loss)standard loss: 0.0042732, inverse loss: 0.0008920
Valid-->> Epoch [175/3000], Standardized Loss: 0.0151874, Inverse Loss: 0.0031702
Training -->> Epoch: 176,(no reg loss)standard loss: 0.0043166, inverse loss: 0.0009010
Valid-->> Epoch [176/3000], Standardized Loss: 0.0151892, Inverse Loss: 0.0031706
Training -->> Epoch: 177,(no reg loss)standard loss: 0.0042901, inverse loss: 0.0008955
Valid-->> Epoch [177/3000], Standardized Loss: 0.0151794, Inverse Loss: 0.0031685
Training -->> Epoch: 178,(no reg loss)standard loss: 0.0043169, inverse loss: 0.0009011
Valid-->> Epoch [178/3000], Standardized Loss: 0.0151959, Inverse Loss: 0.0031720
Training -->> Epoch: 179,(no reg loss)standard loss: 0.0043144, inverse loss: 0.0009006
Valid-->> Epoch [179/3000], Standardized Loss: 0.0151932, Inverse Loss: 0.0031714
Training -->> Epoch: 180,(no reg loss)standard loss: 0.0043528, inverse loss: 0.0009086
Valid-->> Epoch [180/3000], Standardized Loss: 0.0151924, Inverse Loss: 0.0031712
Training -->> Epoch: 181,(no reg loss)standard loss: 0.0043228, inverse loss: 0.0009023
Valid-->> Epoch [181/3000], Standardized Loss: 0.0151504, Inverse Loss: 0.0031625
Training -->> Epoch: 182,(no reg loss)standard loss: 0.0042963, inverse loss: 0.0008968
Valid-->> Epoch [182/3000], Standardized Loss: 0.0151770, Inverse Loss: 0.0031680
Training -->> Epoch: 183,(no reg loss)standard loss: 0.0043425, inverse loss: 0.0009065
Valid-->> Epoch [183/3000], Standardized Loss: 0.0151755, Inverse Loss: 0.0031677
Training -->> Epoch: 184,(no reg loss)standard loss: 0.0042557, inverse loss: 0.0008883
Valid-->> Epoch [184/3000], Standardized Loss: 0.0151134, Inverse Loss: 0.0031547
Valid-->> Lowest loss found at epoch 184, loss: 0.0031547
Epoch 184, Masked params (inverse standardized): tensor([3.228919e+01, 4.430158e+01, 6.858826e-02, 2.632786e+01, 2.766844e+01,
        1.781018e+01, 2.142944e+01, 1.639149e+00, 1.066126e+02, 2.752689e+01,
        2.613388e+01, 4.461472e+01, 2.442766e+01, 1.503728e+01, 8.461589e+01,
        2.834340e+01, 5.667196e+00, 2.981526e+01, 1.196882e+01, 3.203662e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 185,(no reg loss)standard loss: 0.0042687, inverse loss: 0.0008910
Valid-->> Epoch [185/3000], Standardized Loss: 0.0151620, Inverse Loss: 0.0031649
Training -->> Epoch: 186,(no reg loss)standard loss: 0.0043006, inverse loss: 0.0008977
Valid-->> Epoch [186/3000], Standardized Loss: 0.0151523, Inverse Loss: 0.0031629
Training -->> Epoch: 187,(no reg loss)standard loss: 0.0042908, inverse loss: 0.0008957
Valid-->> Epoch [187/3000], Standardized Loss: 0.0151641, Inverse Loss: 0.0031653
Training -->> Epoch: 188,(no reg loss)standard loss: 0.0043298, inverse loss: 0.0009038
Valid-->> Epoch [188/3000], Standardized Loss: 0.0151782, Inverse Loss: 0.0031683
Training -->> Epoch: 189,(no reg loss)standard loss: 0.0043338, inverse loss: 0.0009046
Valid-->> Epoch [189/3000], Standardized Loss: 0.0151368, Inverse Loss: 0.0031596
Training -->> Epoch: 190,(no reg loss)standard loss: 0.0043041, inverse loss: 0.0008984
Valid-->> Epoch [190/3000], Standardized Loss: 0.0151380, Inverse Loss: 0.0031599
Training -->> Epoch: 191,(no reg loss)standard loss: 0.0043153, inverse loss: 0.0009008
Valid-->> Epoch [191/3000], Standardized Loss: 0.0151351, Inverse Loss: 0.0031593
Training -->> Epoch: 192,(no reg loss)standard loss: 0.0042821, inverse loss: 0.0008938
Valid-->> Epoch [192/3000], Standardized Loss: 0.0151227, Inverse Loss: 0.0031567
Training -->> Epoch: 193,(no reg loss)standard loss: 0.0042946, inverse loss: 0.0008964
Valid-->> Epoch [193/3000], Standardized Loss: 0.0151179, Inverse Loss: 0.0031557
Training -->> Epoch: 194,(no reg loss)standard loss: 0.0042839, inverse loss: 0.0008942
Valid-->> Epoch [194/3000], Standardized Loss: 0.0151380, Inverse Loss: 0.0031599
Training -->> Epoch: 195,(no reg loss)standard loss: 0.0043179, inverse loss: 0.0009013
Valid-->> Epoch [195/3000], Standardized Loss: 0.0151390, Inverse Loss: 0.0031601
Training -->> Epoch: 196,(no reg loss)standard loss: 0.0042963, inverse loss: 0.0008968
Valid-->> Epoch [196/3000], Standardized Loss: 0.0151257, Inverse Loss: 0.0031573
Training -->> Epoch: 197,(no reg loss)standard loss: 0.0042965, inverse loss: 0.0008968
Valid-->> Epoch [197/3000], Standardized Loss: 0.0151371, Inverse Loss: 0.0031597
Training -->> Epoch: 198,(no reg loss)standard loss: 0.0043275, inverse loss: 0.0009033
Valid-->> Epoch [198/3000], Standardized Loss: 0.0151218, Inverse Loss: 0.0031565
Training -->> Epoch: 199,(no reg loss)standard loss: 0.0042763, inverse loss: 0.0008926
Valid-->> Epoch [199/3000], Standardized Loss: 0.0151182, Inverse Loss: 0.0031558
Training -->> Epoch: 200,(no reg loss)standard loss: 0.0043279, inverse loss: 0.0009034
Valid-->> Epoch [200/3000], Standardized Loss: 0.0151432, Inverse Loss: 0.0031610
Training -->> Epoch: 201,(no reg loss)standard loss: 0.0043139, inverse loss: 0.0009005
Valid-->> Epoch [201/3000], Standardized Loss: 0.0151079, Inverse Loss: 0.0031536
Valid-->> Lowest loss found at epoch 201, loss: 0.0031536
Epoch 201, Masked params (inverse standardized): tensor([3.228862e+01, 4.430172e+01, 6.838417e-02, 2.632757e+01, 2.766804e+01,
        1.780975e+01, 2.142881e+01, 1.639029e+00, 1.066115e+02, 2.752664e+01,
        2.613343e+01, 4.461460e+01, 2.442688e+01, 1.503695e+01, 8.461627e+01,
        2.834272e+01, 5.667152e+00, 2.981454e+01, 1.196853e+01, 3.203605e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 202,(no reg loss)standard loss: 0.0042262, inverse loss: 0.0008822
Valid-->> Epoch [202/3000], Standardized Loss: 0.0150595, Inverse Loss: 0.0031435
Valid-->> Lowest loss found at epoch 202, loss: 0.0031435
Epoch 202, Masked params (inverse standardized): tensor([3.228927e+01, 4.430216e+01, 6.821060e-02, 2.632899e+01, 2.766943e+01,
        1.781005e+01, 2.142945e+01, 1.638845e+00, 1.066123e+02, 2.752807e+01,
        2.613482e+01, 4.461455e+01, 2.442797e+01, 1.503710e+01, 8.461678e+01,
        2.834397e+01, 5.666912e+00, 2.981553e+01, 1.196859e+01, 3.203677e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 203,(no reg loss)standard loss: 0.0042292, inverse loss: 0.0008828
Valid-->> Epoch [203/3000], Standardized Loss: 0.0150909, Inverse Loss: 0.0031501
Training -->> Epoch: 204,(no reg loss)standard loss: 0.0042953, inverse loss: 0.0008966
Valid-->> Epoch [204/3000], Standardized Loss: 0.0151367, Inverse Loss: 0.0031596
Training -->> Epoch: 205,(no reg loss)standard loss: 0.0043462, inverse loss: 0.0009072
Valid-->> Epoch [205/3000], Standardized Loss: 0.0151192, Inverse Loss: 0.0031560
Training -->> Epoch: 206,(no reg loss)standard loss: 0.0043002, inverse loss: 0.0008976
Valid-->> Epoch [206/3000], Standardized Loss: 0.0150828, Inverse Loss: 0.0031484
Training -->> Epoch: 207,(no reg loss)standard loss: 0.0042616, inverse loss: 0.0008895
Valid-->> Epoch [207/3000], Standardized Loss: 0.0150932, Inverse Loss: 0.0031505
Training -->> Epoch: 208,(no reg loss)standard loss: 0.0043190, inverse loss: 0.0009015
Valid-->> Epoch [208/3000], Standardized Loss: 0.0151193, Inverse Loss: 0.0031560
Training -->> Epoch: 209,(no reg loss)standard loss: 0.0042993, inverse loss: 0.0008974
Valid-->> Epoch [209/3000], Standardized Loss: 0.0150843, Inverse Loss: 0.0031487
Training -->> Epoch: 210,(no reg loss)standard loss: 0.0042873, inverse loss: 0.0008949
Valid-->> Epoch [210/3000], Standardized Loss: 0.0151114, Inverse Loss: 0.0031543
Training -->> Epoch: 211,(no reg loss)standard loss: 0.0043108, inverse loss: 0.0008998
Valid-->> Epoch [211/3000], Standardized Loss: 0.0151121, Inverse Loss: 0.0031545
Training -->> Epoch: 212,(no reg loss)standard loss: 0.0043605, inverse loss: 0.0009102
Valid-->> Epoch [212/3000], Standardized Loss: 0.0151205, Inverse Loss: 0.0031562
Training -->> Epoch: 213,(no reg loss)standard loss: 0.0042965, inverse loss: 0.0008968
Valid-->> Epoch [213/3000], Standardized Loss: 0.0150742, Inverse Loss: 0.0031466
Training -->> Epoch: 214,(no reg loss)standard loss: 0.0042845, inverse loss: 0.0008943
Valid-->> Epoch [214/3000], Standardized Loss: 0.0150965, Inverse Loss: 0.0031512
Training -->> Epoch: 215,(no reg loss)standard loss: 0.0042985, inverse loss: 0.0008973
Valid-->> Epoch [215/3000], Standardized Loss: 0.0150935, Inverse Loss: 0.0031506
Training -->> Epoch: 216,(no reg loss)standard loss: 0.0043132, inverse loss: 0.0009003
Valid-->> Epoch [216/3000], Standardized Loss: 0.0150863, Inverse Loss: 0.0031491
Training -->> Epoch: 217,(no reg loss)standard loss: 0.0043581, inverse loss: 0.0009097
Valid-->> Epoch [217/3000], Standardized Loss: 0.0150884, Inverse Loss: 0.0031495
Training -->> Epoch: 218,(no reg loss)standard loss: 0.0042912, inverse loss: 0.0008957
Valid-->> Epoch [218/3000], Standardized Loss: 0.0150498, Inverse Loss: 0.0031415
Valid-->> Lowest loss found at epoch 218, loss: 0.0031415
Epoch 218, Masked params (inverse standardized): tensor([3.228916e+01, 4.430127e+01, 6.840897e-02, 2.632864e+01, 2.766900e+01,
        1.781011e+01, 2.142931e+01, 1.639036e+00, 1.066124e+02, 2.752774e+01,
        2.613436e+01, 4.461481e+01, 2.442752e+01, 1.503722e+01, 8.461612e+01,
        2.834347e+01, 5.667131e+00, 2.981518e+01, 1.196873e+01, 3.203662e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 219,(no reg loss)standard loss: 0.0042935, inverse loss: 0.0008962
Valid-->> Epoch [219/3000], Standardized Loss: 0.0150751, Inverse Loss: 0.0031468
Training -->> Epoch: 220,(no reg loss)standard loss: 0.0042911, inverse loss: 0.0008957
Valid-->> Epoch [220/3000], Standardized Loss: 0.0150534, Inverse Loss: 0.0031422
Training -->> Epoch: 221,(no reg loss)standard loss: 0.0042505, inverse loss: 0.0008873
Valid-->> Epoch [221/3000], Standardized Loss: 0.0150418, Inverse Loss: 0.0031398
Valid-->> Lowest loss found at epoch 221, loss: 0.0031398
Epoch 221, Masked params (inverse standardized): tensor([3.228863e+01, 4.430114e+01, 6.789780e-02, 2.632894e+01, 2.766925e+01,
        1.780949e+01, 2.142878e+01, 1.638571e+00, 1.066118e+02, 2.752805e+01,
        2.613462e+01, 4.461429e+01, 2.442735e+01, 1.503662e+01, 8.461561e+01,
        2.834351e+01, 5.666670e+00, 2.981492e+01, 1.196814e+01, 3.203613e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 222,(no reg loss)standard loss: 0.0043126, inverse loss: 0.0009002
Valid-->> Epoch [222/3000], Standardized Loss: 0.0151430, Inverse Loss: 0.0031609
Training -->> Epoch: 223,(no reg loss)standard loss: 0.0043813, inverse loss: 0.0009145
Valid-->> Epoch [223/3000], Standardized Loss: 0.0150764, Inverse Loss: 0.0031470
Training -->> Epoch: 224,(no reg loss)standard loss: 0.0043040, inverse loss: 0.0008984
Valid-->> Epoch [224/3000], Standardized Loss: 0.0150619, Inverse Loss: 0.0031440
Training -->> Epoch: 225,(no reg loss)standard loss: 0.0042836, inverse loss: 0.0008942
Valid-->> Epoch [225/3000], Standardized Loss: 0.0150611, Inverse Loss: 0.0031438
Training -->> Epoch: 226,(no reg loss)standard loss: 0.0042872, inverse loss: 0.0008949
Valid-->> Epoch [226/3000], Standardized Loss: 0.0150370, Inverse Loss: 0.0031388
Valid-->> Lowest loss found at epoch 226, loss: 0.0031388
Epoch 226, Masked params (inverse standardized): tensor([3.228872e+01, 4.430138e+01, 6.821251e-02, 2.632887e+01, 2.766917e+01,
        1.780956e+01, 2.142885e+01, 1.638762e+00, 1.066123e+02, 2.752799e+01,
        2.613453e+01, 4.461439e+01, 2.442735e+01, 1.503668e+01, 8.461598e+01,
        2.834346e+01, 5.666864e+00, 2.981496e+01, 1.196829e+01, 3.203617e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 227,(no reg loss)standard loss: 0.0042966, inverse loss: 0.0008969
Valid-->> Epoch [227/3000], Standardized Loss: 0.0150695, Inverse Loss: 0.0031456
Training -->> Epoch: 228,(no reg loss)standard loss: 0.0043517, inverse loss: 0.0009084
Valid-->> Epoch [228/3000], Standardized Loss: 0.0150522, Inverse Loss: 0.0031420
Training -->> Epoch: 229,(no reg loss)standard loss: 0.0042812, inverse loss: 0.0008936
Valid-->> Epoch [229/3000], Standardized Loss: 0.0150141, Inverse Loss: 0.0031340
Valid-->> Lowest loss found at epoch 229, loss: 0.0031340
Epoch 229, Masked params (inverse standardized): tensor([3.228936e+01, 4.430089e+01, 6.837654e-02, 2.632935e+01, 2.766964e+01,
        1.781022e+01, 2.142948e+01, 1.639055e+00, 1.066116e+02, 2.752848e+01,
        2.613499e+01, 4.461490e+01, 2.442780e+01, 1.503733e+01, 8.461568e+01,
        2.834388e+01, 5.667147e+00, 2.981544e+01, 1.196878e+01, 3.203682e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 230,(no reg loss)standard loss: 0.0043133, inverse loss: 0.0009003
Valid-->> Epoch [230/3000], Standardized Loss: 0.0150611, Inverse Loss: 0.0031438
Training -->> Epoch: 231,(no reg loss)standard loss: 0.0042831, inverse loss: 0.0008940
Valid-->> Epoch [231/3000], Standardized Loss: 0.0150384, Inverse Loss: 0.0031391
Training -->> Epoch: 232,(no reg loss)standard loss: 0.0042725, inverse loss: 0.0008918
Valid-->> Epoch [232/3000], Standardized Loss: 0.0150197, Inverse Loss: 0.0031352
Training -->> Epoch: 233,(no reg loss)standard loss: 0.0043272, inverse loss: 0.0009033
Valid-->> Epoch [233/3000], Standardized Loss: 0.0150774, Inverse Loss: 0.0031472
Training -->> Epoch: 234,(no reg loss)standard loss: 0.0043043, inverse loss: 0.0008985
Valid-->> Epoch [234/3000], Standardized Loss: 0.0150418, Inverse Loss: 0.0031398
Training -->> Epoch: 235,(no reg loss)standard loss: 0.0043367, inverse loss: 0.0009052
Valid-->> Epoch [235/3000], Standardized Loss: 0.0150585, Inverse Loss: 0.0031433
Training -->> Epoch: 236,(no reg loss)standard loss: 0.0042874, inverse loss: 0.0008950
Valid-->> Epoch [236/3000], Standardized Loss: 0.0150071, Inverse Loss: 0.0031326
Valid-->> Lowest loss found at epoch 236, loss: 0.0031326
Epoch 236, Masked params (inverse standardized): tensor([3.228917e+01, 4.430135e+01, 6.831932e-02, 2.632935e+01, 2.766960e+01,
        1.781000e+01, 2.142928e+01, 1.638966e+00, 1.066112e+02, 2.752851e+01,
        2.613493e+01, 4.461472e+01, 2.442764e+01, 1.503713e+01, 8.461575e+01,
        2.834375e+01, 5.667076e+00, 2.981530e+01, 1.196862e+01, 3.203666e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 237,(no reg loss)standard loss: 0.0043264, inverse loss: 0.0009031
Valid-->> Epoch [237/3000], Standardized Loss: 0.0150418, Inverse Loss: 0.0031398
Training -->> Epoch: 238,(no reg loss)standard loss: 0.0042933, inverse loss: 0.0008962
Valid-->> Epoch [238/3000], Standardized Loss: 0.0150145, Inverse Loss: 0.0031341
Training -->> Epoch: 239,(no reg loss)standard loss: 0.0042838, inverse loss: 0.0008942
Valid-->> Epoch [239/3000], Standardized Loss: 0.0150216, Inverse Loss: 0.0031356
Training -->> Epoch: 240,(no reg loss)standard loss: 0.0043446, inverse loss: 0.0009069
Valid-->> Epoch [240/3000], Standardized Loss: 0.0150307, Inverse Loss: 0.0031375
Training -->> Epoch: 241,(no reg loss)standard loss: 0.0042947, inverse loss: 0.0008965
Valid-->> Epoch [241/3000], Standardized Loss: 0.0150310, Inverse Loss: 0.0031375
Training -->> Epoch: 242,(no reg loss)standard loss: 0.0043468, inverse loss: 0.0009074
Valid-->> Epoch [242/3000], Standardized Loss: 0.0150318, Inverse Loss: 0.0031377
Training -->> Epoch: 243,(no reg loss)standard loss: 0.0042794, inverse loss: 0.0008933
Valid-->> Epoch [243/3000], Standardized Loss: 0.0150108, Inverse Loss: 0.0031333
Training -->> Epoch: 244,(no reg loss)standard loss: 0.0043037, inverse loss: 0.0008984
Valid-->> Epoch [244/3000], Standardized Loss: 0.0150193, Inverse Loss: 0.0031351
Training -->> Epoch: 245,(no reg loss)standard loss: 0.0043987, inverse loss: 0.0009182
Valid-->> Epoch [245/3000], Standardized Loss: 0.0150754, Inverse Loss: 0.0031468
Training -->> Epoch: 246,(no reg loss)standard loss: 0.0043601, inverse loss: 0.0009101
Valid-->> Epoch [246/3000], Standardized Loss: 0.0150007, Inverse Loss: 0.0031312
Valid-->> Lowest loss found at epoch 246, loss: 0.0031312
Epoch 246, Masked params (inverse standardized): tensor([3.228958e+01, 4.430214e+01, 6.870651e-02, 2.632897e+01, 2.766929e+01,
        1.781063e+01, 2.142969e+01, 1.639502e+00, 1.066117e+02, 2.752821e+01,
        2.613465e+01, 4.461541e+01, 2.442772e+01, 1.503779e+01, 8.461667e+01,
        2.834371e+01, 5.667580e+00, 2.981549e+01, 1.196924e+01, 3.203709e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 247,(no reg loss)standard loss: 0.0042186, inverse loss: 0.0008806
Valid-->> Epoch [247/3000], Standardized Loss: 0.0149652, Inverse Loss: 0.0031238
Valid-->> Lowest loss found at epoch 247, loss: 0.0031238
Epoch 247, Masked params (inverse standardized): tensor([3.228938e+01, 4.430103e+01, 6.743622e-02, 2.633021e+01, 2.767047e+01,
        1.780968e+01, 2.142946e+01, 1.638138e+00, 1.066125e+02, 2.752943e+01,
        2.613581e+01, 4.461410e+01, 2.442833e+01, 1.503661e+01, 8.461587e+01,
        2.834461e+01, 5.666216e+00, 2.981590e+01, 1.196791e+01, 3.203687e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 248,(no reg loss)standard loss: 0.0043252, inverse loss: 0.0009028
Valid-->> Epoch [248/3000], Standardized Loss: 0.0150919, Inverse Loss: 0.0031503
Training -->> Epoch: 249,(no reg loss)standard loss: 0.0043865, inverse loss: 0.0009156
Valid-->> Epoch [249/3000], Standardized Loss: 0.0149828, Inverse Loss: 0.0031275
Training -->> Epoch: 250,(no reg loss)standard loss: 0.0042855, inverse loss: 0.0008945
Valid-->> Epoch [250/3000], Standardized Loss: 0.0150160, Inverse Loss: 0.0031344
Training -->> Epoch: 251,(no reg loss)standard loss: 0.0043072, inverse loss: 0.0008991
Valid-->> Epoch [251/3000], Standardized Loss: 0.0150106, Inverse Loss: 0.0031333
Training -->> Epoch: 252,(no reg loss)standard loss: 0.0044171, inverse loss: 0.0009220
Valid-->> Epoch [252/3000], Standardized Loss: 0.0150691, Inverse Loss: 0.0031455
Training -->> Epoch: 253,(no reg loss)standard loss: 0.0042939, inverse loss: 0.0008963
Valid-->> Epoch [253/3000], Standardized Loss: 0.0149600, Inverse Loss: 0.0031227
Valid-->> Lowest loss found at epoch 253, loss: 0.0031227
Epoch 253, Masked params (inverse standardized): tensor([3.228989e+01, 4.430086e+01, 6.842232e-02, 2.633003e+01, 2.767026e+01,
        1.781063e+01, 2.142996e+01, 1.639048e+00, 1.066118e+02, 2.752921e+01,
        2.613555e+01, 4.461509e+01, 2.442825e+01, 1.503764e+01, 8.461550e+01,
        2.834435e+01, 5.667095e+00, 2.981595e+01, 1.196902e+01, 3.203728e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 254,(no reg loss)standard loss: 0.0042896, inverse loss: 0.0008954
Valid-->> Epoch [254/3000], Standardized Loss: 0.0149957, Inverse Loss: 0.0031302
Training -->> Epoch: 255,(no reg loss)standard loss: 0.0042964, inverse loss: 0.0008968
Valid-->> Epoch [255/3000], Standardized Loss: 0.0150367, Inverse Loss: 0.0031387
Training -->> Epoch: 256,(no reg loss)standard loss: 0.0043936, inverse loss: 0.0009171
Valid-->> Epoch [256/3000], Standardized Loss: 0.0150094, Inverse Loss: 0.0031330
Training -->> Epoch: 257,(no reg loss)standard loss: 0.0043289, inverse loss: 0.0009036
Valid-->> Epoch [257/3000], Standardized Loss: 0.0149824, Inverse Loss: 0.0031274
Training -->> Epoch: 258,(no reg loss)standard loss: 0.0042804, inverse loss: 0.0008935
Valid-->> Epoch [258/3000], Standardized Loss: 0.0150053, Inverse Loss: 0.0031322
Training -->> Epoch: 259,(no reg loss)standard loss: 0.0043397, inverse loss: 0.0009059
Valid-->> Epoch [259/3000], Standardized Loss: 0.0150165, Inverse Loss: 0.0031345
Training -->> Epoch: 260,(no reg loss)standard loss: 0.0043073, inverse loss: 0.0008991
Valid-->> Epoch [260/3000], Standardized Loss: 0.0149781, Inverse Loss: 0.0031265
Training -->> Epoch: 261,(no reg loss)standard loss: 0.0043282, inverse loss: 0.0009035
Valid-->> Epoch [261/3000], Standardized Loss: 0.0150221, Inverse Loss: 0.0031357
Training -->> Epoch: 262,(no reg loss)standard loss: 0.0043395, inverse loss: 0.0009058
Valid-->> Epoch [262/3000], Standardized Loss: 0.0149565, Inverse Loss: 0.0031220
Valid-->> Lowest loss found at epoch 262, loss: 0.0031220
Epoch 262, Masked params (inverse standardized): tensor([3.228944e+01, 4.430142e+01, 6.880760e-02, 2.632989e+01, 2.767008e+01,
        1.781037e+01, 2.142952e+01, 1.639416e+00, 1.066125e+02, 2.752912e+01,
        2.613537e+01, 4.461520e+01, 2.442783e+01, 1.503750e+01, 8.461622e+01,
        2.834406e+01, 5.667433e+00, 2.981552e+01, 1.196907e+01, 3.203689e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 263,(no reg loss)standard loss: 0.0042595, inverse loss: 0.0008891
Valid-->> Epoch [263/3000], Standardized Loss: 0.0149869, Inverse Loss: 0.0031283
Training -->> Epoch: 264,(no reg loss)standard loss: 0.0044096, inverse loss: 0.0009205
Valid-->> Epoch [264/3000], Standardized Loss: 0.0150082, Inverse Loss: 0.0031328
Training -->> Epoch: 265,(no reg loss)standard loss: 0.0042929, inverse loss: 0.0008961
Valid-->> Epoch [265/3000], Standardized Loss: 0.0149349, Inverse Loss: 0.0031175
Valid-->> Lowest loss found at epoch 265, loss: 0.0031175
Epoch 265, Masked params (inverse standardized): tensor([3.228979e+01, 4.430138e+01, 6.834984e-02, 2.633040e+01, 2.767057e+01,
        1.781056e+01, 2.142985e+01, 1.639023e+00, 1.066118e+02, 2.752963e+01,
        2.613586e+01, 4.461510e+01, 2.442824e+01, 1.503763e+01, 8.461581e+01,
        2.834450e+01, 5.667051e+00, 2.981591e+01, 1.196900e+01, 3.203726e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 266,(no reg loss)standard loss: 0.0042720, inverse loss: 0.0008917
Valid-->> Epoch [266/3000], Standardized Loss: 0.0149312, Inverse Loss: 0.0031167
Valid-->> Lowest loss found at epoch 266, loss: 0.0031167
Epoch 266, Masked params (inverse standardized): tensor([3.228916e+01, 4.430235e+01, 6.791878e-02, 2.633066e+01, 2.767080e+01,
        1.780965e+01, 2.142923e+01, 1.638552e+00, 1.066116e+02, 2.752990e+01,
        2.613608e+01, 4.461437e+01, 2.442811e+01, 1.503665e+01, 8.461660e+01,
        2.834457e+01, 5.666729e+00, 2.981565e+01, 1.196810e+01, 3.203664e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 267,(no reg loss)standard loss: 0.0042180, inverse loss: 0.0008805
Valid-->> Epoch [267/3000], Standardized Loss: 0.0149261, Inverse Loss: 0.0031156
Valid-->> Lowest loss found at epoch 267, loss: 0.0031156
Epoch 267, Masked params (inverse standardized): tensor([3.228856e+01, 4.430154e+01, 6.761551e-02, 2.633099e+01, 2.767105e+01,
        1.780909e+01, 2.142862e+01, 1.638168e+00, 1.066123e+02, 2.753024e+01,
        2.613633e+01, 4.461403e+01, 2.442784e+01, 1.503619e+01, 8.461623e+01,
        2.834458e+01, 5.666323e+00, 2.981528e+01, 1.196773e+01, 3.203603e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 268,(no reg loss)standard loss: 0.0043081, inverse loss: 0.0008993
Valid-->> Epoch [268/3000], Standardized Loss: 0.0149525, Inverse Loss: 0.0031212
Training -->> Epoch: 269,(no reg loss)standard loss: 0.0042679, inverse loss: 0.0008909
Valid-->> Epoch [269/3000], Standardized Loss: 0.0149670, Inverse Loss: 0.0031242
Training -->> Epoch: 270,(no reg loss)standard loss: 0.0044053, inverse loss: 0.0009196
Valid-->> Epoch [270/3000], Standardized Loss: 0.0149678, Inverse Loss: 0.0031244
Training -->> Epoch: 271,(no reg loss)standard loss: 0.0042704, inverse loss: 0.0008914
Valid-->> Epoch [271/3000], Standardized Loss: 0.0149586, Inverse Loss: 0.0031224
Training -->> Epoch: 272,(no reg loss)standard loss: 0.0043122, inverse loss: 0.0009001
Valid-->> Epoch [272/3000], Standardized Loss: 0.0149464, Inverse Loss: 0.0031199
Training -->> Epoch: 273,(no reg loss)standard loss: 0.0043275, inverse loss: 0.0009033
Valid-->> Epoch [273/3000], Standardized Loss: 0.0149950, Inverse Loss: 0.0031300
Training -->> Epoch: 274,(no reg loss)standard loss: 0.0043089, inverse loss: 0.0008994
Valid-->> Epoch [274/3000], Standardized Loss: 0.0149239, Inverse Loss: 0.0031152
Valid-->> Lowest loss found at epoch 274, loss: 0.0031152
Epoch 274, Masked params (inverse standardized): tensor([3.228936e+01, 4.430231e+01, 6.834793e-02, 2.633060e+01, 2.767062e+01,
        1.781020e+01, 2.142943e+01, 1.639002e+00, 1.066125e+02, 2.752984e+01,
        2.613591e+01, 4.461495e+01, 2.442791e+01, 1.503730e+01, 8.461717e+01,
        2.834428e+01, 5.667164e+00, 2.981554e+01, 1.196877e+01, 3.203681e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 275,(no reg loss)standard loss: 0.0042727, inverse loss: 0.0008919
Valid-->> Epoch [275/3000], Standardized Loss: 0.0149273, Inverse Loss: 0.0031159
Training -->> Epoch: 276,(no reg loss)standard loss: 0.0042809, inverse loss: 0.0008936
Valid-->> Epoch [276/3000], Standardized Loss: 0.0149407, Inverse Loss: 0.0031187
Training -->> Epoch: 277,(no reg loss)standard loss: 0.0042766, inverse loss: 0.0008927
Valid-->> Epoch [277/3000], Standardized Loss: 0.0149612, Inverse Loss: 0.0031230
Training -->> Epoch: 278,(no reg loss)standard loss: 0.0043082, inverse loss: 0.0008993
Valid-->> Epoch [278/3000], Standardized Loss: 0.0149868, Inverse Loss: 0.0031283
Training -->> Epoch: 279,(no reg loss)standard loss: 0.0044504, inverse loss: 0.0009290
Valid-->> Epoch [279/3000], Standardized Loss: 0.0149491, Inverse Loss: 0.0031205
Training -->> Epoch: 280,(no reg loss)standard loss: 0.0042896, inverse loss: 0.0008954
Valid-->> Epoch [280/3000], Standardized Loss: 0.0149184, Inverse Loss: 0.0031140
Valid-->> Lowest loss found at epoch 280, loss: 0.0031140
Epoch 280, Masked params (inverse standardized): tensor([3.228951e+01, 4.430211e+01, 6.746483e-02, 2.633058e+01, 2.767057e+01,
        1.780989e+01, 2.142956e+01, 1.638144e+00, 1.066124e+02, 2.752982e+01,
        2.613582e+01, 4.461430e+01, 2.442804e+01, 1.503678e+01, 8.461646e+01,
        2.834428e+01, 5.666258e+00, 2.981571e+01, 1.196809e+01, 3.203691e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 281,(no reg loss)standard loss: 0.0042604, inverse loss: 0.0008893
Valid-->> Epoch [281/3000], Standardized Loss: 0.0149199, Inverse Loss: 0.0031143
Training -->> Epoch: 282,(no reg loss)standard loss: 0.0044335, inverse loss: 0.0009254
Valid-->> Epoch [282/3000], Standardized Loss: 0.0149528, Inverse Loss: 0.0031212
Training -->> Epoch: 283,(no reg loss)standard loss: 0.0042725, inverse loss: 0.0008918
Valid-->> Epoch [283/3000], Standardized Loss: 0.0148972, Inverse Loss: 0.0031096
Valid-->> Lowest loss found at epoch 283, loss: 0.0031096
Epoch 283, Masked params (inverse standardized): tensor([3.228969e+01, 4.430189e+01, 6.766701e-02, 2.633107e+01, 2.767106e+01,
        1.781025e+01, 2.142974e+01, 1.638388e+00, 1.066118e+02, 2.753032e+01,
        2.613632e+01, 4.461460e+01, 2.442828e+01, 1.503721e+01, 8.461618e+01,
        2.834469e+01, 5.666510e+00, 2.981590e+01, 1.196843e+01, 3.203719e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 284,(no reg loss)standard loss: 0.0042690, inverse loss: 0.0008911
Valid-->> Epoch [284/3000], Standardized Loss: 0.0148710, Inverse Loss: 0.0031041
Valid-->> Lowest loss found at epoch 284, loss: 0.0031041
Epoch 284, Masked params (inverse standardized): tensor([3.228951e+01, 4.430055e+01, 6.834221e-02, 2.633199e+01, 2.767193e+01,
        1.780984e+01, 2.142956e+01, 1.638941e+00, 1.066112e+02, 2.753123e+01,
        2.613720e+01, 4.461472e+01, 2.442870e+01, 1.503688e+01, 8.461519e+01,
        2.834534e+01, 5.667152e+00, 2.981619e+01, 1.196839e+01, 3.203700e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 285,(no reg loss)standard loss: 0.0043061, inverse loss: 0.0008989
Valid-->> Epoch [285/3000], Standardized Loss: 0.0149239, Inverse Loss: 0.0031152
Training -->> Epoch: 286,(no reg loss)standard loss: 0.0043589, inverse loss: 0.0009099
Valid-->> Epoch [286/3000], Standardized Loss: 0.0149415, Inverse Loss: 0.0031189
Training -->> Epoch: 287,(no reg loss)standard loss: 0.0043464, inverse loss: 0.0009073
Valid-->> Epoch [287/3000], Standardized Loss: 0.0148974, Inverse Loss: 0.0031097
Training -->> Epoch: 288,(no reg loss)standard loss: 0.0043282, inverse loss: 0.0009035
Valid-->> Epoch [288/3000], Standardized Loss: 0.0149389, Inverse Loss: 0.0031183
Training -->> Epoch: 289,(no reg loss)standard loss: 0.0042508, inverse loss: 0.0008873
Valid-->> Epoch [289/3000], Standardized Loss: 0.0149291, Inverse Loss: 0.0031163
Training -->> Epoch: 290,(no reg loss)standard loss: 0.0044168, inverse loss: 0.0009220
Valid-->> Epoch [290/3000], Standardized Loss: 0.0149254, Inverse Loss: 0.0031155
Training -->> Epoch: 291,(no reg loss)standard loss: 0.0043572, inverse loss: 0.0009095
Valid-->> Epoch [291/3000], Standardized Loss: 0.0148894, Inverse Loss: 0.0031080
Training -->> Epoch: 292,(no reg loss)standard loss: 0.0042250, inverse loss: 0.0008819
Valid-->> Epoch [292/3000], Standardized Loss: 0.0148929, Inverse Loss: 0.0031087
Training -->> Epoch: 293,(no reg loss)standard loss: 0.0043254, inverse loss: 0.0009029
Valid-->> Epoch [293/3000], Standardized Loss: 0.0148876, Inverse Loss: 0.0031076
Training -->> Epoch: 294,(no reg loss)standard loss: 0.0042576, inverse loss: 0.0008887
Valid-->> Epoch [294/3000], Standardized Loss: 0.0149098, Inverse Loss: 0.0031123
Training -->> Epoch: 295,(no reg loss)standard loss: 0.0043203, inverse loss: 0.0009018
Valid-->> Epoch [295/3000], Standardized Loss: 0.0149109, Inverse Loss: 0.0031125
Training -->> Epoch: 296,(no reg loss)standard loss: 0.0042988, inverse loss: 0.0008973
Valid-->> Epoch [296/3000], Standardized Loss: 0.0149174, Inverse Loss: 0.0031138
Training -->> Epoch: 297,(no reg loss)standard loss: 0.0043567, inverse loss: 0.0009094
Valid-->> Epoch [297/3000], Standardized Loss: 0.0149416, Inverse Loss: 0.0031189
Training -->> Epoch: 298,(no reg loss)standard loss: 0.0043517, inverse loss: 0.0009084
Valid-->> Epoch [298/3000], Standardized Loss: 0.0148545, Inverse Loss: 0.0031007
Valid-->> Lowest loss found at epoch 298, loss: 0.0031007
Epoch 298, Masked params (inverse standardized): tensor([3.229037e+01, 4.430038e+01, 6.900024e-02, 2.633171e+01, 2.767162e+01,
        1.781113e+01, 2.143041e+01, 1.639639e+00, 1.066131e+02, 2.753100e+01,
        2.613688e+01, 4.461575e+01, 2.442878e+01, 1.503818e+01, 8.461588e+01,
        2.834511e+01, 5.667681e+00, 2.981649e+01, 1.196962e+01, 3.203782e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 299,(no reg loss)standard loss: 0.0043022, inverse loss: 0.0008980
Valid-->> Epoch [299/3000], Standardized Loss: 0.0148951, Inverse Loss: 0.0031092
Training -->> Epoch: 300,(no reg loss)standard loss: 0.0043443, inverse loss: 0.0009068
Valid-->> Epoch [300/3000], Standardized Loss: 0.0149188, Inverse Loss: 0.0031141
Training -->> Epoch: 301,(no reg loss)standard loss: 0.0043026, inverse loss: 0.0008981
Valid-->> Epoch [301/3000], Standardized Loss: 0.0149025, Inverse Loss: 0.0031107
Training -->> Epoch: 302,(no reg loss)standard loss: 0.0042517, inverse loss: 0.0008875
Valid-->> Epoch [302/3000], Standardized Loss: 0.0148830, Inverse Loss: 0.0031067
Training -->> Epoch: 303,(no reg loss)standard loss: 0.0043116, inverse loss: 0.0009000
Valid-->> Epoch [303/3000], Standardized Loss: 0.0148616, Inverse Loss: 0.0031022
Training -->> Epoch: 304,(no reg loss)standard loss: 0.0042570, inverse loss: 0.0008886
Valid-->> Epoch [304/3000], Standardized Loss: 0.0149022, Inverse Loss: 0.0031107
Training -->> Epoch: 305,(no reg loss)standard loss: 0.0043907, inverse loss: 0.0009165
Valid-->> Epoch [305/3000], Standardized Loss: 0.0148940, Inverse Loss: 0.0031090
Training -->> Epoch: 306,(no reg loss)standard loss: 0.0042992, inverse loss: 0.0008974
Valid-->> Epoch [306/3000], Standardized Loss: 0.0148769, Inverse Loss: 0.0031054
Training -->> Epoch: 307,(no reg loss)standard loss: 0.0042711, inverse loss: 0.0008915
Valid-->> Epoch [307/3000], Standardized Loss: 0.0148604, Inverse Loss: 0.0031019
Training -->> Epoch: 308,(no reg loss)standard loss: 0.0043467, inverse loss: 0.0009073
Valid-->> Epoch [308/3000], Standardized Loss: 0.0149217, Inverse Loss: 0.0031147
Training -->> Epoch: 309,(no reg loss)standard loss: 0.0043502, inverse loss: 0.0009080
Valid-->> Epoch [309/3000], Standardized Loss: 0.0148823, Inverse Loss: 0.0031065
Training -->> Epoch: 310,(no reg loss)standard loss: 0.0042850, inverse loss: 0.0008944
Valid-->> Epoch [310/3000], Standardized Loss: 0.0148431, Inverse Loss: 0.0030983
Valid-->> Lowest loss found at epoch 310, loss: 0.0030983
Epoch 310, Masked params (inverse standardized): tensor([3.228944e+01, 4.430085e+01, 6.787682e-02, 2.633194e+01, 2.767174e+01,
        1.780974e+01, 2.142944e+01, 1.638487e+00, 1.066121e+02, 2.753123e+01,
        2.613698e+01, 4.461446e+01, 2.442832e+01, 1.503673e+01, 8.461550e+01,
        2.834492e+01, 5.666569e+00, 2.981592e+01, 1.196813e+01, 3.203695e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 311,(no reg loss)standard loss: 0.0043293, inverse loss: 0.0009037
Valid-->> Epoch [311/3000], Standardized Loss: 0.0148740, Inverse Loss: 0.0031048
Training -->> Epoch: 312,(no reg loss)standard loss: 0.0043103, inverse loss: 0.0008997
Valid-->> Epoch [312/3000], Standardized Loss: 0.0148763, Inverse Loss: 0.0031053
Training -->> Epoch: 313,(no reg loss)standard loss: 0.0042645, inverse loss: 0.0008902
Valid-->> Epoch [313/3000], Standardized Loss: 0.0148674, Inverse Loss: 0.0031034
Training -->> Epoch: 314,(no reg loss)standard loss: 0.0043029, inverse loss: 0.0008982
Valid-->> Epoch [314/3000], Standardized Loss: 0.0148732, Inverse Loss: 0.0031046
Training -->> Epoch: 315,(no reg loss)standard loss: 0.0043149, inverse loss: 0.0009007
Valid-->> Epoch [315/3000], Standardized Loss: 0.0148872, Inverse Loss: 0.0031075
Training -->> Epoch: 316,(no reg loss)standard loss: 0.0043013, inverse loss: 0.0008978
Valid-->> Epoch [316/3000], Standardized Loss: 0.0148589, Inverse Loss: 0.0031016
Training -->> Epoch: 317,(no reg loss)standard loss: 0.0043605, inverse loss: 0.0009102
Valid-->> Epoch [317/3000], Standardized Loss: 0.0148888, Inverse Loss: 0.0031079
Training -->> Epoch: 318,(no reg loss)standard loss: 0.0043450, inverse loss: 0.0009070
Valid-->> Epoch [318/3000], Standardized Loss: 0.0147938, Inverse Loss: 0.0030880
Valid-->> Lowest loss found at epoch 318, loss: 0.0030880
Epoch 318, Masked params (inverse standardized): tensor([3.229083e+01, 4.430106e+01, 6.898499e-02, 2.633293e+01, 2.767270e+01,
        1.781143e+01, 2.143085e+01, 1.639713e+00, 1.066110e+02, 2.753223e+01,
        2.613795e+01, 4.461589e+01, 2.442943e+01, 1.503847e+01, 8.461523e+01,
        2.834593e+01, 5.667791e+00, 2.981710e+01, 1.196979e+01, 3.203828e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 319,(no reg loss)standard loss: 0.0042599, inverse loss: 0.0008892
Valid-->> Epoch [319/3000], Standardized Loss: 0.0148863, Inverse Loss: 0.0031073
Training -->> Epoch: 320,(no reg loss)standard loss: 0.0043812, inverse loss: 0.0009145
Valid-->> Epoch [320/3000], Standardized Loss: 0.0148326, Inverse Loss: 0.0030961
Training -->> Epoch: 321,(no reg loss)standard loss: 0.0044013, inverse loss: 0.0009187
Valid-->> Epoch [321/3000], Standardized Loss: 0.0148401, Inverse Loss: 0.0030977
Training -->> Epoch: 322,(no reg loss)standard loss: 0.0042054, inverse loss: 0.0008778
Valid-->> Epoch [322/3000], Standardized Loss: 0.0148541, Inverse Loss: 0.0031006
Training -->> Epoch: 323,(no reg loss)standard loss: 0.0044344, inverse loss: 0.0009256
Valid-->> Epoch [323/3000], Standardized Loss: 0.0148320, Inverse Loss: 0.0030960
Training -->> Epoch: 324,(no reg loss)standard loss: 0.0043206, inverse loss: 0.0009019
Valid-->> Epoch [324/3000], Standardized Loss: 0.0149109, Inverse Loss: 0.0031125
Training -->> Epoch: 325,(no reg loss)standard loss: 0.0043580, inverse loss: 0.0009097
Valid-->> Epoch [325/3000], Standardized Loss: 0.0148074, Inverse Loss: 0.0030909
Training -->> Epoch: 326,(no reg loss)standard loss: 0.0044481, inverse loss: 0.0009285
Valid-->> Epoch [326/3000], Standardized Loss: 0.0148246, Inverse Loss: 0.0030945
Training -->> Epoch: 327,(no reg loss)standard loss: 0.0042701, inverse loss: 0.0008913
Valid-->> Epoch [327/3000], Standardized Loss: 0.0148561, Inverse Loss: 0.0031010
Training -->> Epoch: 328,(no reg loss)standard loss: 0.0043770, inverse loss: 0.0009136
Valid-->> Epoch [328/3000], Standardized Loss: 0.0148326, Inverse Loss: 0.0030961
Training -->> Epoch: 329,(no reg loss)standard loss: 0.0043362, inverse loss: 0.0009051
Valid-->> Epoch [329/3000], Standardized Loss: 0.0148461, Inverse Loss: 0.0030990
Training -->> Epoch: 330,(no reg loss)standard loss: 0.0043343, inverse loss: 0.0009047
Valid-->> Epoch [330/3000], Standardized Loss: 0.0148220, Inverse Loss: 0.0030939
Training -->> Epoch: 331,(no reg loss)standard loss: 0.0044329, inverse loss: 0.0009253
Valid-->> Epoch [331/3000], Standardized Loss: 0.0148524, Inverse Loss: 0.0031003
Training -->> Epoch: 332,(no reg loss)standard loss: 0.0043016, inverse loss: 0.0008979
Valid-->> Epoch [332/3000], Standardized Loss: 0.0148039, Inverse Loss: 0.0030901
Training -->> Epoch: 333,(no reg loss)standard loss: 0.0043164, inverse loss: 0.0009010
Valid-->> Epoch [333/3000], Standardized Loss: 0.0148453, Inverse Loss: 0.0030988
Training -->> Epoch: 334,(no reg loss)standard loss: 0.0043268, inverse loss: 0.0009032
Valid-->> Epoch [334/3000], Standardized Loss: 0.0147979, Inverse Loss: 0.0030889
Training -->> Epoch: 335,(no reg loss)standard loss: 0.0043762, inverse loss: 0.0009135
Valid-->> Epoch [335/3000], Standardized Loss: 0.0148161, Inverse Loss: 0.0030927
Training -->> Epoch: 336,(no reg loss)standard loss: 0.0042392, inverse loss: 0.0008849
Valid-->> Epoch [336/3000], Standardized Loss: 0.0148515, Inverse Loss: 0.0031001
Training -->> Epoch: 337,(no reg loss)standard loss: 0.0043210, inverse loss: 0.0009020
Valid-->> Epoch [337/3000], Standardized Loss: 0.0148520, Inverse Loss: 0.0031002
Training -->> Epoch: 338,(no reg loss)standard loss: 0.0044335, inverse loss: 0.0009254
Valid-->> Epoch [338/3000], Standardized Loss: 0.0148485, Inverse Loss: 0.0030994
Training -->> Epoch: 339,(no reg loss)standard loss: 0.0042643, inverse loss: 0.0008901
Valid-->> Epoch [339/3000], Standardized Loss: 0.0147545, Inverse Loss: 0.0030798
Valid-->> Lowest loss found at epoch 339, loss: 0.0030798
Epoch 339, Masked params (inverse standardized): tensor([3.229040e+01, 4.430050e+01, 6.724930e-02, 2.633361e+01, 2.767326e+01,
        1.781014e+01, 2.143038e+01, 1.637991e+00, 1.066115e+02, 2.753293e+01,
        2.613849e+01, 4.461422e+01, 2.442947e+01, 1.503694e+01, 8.461519e+01,
        2.834616e+01, 5.666161e+00, 2.981706e+01, 1.196805e+01, 3.203790e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 340,(no reg loss)standard loss: 0.0043308, inverse loss: 0.0009040
Valid-->> Epoch [340/3000], Standardized Loss: 0.0148109, Inverse Loss: 0.0030916
Training -->> Epoch: 341,(no reg loss)standard loss: 0.0043345, inverse loss: 0.0009048
Valid-->> Epoch [341/3000], Standardized Loss: 0.0148104, Inverse Loss: 0.0030915
Training -->> Epoch: 342,(no reg loss)standard loss: 0.0043816, inverse loss: 0.0009146
Valid-->> Epoch [342/3000], Standardized Loss: 0.0148284, Inverse Loss: 0.0030953
Training -->> Epoch: 343,(no reg loss)standard loss: 0.0042525, inverse loss: 0.0008877
Valid-->> Epoch [343/3000], Standardized Loss: 0.0148438, Inverse Loss: 0.0030985
Training -->> Epoch: 344,(no reg loss)standard loss: 0.0044459, inverse loss: 0.0009280
Valid-->> Epoch [344/3000], Standardized Loss: 0.0147731, Inverse Loss: 0.0030837
Training -->> Epoch: 345,(no reg loss)standard loss: 0.0042325, inverse loss: 0.0008835
Valid-->> Epoch [345/3000], Standardized Loss: 0.0148605, Inverse Loss: 0.0031020
Training -->> Epoch: 346,(no reg loss)standard loss: 0.0043160, inverse loss: 0.0009009
Valid-->> Epoch [346/3000], Standardized Loss: 0.0148245, Inverse Loss: 0.0030944
Training -->> Epoch: 347,(no reg loss)standard loss: 0.0043938, inverse loss: 0.0009171
Valid-->> Epoch [347/3000], Standardized Loss: 0.0148453, Inverse Loss: 0.0030988
Training -->> Epoch: 348,(no reg loss)standard loss: 0.0042815, inverse loss: 0.0008937
Valid-->> Epoch [348/3000], Standardized Loss: 0.0147653, Inverse Loss: 0.0030821
Training -->> Epoch: 349,(no reg loss)standard loss: 0.0043099, inverse loss: 0.0008996
Valid-->> Epoch [349/3000], Standardized Loss: 0.0147726, Inverse Loss: 0.0030836
Training -->> Epoch: 350,(no reg loss)standard loss: 0.0043447, inverse loss: 0.0009069
Valid-->> Epoch [350/3000], Standardized Loss: 0.0147704, Inverse Loss: 0.0030832
Training -->> Epoch: 351,(no reg loss)standard loss: 0.0042384, inverse loss: 0.0008847
Valid-->> Epoch [351/3000], Standardized Loss: 0.0147732, Inverse Loss: 0.0030837
Training -->> Epoch: 352,(no reg loss)standard loss: 0.0043957, inverse loss: 0.0009175
Valid-->> Epoch [352/3000], Standardized Loss: 0.0147493, Inverse Loss: 0.0030787
Valid-->> Lowest loss found at epoch 352, loss: 0.0030787
Epoch 352, Masked params (inverse standardized): tensor([3.228986e+01, 4.430187e+01, 7.042122e-02, 2.633335e+01, 2.767288e+01,
        1.781126e+01, 2.142987e+01, 1.641108e+00, 1.066121e+02, 2.753275e+01,
        2.613811e+01, 4.461708e+01, 2.442844e+01, 1.503873e+01, 8.461665e+01,
        2.834539e+01, 5.669052e+00, 2.981600e+01, 1.197056e+01, 3.203732e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 353,(no reg loss)standard loss: 0.0042227, inverse loss: 0.0008814
Valid-->> Epoch [353/3000], Standardized Loss: 0.0147647, Inverse Loss: 0.0030820
Training -->> Epoch: 354,(no reg loss)standard loss: 0.0044035, inverse loss: 0.0009192
Valid-->> Epoch [354/3000], Standardized Loss: 0.0147784, Inverse Loss: 0.0030848
Training -->> Epoch: 355,(no reg loss)standard loss: 0.0043074, inverse loss: 0.0008991
Valid-->> Epoch [355/3000], Standardized Loss: 0.0147822, Inverse Loss: 0.0030856
Training -->> Epoch: 356,(no reg loss)standard loss: 0.0043576, inverse loss: 0.0009096
Valid-->> Epoch [356/3000], Standardized Loss: 0.0147535, Inverse Loss: 0.0030796
Training -->> Epoch: 357,(no reg loss)standard loss: 0.0043020, inverse loss: 0.0008980
Valid-->> Epoch [357/3000], Standardized Loss: 0.0147793, Inverse Loss: 0.0030850
Training -->> Epoch: 358,(no reg loss)standard loss: 0.0043198, inverse loss: 0.0009017
Valid-->> Epoch [358/3000], Standardized Loss: 0.0147753, Inverse Loss: 0.0030842
Training -->> Epoch: 359,(no reg loss)standard loss: 0.0043433, inverse loss: 0.0009066
Valid-->> Epoch [359/3000], Standardized Loss: 0.0147145, Inverse Loss: 0.0030715
Valid-->> Lowest loss found at epoch 359, loss: 0.0030715
Epoch 359, Masked params (inverse standardized): tensor([3.229047e+01, 4.430161e+01, 6.860733e-02, 2.633418e+01, 2.767368e+01,
        1.781099e+01, 2.143048e+01, 1.639423e+00, 1.066115e+02, 2.753353e+01,
        2.613890e+01, 4.461561e+01, 2.442931e+01, 1.503798e+01, 8.461618e+01,
        2.834622e+01, 5.667545e+00, 2.981686e+01, 1.196931e+01, 3.203794e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 360,(no reg loss)standard loss: 0.0043663, inverse loss: 0.0009114
Valid-->> Epoch [360/3000], Standardized Loss: 0.0147369, Inverse Loss: 0.0030762
Training -->> Epoch: 361,(no reg loss)standard loss: 0.0043794, inverse loss: 0.0009141
Valid-->> Epoch [361/3000], Standardized Loss: 0.0147969, Inverse Loss: 0.0030887
Training -->> Epoch: 362,(no reg loss)standard loss: 0.0043443, inverse loss: 0.0009068
Valid-->> Epoch [362/3000], Standardized Loss: 0.0147551, Inverse Loss: 0.0030800
Training -->> Epoch: 363,(no reg loss)standard loss: 0.0041563, inverse loss: 0.0008676
Valid-->> Epoch [363/3000], Standardized Loss: 0.0147238, Inverse Loss: 0.0030734
Training -->> Epoch: 364,(no reg loss)standard loss: 0.0043973, inverse loss: 0.0009179
Valid-->> Epoch [364/3000], Standardized Loss: 0.0147722, Inverse Loss: 0.0030835
Training -->> Epoch: 365,(no reg loss)standard loss: 0.0043339, inverse loss: 0.0009046
Valid-->> Epoch [365/3000], Standardized Loss: 0.0147473, Inverse Loss: 0.0030783
Training -->> Epoch: 366,(no reg loss)standard loss: 0.0043938, inverse loss: 0.0009172
Valid-->> Epoch [366/3000], Standardized Loss: 0.0147287, Inverse Loss: 0.0030745
Training -->> Epoch: 367,(no reg loss)standard loss: 0.0042936, inverse loss: 0.0008962
Valid-->> Epoch [367/3000], Standardized Loss: 0.0147447, Inverse Loss: 0.0030778
Training -->> Epoch: 368,(no reg loss)standard loss: 0.0042964, inverse loss: 0.0008968
Valid-->> Epoch [368/3000], Standardized Loss: 0.0148173, Inverse Loss: 0.0030929
Training -->> Epoch: 369,(no reg loss)standard loss: 0.0044663, inverse loss: 0.0009323
Valid-->> Epoch [369/3000], Standardized Loss: 0.0147115, Inverse Loss: 0.0030708
Valid-->> Lowest loss found at epoch 369, loss: 0.0030708
Epoch 369, Masked params (inverse standardized): tensor([3.229116e+01, 4.430146e+01, 7.062912e-02, 2.633369e+01, 2.767317e+01,
        1.781250e+01, 2.143119e+01, 1.641348e+00, 1.066119e+02, 2.753303e+01,
        2.613837e+01, 4.461748e+01, 2.442915e+01, 1.503967e+01, 8.461586e+01,
        2.834587e+01, 5.669504e+00, 2.981684e+01, 1.197123e+01, 3.203852e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 370,(no reg loss)standard loss: 0.0043775, inverse loss: 0.0009137
Valid-->> Epoch [370/3000], Standardized Loss: 0.0147514, Inverse Loss: 0.0030792
Training -->> Epoch: 371,(no reg loss)standard loss: 0.0043681, inverse loss: 0.0009118
Valid-->> Epoch [371/3000], Standardized Loss: 0.0147461, Inverse Loss: 0.0030781
Training -->> Epoch: 372,(no reg loss)standard loss: 0.0042784, inverse loss: 0.0008931
Valid-->> Epoch [372/3000], Standardized Loss: 0.0146797, Inverse Loss: 0.0030642
Valid-->> Lowest loss found at epoch 372, loss: 0.0030642
Epoch 372, Masked params (inverse standardized): tensor([3.229020e+01, 4.430048e+01, 6.772995e-02, 2.633489e+01, 2.767439e+01,
        1.781034e+01, 2.143017e+01, 1.638607e+00, 1.066110e+02, 2.753423e+01,
        2.613961e+01, 4.461477e+01, 2.442980e+01, 1.503726e+01, 8.461508e+01,
        2.834690e+01, 5.666834e+00, 2.981715e+01, 1.196843e+01, 3.203775e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 373,(no reg loss)standard loss: 0.0043119, inverse loss: 0.0009001
Valid-->> Epoch [373/3000], Standardized Loss: 0.0148876, Inverse Loss: 0.0031076
Training -->> Epoch: 374,(no reg loss)standard loss: 0.0044273, inverse loss: 0.0009242
Valid-->> Epoch [374/3000], Standardized Loss: 0.0146703, Inverse Loss: 0.0030623
Valid-->> Lowest loss found at epoch 374, loss: 0.0030623
Epoch 374, Masked params (inverse standardized): tensor([3.229206e+01, 4.430082e+01, 6.958008e-02, 2.633474e+01, 2.767423e+01,
        1.781317e+01, 2.143207e+01, 1.640690e+00, 1.066117e+02, 2.753408e+01,
        2.613943e+01, 4.461743e+01, 2.443004e+01, 1.504020e+01, 8.461540e+01,
        2.834679e+01, 5.668596e+00, 2.981776e+01, 1.197132e+01, 3.203942e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 375,(no reg loss)standard loss: 0.0043058, inverse loss: 0.0008988
Valid-->> Epoch [375/3000], Standardized Loss: 0.0147345, Inverse Loss: 0.0030757
Training -->> Epoch: 376,(no reg loss)standard loss: 0.0043235, inverse loss: 0.0009025
Valid-->> Epoch [376/3000], Standardized Loss: 0.0148530, Inverse Loss: 0.0031004
Training -->> Epoch: 377,(no reg loss)standard loss: 0.0044087, inverse loss: 0.0009203
Valid-->> Epoch [377/3000], Standardized Loss: 0.0146870, Inverse Loss: 0.0030657
Training -->> Epoch: 378,(no reg loss)standard loss: 0.0042815, inverse loss: 0.0008937
Valid-->> Epoch [378/3000], Standardized Loss: 0.0147564, Inverse Loss: 0.0030802
Training -->> Epoch: 379,(no reg loss)standard loss: 0.0043904, inverse loss: 0.0009164
Valid-->> Epoch [379/3000], Standardized Loss: 0.0147350, Inverse Loss: 0.0030758
Training -->> Epoch: 380,(no reg loss)standard loss: 0.0043043, inverse loss: 0.0008985
Valid-->> Epoch [380/3000], Standardized Loss: 0.0147134, Inverse Loss: 0.0030713
Training -->> Epoch: 381,(no reg loss)standard loss: 0.0043098, inverse loss: 0.0008996
Valid-->> Epoch [381/3000], Standardized Loss: 0.0147367, Inverse Loss: 0.0030761
Training -->> Epoch: 382,(no reg loss)standard loss: 0.0044635, inverse loss: 0.0009317
Valid-->> Epoch [382/3000], Standardized Loss: 0.0146975, Inverse Loss: 0.0030679
Training -->> Epoch: 383,(no reg loss)standard loss: 0.0042942, inverse loss: 0.0008964
Valid-->> Epoch [383/3000], Standardized Loss: 0.0146976, Inverse Loss: 0.0030680
Training -->> Epoch: 384,(no reg loss)standard loss: 0.0043446, inverse loss: 0.0009069
Valid-->> Epoch [384/3000], Standardized Loss: 0.0147387, Inverse Loss: 0.0030765
Training -->> Epoch: 385,(no reg loss)standard loss: 0.0043724, inverse loss: 0.0009127
Valid-->> Epoch [385/3000], Standardized Loss: 0.0146734, Inverse Loss: 0.0030629
Training -->> Epoch: 386,(no reg loss)standard loss: 0.0041972, inverse loss: 0.0008761
Valid-->> Epoch [386/3000], Standardized Loss: 0.0147618, Inverse Loss: 0.0030814
Training -->> Epoch: 387,(no reg loss)standard loss: 0.0044610, inverse loss: 0.0009312
Valid-->> Epoch [387/3000], Standardized Loss: 0.0146440, Inverse Loss: 0.0030568
Valid-->> Lowest loss found at epoch 387, loss: 0.0030568
Epoch 387, Masked params (inverse standardized): tensor([3.229131e+01, 4.430093e+01, 7.126808e-02, 2.633529e+01, 2.767461e+01,
        1.781334e+01, 2.143131e+01, 1.642401e+00, 1.066113e+02, 2.753468e+01,
        2.613983e+01, 4.461890e+01, 2.442959e+01, 1.504085e+01, 8.461548e+01,
        2.834671e+01, 5.670238e+00, 2.981717e+01, 1.197268e+01, 3.203867e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 388,(no reg loss)standard loss: 0.0043270, inverse loss: 0.0009032
Valid-->> Epoch [388/3000], Standardized Loss: 0.0147372, Inverse Loss: 0.0030762
Training -->> Epoch: 389,(no reg loss)standard loss: 0.0043506, inverse loss: 0.0009081
Valid-->> Epoch [389/3000], Standardized Loss: 0.0147249, Inverse Loss: 0.0030736
Training -->> Epoch: 390,(no reg loss)standard loss: 0.0042847, inverse loss: 0.0008944
Valid-->> Epoch [390/3000], Standardized Loss: 0.0147809, Inverse Loss: 0.0030853
Training -->> Epoch: 391,(no reg loss)standard loss: 0.0044291, inverse loss: 0.0009245
Valid-->> Epoch [391/3000], Standardized Loss: 0.0147386, Inverse Loss: 0.0030765
Training -->> Epoch: 392,(no reg loss)standard loss: 0.0043883, inverse loss: 0.0009160
Valid-->> Epoch [392/3000], Standardized Loss: 0.0147082, Inverse Loss: 0.0030702
Training -->> Epoch: 393,(no reg loss)standard loss: 0.0042826, inverse loss: 0.0008939
Valid-->> Epoch [393/3000], Standardized Loss: 0.0147318, Inverse Loss: 0.0030751
Training -->> Epoch: 394,(no reg loss)standard loss: 0.0044200, inverse loss: 0.0009226
Valid-->> Epoch [394/3000], Standardized Loss: 0.0146616, Inverse Loss: 0.0030604
Training -->> Epoch: 395,(no reg loss)standard loss: 0.0042609, inverse loss: 0.0008894
Valid-->> Epoch [395/3000], Standardized Loss: 0.0147841, Inverse Loss: 0.0030860
Training -->> Epoch: 396,(no reg loss)standard loss: 0.0044415, inverse loss: 0.0009271
Valid-->> Epoch [396/3000], Standardized Loss: 0.0146358, Inverse Loss: 0.0030551
Valid-->> Lowest loss found at epoch 396, loss: 0.0030551
Epoch 396, Masked params (inverse standardized): tensor([3.229139e+01, 4.430183e+01, 7.043648e-02, 2.633515e+01, 2.767449e+01,
        1.781306e+01, 2.143138e+01, 1.641672e+00, 1.066122e+02, 2.753452e+01,
        2.613972e+01, 4.461852e+01, 2.442975e+01, 1.504047e+01, 8.461647e+01,
        2.834672e+01, 5.669231e+00, 2.981737e+01, 1.197227e+01, 3.203879e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 397,(no reg loss)standard loss: 0.0042046, inverse loss: 0.0008777
Valid-->> Epoch [397/3000], Standardized Loss: 0.0147203, Inverse Loss: 0.0030727
Training -->> Epoch: 398,(no reg loss)standard loss: 0.0044698, inverse loss: 0.0009330
Valid-->> Epoch [398/3000], Standardized Loss: 0.0146348, Inverse Loss: 0.0030548
Valid-->> Lowest loss found at epoch 398, loss: 0.0030548
Epoch 398, Masked params (inverse standardized): tensor([3.229105e+01, 4.430075e+01, 7.131004e-02, 2.633522e+01, 2.767447e+01,
        1.781321e+01, 2.143106e+01, 1.642250e+00, 1.066116e+02, 2.753459e+01,
        2.613969e+01, 4.461920e+01, 2.442940e+01, 1.504085e+01, 8.461531e+01,
        2.834647e+01, 5.669685e+00, 2.981695e+01, 1.197301e+01, 3.203843e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 399,(no reg loss)standard loss: 0.0044137, inverse loss: 0.0009213
Valid-->> Epoch [399/3000], Standardized Loss: 0.0146596, Inverse Loss: 0.0030600
Training -->> Epoch: 400,(no reg loss)standard loss: 0.0042711, inverse loss: 0.0008915
Valid-->> Epoch [400/3000], Standardized Loss: 0.0146788, Inverse Loss: 0.0030640
Training -->> Epoch: 401,(no reg loss)standard loss: 0.0044033, inverse loss: 0.0009191
Valid-->> Epoch [401/3000], Standardized Loss: 0.0146389, Inverse Loss: 0.0030557
Training -->> Epoch: 402,(no reg loss)standard loss: 0.0043466, inverse loss: 0.0009073
Valid-->> Epoch [402/3000], Standardized Loss: 0.0147244, Inverse Loss: 0.0030736
Training -->> Epoch: 403,(no reg loss)standard loss: 0.0043229, inverse loss: 0.0009023
Valid-->> Epoch [403/3000], Standardized Loss: 0.0146166, Inverse Loss: 0.0030510
Valid-->> Lowest loss found at epoch 403, loss: 0.0030510
Epoch 403, Masked params (inverse standardized): tensor([3.229072e+01, 4.430064e+01, 6.758690e-02, 2.633580e+01, 2.767507e+01,
        1.781106e+01, 2.143072e+01, 1.638887e+00, 1.066116e+02, 2.753518e+01,
        2.614030e+01, 4.461565e+01, 2.443003e+01, 1.503814e+01, 8.461503e+01,
        2.834718e+01, 5.667017e+00, 2.981747e+01, 1.196941e+01, 3.203819e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 404,(no reg loss)standard loss: 0.0043810, inverse loss: 0.0009145
Valid-->> Epoch [404/3000], Standardized Loss: 0.0147060, Inverse Loss: 0.0030697
Training -->> Epoch: 405,(no reg loss)standard loss: 0.0043386, inverse loss: 0.0009056
Valid-->> Epoch [405/3000], Standardized Loss: 0.0146422, Inverse Loss: 0.0030564
Training -->> Epoch: 406,(no reg loss)standard loss: 0.0043566, inverse loss: 0.0009094
Valid-->> Epoch [406/3000], Standardized Loss: 0.0146811, Inverse Loss: 0.0030645
Training -->> Epoch: 407,(no reg loss)standard loss: 0.0043705, inverse loss: 0.0009123
Valid-->> Epoch [407/3000], Standardized Loss: 0.0146590, Inverse Loss: 0.0030599
Training -->> Epoch: 408,(no reg loss)standard loss: 0.0045054, inverse loss: 0.0009404
Valid-->> Epoch [408/3000], Standardized Loss: 0.0146270, Inverse Loss: 0.0030532
Training -->> Epoch: 409,(no reg loss)standard loss: 0.0041929, inverse loss: 0.0008752
Valid-->> Epoch [409/3000], Standardized Loss: 0.0146677, Inverse Loss: 0.0030617
Training -->> Epoch: 410,(no reg loss)standard loss: 0.0043208, inverse loss: 0.0009019
Valid-->> Epoch [410/3000], Standardized Loss: 0.0147389, Inverse Loss: 0.0030766
Training -->> Epoch: 411,(no reg loss)standard loss: 0.0043929, inverse loss: 0.0009170
Valid-->> Epoch [411/3000], Standardized Loss: 0.0146350, Inverse Loss: 0.0030549
Training -->> Epoch: 412,(no reg loss)standard loss: 0.0043451, inverse loss: 0.0009070
Valid-->> Epoch [412/3000], Standardized Loss: 0.0147610, Inverse Loss: 0.0030812
Training -->> Epoch: 413,(no reg loss)standard loss: 0.0044438, inverse loss: 0.0009276
Valid-->> Epoch [413/3000], Standardized Loss: 0.0145917, Inverse Loss: 0.0030458
Valid-->> Lowest loss found at epoch 413, loss: 0.0030458
Epoch 413, Masked params (inverse standardized): tensor([3.229209e+01, 4.430156e+01, 7.036018e-02, 2.633581e+01, 2.767509e+01,
        1.781340e+01, 2.143211e+01, 1.641420e+00, 1.066124e+02, 2.753522e+01,
        2.614033e+01, 4.461794e+01, 2.443064e+01, 1.504053e+01, 8.461626e+01,
        2.834744e+01, 5.669319e+00, 2.981828e+01, 1.197182e+01, 3.203952e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 414,(no reg loss)standard loss: 0.0043068, inverse loss: 0.0008990
Valid-->> Epoch [414/3000], Standardized Loss: 0.0146015, Inverse Loss: 0.0030479
Training -->> Epoch: 415,(no reg loss)standard loss: 0.0043319, inverse loss: 0.0009042
Valid-->> Epoch [415/3000], Standardized Loss: 0.0147128, Inverse Loss: 0.0030711
Training -->> Epoch: 416,(no reg loss)standard loss: 0.0043368, inverse loss: 0.0009053
Valid-->> Epoch [416/3000], Standardized Loss: 0.0146025, Inverse Loss: 0.0030481
Training -->> Epoch: 417,(no reg loss)standard loss: 0.0043343, inverse loss: 0.0009047
Valid-->> Epoch [417/3000], Standardized Loss: 0.0146542, Inverse Loss: 0.0030589
Training -->> Epoch: 418,(no reg loss)standard loss: 0.0042765, inverse loss: 0.0008927
Valid-->> Epoch [418/3000], Standardized Loss: 0.0146585, Inverse Loss: 0.0030598
Training -->> Epoch: 419,(no reg loss)standard loss: 0.0043975, inverse loss: 0.0009179
Valid-->> Epoch [419/3000], Standardized Loss: 0.0147095, Inverse Loss: 0.0030704
Training -->> Epoch: 420,(no reg loss)standard loss: 0.0043729, inverse loss: 0.0009128
Valid-->> Epoch [420/3000], Standardized Loss: 0.0145815, Inverse Loss: 0.0030437
Valid-->> Lowest loss found at epoch 420, loss: 0.0030437
Epoch 420, Masked params (inverse standardized): tensor([3.229161e+01, 4.430103e+01, 6.896591e-02, 2.633618e+01, 2.767534e+01,
        1.781178e+01, 2.143161e+01, 1.639479e+00, 1.066123e+02, 2.753560e+01,
        2.614055e+01, 4.461573e+01, 2.443038e+01, 1.503859e+01, 8.461597e+01,
        2.834737e+01, 5.667871e+00, 2.981798e+01, 1.196960e+01, 3.203904e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 421,(no reg loss)standard loss: 0.0044247, inverse loss: 0.0009236
Valid-->> Epoch [421/3000], Standardized Loss: 0.0145619, Inverse Loss: 0.0030396
Valid-->> Lowest loss found at epoch 421, loss: 0.0030396
Epoch 421, Masked params (inverse standardized): tensor([3.229159e+01, 4.430114e+01, 7.161140e-02, 2.633669e+01, 2.767589e+01,
        1.781222e+01, 2.143155e+01, 1.642248e+00, 1.066120e+02, 2.753612e+01,
        2.614111e+01, 4.461809e+01, 2.443090e+01, 1.503957e+01, 8.461554e+01,
        2.834799e+01, 5.670347e+00, 2.981836e+01, 1.197152e+01, 3.203912e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 422,(no reg loss)standard loss: 0.0042284, inverse loss: 0.0008826
Valid-->> Epoch [422/3000], Standardized Loss: 0.0148052, Inverse Loss: 0.0030904
Training -->> Epoch: 423,(no reg loss)standard loss: 0.0045089, inverse loss: 0.0009412
Valid-->> Epoch [423/3000], Standardized Loss: 0.0145897, Inverse Loss: 0.0030454
Training -->> Epoch: 424,(no reg loss)standard loss: 0.0042557, inverse loss: 0.0008883
Valid-->> Epoch [424/3000], Standardized Loss: 0.0146694, Inverse Loss: 0.0030621
Training -->> Epoch: 425,(no reg loss)standard loss: 0.0043567, inverse loss: 0.0009094
Valid-->> Epoch [425/3000], Standardized Loss: 0.0147085, Inverse Loss: 0.0030702
Training -->> Epoch: 426,(no reg loss)standard loss: 0.0043463, inverse loss: 0.0009072
Valid-->> Epoch [426/3000], Standardized Loss: 0.0146771, Inverse Loss: 0.0030637
Training -->> Epoch: 427,(no reg loss)standard loss: 0.0043985, inverse loss: 0.0009181
Valid-->> Epoch [427/3000], Standardized Loss: 0.0146128, Inverse Loss: 0.0030503
Training -->> Epoch: 428,(no reg loss)standard loss: 0.0044417, inverse loss: 0.0009272
Valid-->> Epoch [428/3000], Standardized Loss: 0.0145672, Inverse Loss: 0.0030407
Training -->> Epoch: 429,(no reg loss)standard loss: 0.0043158, inverse loss: 0.0009009
Valid-->> Epoch [429/3000], Standardized Loss: 0.0146111, Inverse Loss: 0.0030499
Training -->> Epoch: 430,(no reg loss)standard loss: 0.0043622, inverse loss: 0.0009105
Valid-->> Epoch [430/3000], Standardized Loss: 0.0145936, Inverse Loss: 0.0030462
Training -->> Epoch: 431,(no reg loss)standard loss: 0.0043260, inverse loss: 0.0009030
Valid-->> Epoch [431/3000], Standardized Loss: 0.0145937, Inverse Loss: 0.0030463
Training -->> Epoch: 432,(no reg loss)standard loss: 0.0042494, inverse loss: 0.0008870
Valid-->> Epoch [432/3000], Standardized Loss: 0.0146219, Inverse Loss: 0.0030522
Training -->> Epoch: 433,(no reg loss)standard loss: 0.0044555, inverse loss: 0.0009300
Valid-->> Epoch [433/3000], Standardized Loss: 0.0146413, Inverse Loss: 0.0030562
Training -->> Epoch: 434,(no reg loss)standard loss: 0.0043504, inverse loss: 0.0009081
Valid-->> Epoch [434/3000], Standardized Loss: 0.0145902, Inverse Loss: 0.0030455
Training -->> Epoch: 435,(no reg loss)standard loss: 0.0042643, inverse loss: 0.0008901
Valid-->> Epoch [435/3000], Standardized Loss: 0.0146336, Inverse Loss: 0.0030546
Training -->> Epoch: 436,(no reg loss)standard loss: 0.0042750, inverse loss: 0.0008923
Valid-->> Epoch [436/3000], Standardized Loss: 0.0146251, Inverse Loss: 0.0030528
Training -->> Epoch: 437,(no reg loss)standard loss: 0.0044999, inverse loss: 0.0009393
Valid-->> Epoch [437/3000], Standardized Loss: 0.0145330, Inverse Loss: 0.0030336
Valid-->> Lowest loss found at epoch 437, loss: 0.0030336
Epoch 437, Masked params (inverse standardized): tensor([3.229252e+01, 4.430133e+01, 7.198524e-02, 2.633705e+01, 2.767603e+01,
        1.781382e+01, 2.143250e+01, 1.642899e+00, 1.066121e+02, 2.753649e+01,
        2.614125e+01, 4.461916e+01, 2.443043e+01, 1.504108e+01, 8.461591e+01,
        2.834760e+01, 5.670750e+00, 2.981811e+01, 1.197281e+01, 3.203987e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 438,(no reg loss)standard loss: 0.0043442, inverse loss: 0.0009068
Valid-->> Epoch [438/3000], Standardized Loss: 0.0146167, Inverse Loss: 0.0030511
Training -->> Epoch: 439,(no reg loss)standard loss: 0.0042870, inverse loss: 0.0008949
Valid-->> Epoch [439/3000], Standardized Loss: 0.0145922, Inverse Loss: 0.0030459
Training -->> Epoch: 440,(no reg loss)standard loss: 0.0043487, inverse loss: 0.0009077
Valid-->> Epoch [440/3000], Standardized Loss: 0.0146190, Inverse Loss: 0.0030515
Training -->> Epoch: 441,(no reg loss)standard loss: 0.0044034, inverse loss: 0.0009192
Valid-->> Epoch [441/3000], Standardized Loss: 0.0145878, Inverse Loss: 0.0030450
Training -->> Epoch: 442,(no reg loss)standard loss: 0.0043130, inverse loss: 0.0009003
Valid-->> Epoch [442/3000], Standardized Loss: 0.0145875, Inverse Loss: 0.0030450
Training -->> Epoch: 443,(no reg loss)standard loss: 0.0043443, inverse loss: 0.0009068
Valid-->> Epoch [443/3000], Standardized Loss: 0.0145863, Inverse Loss: 0.0030447
Training -->> Epoch: 444,(no reg loss)standard loss: 0.0044560, inverse loss: 0.0009301
Valid-->> Epoch [444/3000], Standardized Loss: 0.0145171, Inverse Loss: 0.0030303
Valid-->> Lowest loss found at epoch 444, loss: 0.0030303
Epoch 444, Masked params (inverse standardized): tensor([3.229229e+01, 4.430128e+01, 7.202339e-02, 2.633734e+01, 2.767634e+01,
        1.781337e+01, 2.143224e+01, 1.642651e+00, 1.066116e+02, 2.753674e+01,
        2.614155e+01, 4.461854e+01, 2.443075e+01, 1.504047e+01, 8.461588e+01,
        2.834795e+01, 5.670710e+00, 2.981830e+01, 1.197211e+01, 3.203969e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 445,(no reg loss)standard loss: 0.0042544, inverse loss: 0.0008881
Valid-->> Epoch [445/3000], Standardized Loss: 0.0145825, Inverse Loss: 0.0030439
Training -->> Epoch: 446,(no reg loss)standard loss: 0.0043381, inverse loss: 0.0009055
Valid-->> Epoch [446/3000], Standardized Loss: 0.0146661, Inverse Loss: 0.0030614
Training -->> Epoch: 447,(no reg loss)standard loss: 0.0043554, inverse loss: 0.0009091
Valid-->> Epoch [447/3000], Standardized Loss: 0.0145961, Inverse Loss: 0.0030468
Training -->> Epoch: 448,(no reg loss)standard loss: 0.0044683, inverse loss: 0.0009327
Valid-->> Epoch [448/3000], Standardized Loss: 0.0145686, Inverse Loss: 0.0030410
Training -->> Epoch: 449,(no reg loss)standard loss: 0.0042599, inverse loss: 0.0008892
Valid-->> Epoch [449/3000], Standardized Loss: 0.0146716, Inverse Loss: 0.0030625
Training -->> Epoch: 450,(no reg loss)standard loss: 0.0044207, inverse loss: 0.0009228
Valid-->> Epoch [450/3000], Standardized Loss: 0.0145977, Inverse Loss: 0.0030471
Training -->> Epoch: 451,(no reg loss)standard loss: 0.0043167, inverse loss: 0.0009011
Valid-->> Epoch [451/3000], Standardized Loss: 0.0145765, Inverse Loss: 0.0030427
Training -->> Epoch: 452,(no reg loss)standard loss: 0.0043319, inverse loss: 0.0009042
Valid-->> Epoch [452/3000], Standardized Loss: 0.0146890, Inverse Loss: 0.0030662
Training -->> Epoch: 453,(no reg loss)standard loss: 0.0044349, inverse loss: 0.0009257
Valid-->> Epoch [453/3000], Standardized Loss: 0.0145048, Inverse Loss: 0.0030277
Valid-->> Lowest loss found at epoch 453, loss: 0.0030277
Epoch 453, Masked params (inverse standardized): tensor([3.229215e+01, 4.430029e+01, 6.960869e-02, 2.633749e+01, 2.767645e+01,
        1.781296e+01, 2.143214e+01, 1.640423e+00, 1.066124e+02, 2.753691e+01,
        2.614167e+01, 4.461702e+01, 2.443078e+01, 1.503988e+01, 8.461546e+01,
        2.834799e+01, 5.668089e+00, 2.981832e+01, 1.197102e+01, 3.203960e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 454,(no reg loss)standard loss: 0.0043765, inverse loss: 0.0009135
Valid-->> Epoch [454/3000], Standardized Loss: 0.0146333, Inverse Loss: 0.0030545
Training -->> Epoch: 455,(no reg loss)standard loss: 0.0044223, inverse loss: 0.0009231
Valid-->> Epoch [455/3000], Standardized Loss: 0.0145315, Inverse Loss: 0.0030333
Training -->> Epoch: 456,(no reg loss)standard loss: 0.0043752, inverse loss: 0.0009133
Valid-->> Epoch [456/3000], Standardized Loss: 0.0145397, Inverse Loss: 0.0030350
Training -->> Epoch: 457,(no reg loss)standard loss: 0.0044307, inverse loss: 0.0009249
Valid-->> Epoch [457/3000], Standardized Loss: 0.0145437, Inverse Loss: 0.0030358
Training -->> Epoch: 458,(no reg loss)standard loss: 0.0041984, inverse loss: 0.0008764
Valid-->> Epoch [458/3000], Standardized Loss: 0.0146482, Inverse Loss: 0.0030577
Training -->> Epoch: 459,(no reg loss)standard loss: 0.0044423, inverse loss: 0.0009273
Valid-->> Epoch [459/3000], Standardized Loss: 0.0145904, Inverse Loss: 0.0030456
Training -->> Epoch: 460,(no reg loss)standard loss: 0.0044015, inverse loss: 0.0009188
Valid-->> Epoch [460/3000], Standardized Loss: 0.0145511, Inverse Loss: 0.0030374
Training -->> Epoch: 461,(no reg loss)standard loss: 0.0043741, inverse loss: 0.0009130
Valid-->> Epoch [461/3000], Standardized Loss: 0.0145214, Inverse Loss: 0.0030312
Training -->> Epoch: 462,(no reg loss)standard loss: 0.0042457, inverse loss: 0.0008862
Valid-->> Epoch [462/3000], Standardized Loss: 0.0146450, Inverse Loss: 0.0030570
Training -->> Epoch: 463,(no reg loss)standard loss: 0.0045231, inverse loss: 0.0009441
Valid-->> Epoch [463/3000], Standardized Loss: 0.0144913, Inverse Loss: 0.0030249
Valid-->> Lowest loss found at epoch 463, loss: 0.0030249
Epoch 463, Masked params (inverse standardized): tensor([3.229265e+01, 4.430225e+01, 7.259941e-02, 2.633736e+01, 2.767628e+01,
        1.781484e+01, 2.143263e+01, 1.643415e+00, 1.066124e+02, 2.753682e+01,
        2.614149e+01, 4.462022e+01, 2.443046e+01, 1.504229e+01, 8.461687e+01,
        2.834774e+01, 5.670971e+00, 2.981805e+01, 1.197423e+01, 3.203995e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 464,(no reg loss)standard loss: 0.0042409, inverse loss: 0.0008852
Valid-->> Epoch [464/3000], Standardized Loss: 0.0145688, Inverse Loss: 0.0030411
Training -->> Epoch: 465,(no reg loss)standard loss: 0.0043662, inverse loss: 0.0009114
Valid-->> Epoch [465/3000], Standardized Loss: 0.0146197, Inverse Loss: 0.0030517
Training -->> Epoch: 466,(no reg loss)standard loss: 0.0044084, inverse loss: 0.0009202
Valid-->> Epoch [466/3000], Standardized Loss: 0.0145545, Inverse Loss: 0.0030381
Training -->> Epoch: 467,(no reg loss)standard loss: 0.0042330, inverse loss: 0.0008836
Valid-->> Epoch [467/3000], Standardized Loss: 0.0146141, Inverse Loss: 0.0030505
Training -->> Epoch: 468,(no reg loss)standard loss: 0.0045143, inverse loss: 0.0009423
Valid-->> Epoch [468/3000], Standardized Loss: 0.0144653, Inverse Loss: 0.0030195
Valid-->> Lowest loss found at epoch 468, loss: 0.0030195
Epoch 468, Masked params (inverse standardized): tensor([3.229243e+01, 4.430100e+01, 7.119942e-02, 2.633818e+01, 2.767701e+01,
        1.781467e+01, 2.143245e+01, 1.643019e+00, 1.066118e+02, 2.753765e+01,
        2.614223e+01, 4.462016e+01, 2.443092e+01, 1.504212e+01, 8.461591e+01,
        2.834831e+01, 5.670534e+00, 2.981841e+01, 1.197372e+01, 3.203981e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 469,(no reg loss)standard loss: 0.0043444, inverse loss: 0.0009068
Valid-->> Epoch [469/3000], Standardized Loss: 0.0145288, Inverse Loss: 0.0030327
Training -->> Epoch: 470,(no reg loss)standard loss: 0.0043996, inverse loss: 0.0009184
Valid-->> Epoch [470/3000], Standardized Loss: 0.0145616, Inverse Loss: 0.0030396
Training -->> Epoch: 471,(no reg loss)standard loss: 0.0043443, inverse loss: 0.0009068
Valid-->> Epoch [471/3000], Standardized Loss: 0.0145364, Inverse Loss: 0.0030343
Training -->> Epoch: 472,(no reg loss)standard loss: 0.0043511, inverse loss: 0.0009082
Valid-->> Epoch [472/3000], Standardized Loss: 0.0145858, Inverse Loss: 0.0030446
Training -->> Epoch: 473,(no reg loss)standard loss: 0.0044908, inverse loss: 0.0009374
Valid-->> Epoch [473/3000], Standardized Loss: 0.0145057, Inverse Loss: 0.0030279
Training -->> Epoch: 474,(no reg loss)standard loss: 0.0043565, inverse loss: 0.0009094
Valid-->> Epoch [474/3000], Standardized Loss: 0.0144991, Inverse Loss: 0.0030265
Training -->> Epoch: 475,(no reg loss)standard loss: 0.0043458, inverse loss: 0.0009071
Valid-->> Epoch [475/3000], Standardized Loss: 0.0145726, Inverse Loss: 0.0030419
Training -->> Epoch: 476,(no reg loss)standard loss: 0.0042575, inverse loss: 0.0008887
Valid-->> Epoch [476/3000], Standardized Loss: 0.0145764, Inverse Loss: 0.0030427
Training -->> Epoch: 477,(no reg loss)standard loss: 0.0044593, inverse loss: 0.0009308
Valid-->> Epoch [477/3000], Standardized Loss: 0.0145151, Inverse Loss: 0.0030299
Training -->> Epoch: 478,(no reg loss)standard loss: 0.0042916, inverse loss: 0.0008958
Valid-->> Epoch [478/3000], Standardized Loss: 0.0145246, Inverse Loss: 0.0030318
Training -->> Epoch: 479,(no reg loss)standard loss: 0.0044104, inverse loss: 0.0009206
Valid-->> Epoch [479/3000], Standardized Loss: 0.0144900, Inverse Loss: 0.0030246
Training -->> Epoch: 480,(no reg loss)standard loss: 0.0043214, inverse loss: 0.0009020
Valid-->> Epoch [480/3000], Standardized Loss: 0.0145318, Inverse Loss: 0.0030334
Training -->> Epoch: 481,(no reg loss)standard loss: 0.0044881, inverse loss: 0.0009368
Valid-->> Epoch [481/3000], Standardized Loss: 0.0144686, Inverse Loss: 0.0030202
Training -->> Epoch: 482,(no reg loss)standard loss: 0.0043851, inverse loss: 0.0009153
Valid-->> Epoch [482/3000], Standardized Loss: 0.0145378, Inverse Loss: 0.0030346
Training -->> Epoch: 483,(no reg loss)standard loss: 0.0043547, inverse loss: 0.0009090
Valid-->> Epoch [483/3000], Standardized Loss: 0.0145197, Inverse Loss: 0.0030308
Training -->> Epoch: 484,(no reg loss)standard loss: 0.0043818, inverse loss: 0.0009147
Valid-->> Epoch [484/3000], Standardized Loss: 0.0144602, Inverse Loss: 0.0030184
Valid-->> Lowest loss found at epoch 484, loss: 0.0030184
Epoch 484, Masked params (inverse standardized): tensor([3.229099e+01, 4.430054e+01, 6.942558e-02, 2.633815e+01, 2.767694e+01,
        1.781146e+01, 2.143097e+01, 1.640381e+00, 1.066124e+02, 2.753759e+01,
        2.614217e+01, 4.461640e+01, 2.443048e+01, 1.503848e+01, 8.461533e+01,
        2.834809e+01, 5.668417e+00, 2.981778e+01, 1.196992e+01, 3.203846e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 485,(no reg loss)standard loss: 0.0043670, inverse loss: 0.0009116
Valid-->> Epoch [485/3000], Standardized Loss: 0.0145010, Inverse Loss: 0.0030269
Training -->> Epoch: 486,(no reg loss)standard loss: 0.0043701, inverse loss: 0.0009122
Valid-->> Epoch [486/3000], Standardized Loss: 0.0144887, Inverse Loss: 0.0030244
Training -->> Epoch: 487,(no reg loss)standard loss: 0.0043644, inverse loss: 0.0009110
Valid-->> Epoch [487/3000], Standardized Loss: 0.0145983, Inverse Loss: 0.0030472
Training -->> Epoch: 488,(no reg loss)standard loss: 0.0043841, inverse loss: 0.0009151
Valid-->> Epoch [488/3000], Standardized Loss: 0.0144633, Inverse Loss: 0.0030190
Training -->> Epoch: 489,(no reg loss)standard loss: 0.0043170, inverse loss: 0.0009011
Valid-->> Epoch [489/3000], Standardized Loss: 0.0145313, Inverse Loss: 0.0030332
Training -->> Epoch: 490,(no reg loss)standard loss: 0.0043127, inverse loss: 0.0009002
Valid-->> Epoch [490/3000], Standardized Loss: 0.0145216, Inverse Loss: 0.0030312
Training -->> Epoch: 491,(no reg loss)standard loss: 0.0043996, inverse loss: 0.0009184
Valid-->> Epoch [491/3000], Standardized Loss: 0.0145297, Inverse Loss: 0.0030329
Training -->> Epoch: 492,(no reg loss)standard loss: 0.0043785, inverse loss: 0.0009140
Valid-->> Epoch [492/3000], Standardized Loss: 0.0145680, Inverse Loss: 0.0030409
Training -->> Epoch: 493,(no reg loss)standard loss: 0.0044441, inverse loss: 0.0009276
Valid-->> Epoch [493/3000], Standardized Loss: 0.0144502, Inverse Loss: 0.0030163
Valid-->> Lowest loss found at epoch 493, loss: 0.0030163
Epoch 493, Masked params (inverse standardized): tensor([3.229166e+01, 4.430306e+01, 7.000732e-02, 2.633800e+01, 2.767675e+01,
        1.781241e+01, 2.143167e+01, 1.640713e+00, 1.066122e+02, 2.753745e+01,
        2.614197e+01, 4.461714e+01, 2.443076e+01, 1.503952e+01, 8.461758e+01,
        2.834802e+01, 5.668423e+00, 2.981824e+01, 1.197105e+01, 3.203910e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 494,(no reg loss)standard loss: 0.0042730, inverse loss: 0.0008919
Valid-->> Epoch [494/3000], Standardized Loss: 0.0145137, Inverse Loss: 0.0030296
Training -->> Epoch: 495,(no reg loss)standard loss: 0.0042924, inverse loss: 0.0008960
Valid-->> Epoch [495/3000], Standardized Loss: 0.0145263, Inverse Loss: 0.0030322
Training -->> Epoch: 496,(no reg loss)standard loss: 0.0044090, inverse loss: 0.0009203
Valid-->> Epoch [496/3000], Standardized Loss: 0.0144919, Inverse Loss: 0.0030250
Training -->> Epoch: 497,(no reg loss)standard loss: 0.0044479, inverse loss: 0.0009284
Valid-->> Epoch [497/3000], Standardized Loss: 0.0144462, Inverse Loss: 0.0030155
Valid-->> Lowest loss found at epoch 497, loss: 0.0030155
Epoch 497, Masked params (inverse standardized): tensor([3.229147e+01, 4.430148e+01, 7.125473e-02, 2.633818e+01, 2.767677e+01,
        1.781188e+01, 2.143144e+01, 1.641476e+00, 1.066123e+02, 2.753766e+01,
        2.614199e+01, 4.461724e+01, 2.443038e+01, 1.503907e+01, 8.461607e+01,
        2.834764e+01, 5.669746e+00, 2.981795e+01, 1.197085e+01, 3.203895e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 498,(no reg loss)standard loss: 0.0043039, inverse loss: 0.0008984
Valid-->> Epoch [498/3000], Standardized Loss: 0.0145303, Inverse Loss: 0.0030330
Training -->> Epoch: 499,(no reg loss)standard loss: 0.0044088, inverse loss: 0.0009203
Valid-->> Epoch [499/3000], Standardized Loss: 0.0145003, Inverse Loss: 0.0030268
Training -->> Epoch: 500,(no reg loss)standard loss: 0.0042400, inverse loss: 0.0008850
Valid-->> Epoch [500/3000], Standardized Loss: 0.0145502, Inverse Loss: 0.0030372
Training -->> Epoch: 501,(no reg loss)standard loss: 0.0044997, inverse loss: 0.0009393
Valid-->> Epoch [501/3000], Standardized Loss: 0.0144749, Inverse Loss: 0.0030215
Training -->> Epoch: 502,(no reg loss)standard loss: 0.0044636, inverse loss: 0.0009317
Valid-->> Epoch [502/3000], Standardized Loss: 0.0144791, Inverse Loss: 0.0030224
Training -->> Epoch: 503,(no reg loss)standard loss: 0.0043004, inverse loss: 0.0008977
Valid-->> Epoch [503/3000], Standardized Loss: 0.0145151, Inverse Loss: 0.0030299
Training -->> Epoch: 504,(no reg loss)standard loss: 0.0043700, inverse loss: 0.0009122
Valid-->> Epoch [504/3000], Standardized Loss: 0.0145024, Inverse Loss: 0.0030272
Training -->> Epoch: 505,(no reg loss)standard loss: 0.0044411, inverse loss: 0.0009270
Valid-->> Epoch [505/3000], Standardized Loss: 0.0144056, Inverse Loss: 0.0030070
Valid-->> Lowest loss found at epoch 505, loss: 0.0030070
Epoch 505, Masked params (inverse standardized): tensor([3.229206e+01, 4.430058e+01, 6.981659e-02, 2.633910e+01, 2.767775e+01,
        1.781236e+01, 2.143202e+01, 1.640476e+00, 1.066114e+02, 2.753859e+01,
        2.614295e+01, 4.461671e+01, 2.443105e+01, 1.503924e+01, 8.461545e+01,
        2.834868e+01, 5.668701e+00, 2.981844e+01, 1.197048e+01, 3.203938e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 506,(no reg loss)standard loss: 0.0044325, inverse loss: 0.0009252
Valid-->> Epoch [506/3000], Standardized Loss: 0.0145228, Inverse Loss: 0.0030315
Training -->> Epoch: 507,(no reg loss)standard loss: 0.0043374, inverse loss: 0.0009054
Valid-->> Epoch [507/3000], Standardized Loss: 0.0144614, Inverse Loss: 0.0030187
Training -->> Epoch: 508,(no reg loss)standard loss: 0.0044490, inverse loss: 0.0009287
Valid-->> Epoch [508/3000], Standardized Loss: 0.0144768, Inverse Loss: 0.0030219
Training -->> Epoch: 509,(no reg loss)standard loss: 0.0044408, inverse loss: 0.0009270
Valid-->> Epoch [509/3000], Standardized Loss: 0.0143952, Inverse Loss: 0.0030048
Valid-->> Lowest loss found at epoch 509, loss: 0.0030048
Epoch 509, Masked params (inverse standardized): tensor([3.229248e+01, 4.430118e+01, 7.021713e-02, 2.633914e+01, 2.767785e+01,
        1.781273e+01, 2.143249e+01, 1.640705e+00, 1.066125e+02, 2.753858e+01,
        2.614302e+01, 4.461678e+01, 2.443157e+01, 1.503956e+01, 8.461604e+01,
        2.834897e+01, 5.669142e+00, 2.981899e+01, 1.197054e+01, 3.203994e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 510,(no reg loss)standard loss: 0.0043184, inverse loss: 0.0009014
Valid-->> Epoch [510/3000], Standardized Loss: 0.0144349, Inverse Loss: 0.0030131
Training -->> Epoch: 511,(no reg loss)standard loss: 0.0044824, inverse loss: 0.0009357
Valid-->> Epoch [511/3000], Standardized Loss: 0.0143612, Inverse Loss: 0.0029977
Valid-->> Lowest loss found at epoch 511, loss: 0.0029977
Epoch 511, Masked params (inverse standardized): tensor([3.229285e+01, 4.430067e+01, 7.161522e-02, 2.634030e+01, 2.767894e+01,
        1.781426e+01, 2.143283e+01, 1.642488e+00, 1.066116e+02, 2.753976e+01,
        2.614413e+01, 4.461913e+01, 2.443198e+01, 1.504150e+01, 8.461548e+01,
        2.834975e+01, 5.670368e+00, 2.981927e+01, 1.197294e+01, 3.204016e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 512,(no reg loss)standard loss: 0.0043587, inverse loss: 0.0009098
Valid-->> Epoch [512/3000], Standardized Loss: 0.0144501, Inverse Loss: 0.0030163
Training -->> Epoch: 513,(no reg loss)standard loss: 0.0043254, inverse loss: 0.0009029
Valid-->> Epoch [513/3000], Standardized Loss: 0.0144147, Inverse Loss: 0.0030089
Training -->> Epoch: 514,(no reg loss)standard loss: 0.0044736, inverse loss: 0.0009338
Valid-->> Epoch [514/3000], Standardized Loss: 0.0144033, Inverse Loss: 0.0030065
Training -->> Epoch: 515,(no reg loss)standard loss: 0.0042470, inverse loss: 0.0008865
Valid-->> Epoch [515/3000], Standardized Loss: 0.0144883, Inverse Loss: 0.0030243
Training -->> Epoch: 516,(no reg loss)standard loss: 0.0043959, inverse loss: 0.0009176
Valid-->> Epoch [516/3000], Standardized Loss: 0.0144897, Inverse Loss: 0.0030246
Training -->> Epoch: 517,(no reg loss)standard loss: 0.0043594, inverse loss: 0.0009100
Valid-->> Epoch [517/3000], Standardized Loss: 0.0144417, Inverse Loss: 0.0030145
Training -->> Epoch: 518,(no reg loss)standard loss: 0.0043518, inverse loss: 0.0009084
Valid-->> Epoch [518/3000], Standardized Loss: 0.0145351, Inverse Loss: 0.0030340
Training -->> Epoch: 519,(no reg loss)standard loss: 0.0043674, inverse loss: 0.0009116
Valid-->> Epoch [519/3000], Standardized Loss: 0.0144697, Inverse Loss: 0.0030204
Training -->> Epoch: 520,(no reg loss)standard loss: 0.0043751, inverse loss: 0.0009132
Valid-->> Epoch [520/3000], Standardized Loss: 0.0144370, Inverse Loss: 0.0030136
Training -->> Epoch: 521,(no reg loss)standard loss: 0.0043747, inverse loss: 0.0009132
Valid-->> Epoch [521/3000], Standardized Loss: 0.0145042, Inverse Loss: 0.0030276
Training -->> Epoch: 522,(no reg loss)standard loss: 0.0044044, inverse loss: 0.0009194
Valid-->> Epoch [522/3000], Standardized Loss: 0.0144058, Inverse Loss: 0.0030070
Training -->> Epoch: 523,(no reg loss)standard loss: 0.0042699, inverse loss: 0.0008913
Valid-->> Epoch [523/3000], Standardized Loss: 0.0145335, Inverse Loss: 0.0030337
Training -->> Epoch: 524,(no reg loss)standard loss: 0.0044799, inverse loss: 0.0009351
Valid-->> Epoch [524/3000], Standardized Loss: 0.0144454, Inverse Loss: 0.0030153
Training -->> Epoch: 525,(no reg loss)standard loss: 0.0044337, inverse loss: 0.0009255
Valid-->> Epoch [525/3000], Standardized Loss: 0.0144099, Inverse Loss: 0.0030079
Training -->> Epoch: 526,(no reg loss)standard loss: 0.0042544, inverse loss: 0.0008881
Valid-->> Epoch [526/3000], Standardized Loss: 0.0145110, Inverse Loss: 0.0030290
Training -->> Epoch: 527,(no reg loss)standard loss: 0.0045369, inverse loss: 0.0009470
Valid-->> Epoch [527/3000], Standardized Loss: 0.0143155, Inverse Loss: 0.0029882
Valid-->> Lowest loss found at epoch 527, loss: 0.0029882
Epoch 527, Masked params (inverse standardized): tensor([3.229402e+01, 4.430072e+01, 7.319641e-02, 2.634130e+01, 2.767977e+01,
        1.781637e+01, 2.143405e+01, 1.644552e+00, 1.066128e+02, 2.754081e+01,
        2.614498e+01, 4.462162e+01, 2.443253e+01, 1.504372e+01, 8.461592e+01,
        2.835031e+01, 5.671724e+00, 2.981991e+01, 1.197555e+01, 3.204131e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 528,(no reg loss)standard loss: 0.0042232, inverse loss: 0.0008815
Valid-->> Epoch [528/3000], Standardized Loss: 0.0145565, Inverse Loss: 0.0030385
Training -->> Epoch: 529,(no reg loss)standard loss: 0.0043773, inverse loss: 0.0009137
Valid-->> Epoch [529/3000], Standardized Loss: 0.0144712, Inverse Loss: 0.0030207
Training -->> Epoch: 530,(no reg loss)standard loss: 0.0043684, inverse loss: 0.0009118
Valid-->> Epoch [530/3000], Standardized Loss: 0.0144438, Inverse Loss: 0.0030150
Training -->> Epoch: 531,(no reg loss)standard loss: 0.0043597, inverse loss: 0.0009100
Valid-->> Epoch [531/3000], Standardized Loss: 0.0144121, Inverse Loss: 0.0030084
Training -->> Epoch: 532,(no reg loss)standard loss: 0.0044722, inverse loss: 0.0009335
Valid-->> Epoch [532/3000], Standardized Loss: 0.0143876, Inverse Loss: 0.0030032
Training -->> Epoch: 533,(no reg loss)standard loss: 0.0043922, inverse loss: 0.0009168
Valid-->> Epoch [533/3000], Standardized Loss: 0.0145645, Inverse Loss: 0.0030402
Training -->> Epoch: 534,(no reg loss)standard loss: 0.0044119, inverse loss: 0.0009209
Valid-->> Epoch [534/3000], Standardized Loss: 0.0143621, Inverse Loss: 0.0029979
Training -->> Epoch: 535,(no reg loss)standard loss: 0.0043777, inverse loss: 0.0009138
Valid-->> Epoch [535/3000], Standardized Loss: 0.0143968, Inverse Loss: 0.0030052
Training -->> Epoch: 536,(no reg loss)standard loss: 0.0042234, inverse loss: 0.0008816
Valid-->> Epoch [536/3000], Standardized Loss: 0.0144651, Inverse Loss: 0.0030194
Training -->> Epoch: 537,(no reg loss)standard loss: 0.0047588, inverse loss: 0.0009933
Valid-->> Epoch [537/3000], Standardized Loss: 0.0143733, Inverse Loss: 0.0030003
Training -->> Epoch: 538,(no reg loss)standard loss: 0.0042653, inverse loss: 0.0008903
Valid-->> Epoch [538/3000], Standardized Loss: 0.0144592, Inverse Loss: 0.0030182
Training -->> Epoch: 539,(no reg loss)standard loss: 0.0044252, inverse loss: 0.0009237
Valid-->> Epoch [539/3000], Standardized Loss: 0.0143982, Inverse Loss: 0.0030055
Training -->> Epoch: 540,(no reg loss)standard loss: 0.0042841, inverse loss: 0.0008943
Valid-->> Epoch [540/3000], Standardized Loss: 0.0144677, Inverse Loss: 0.0030200
Training -->> Epoch: 541,(no reg loss)standard loss: 0.0044647, inverse loss: 0.0009319
Valid-->> Epoch [541/3000], Standardized Loss: 0.0143950, Inverse Loss: 0.0030048
Training -->> Epoch: 542,(no reg loss)standard loss: 0.0044289, inverse loss: 0.0009245
Valid-->> Epoch [542/3000], Standardized Loss: 0.0143715, Inverse Loss: 0.0029999
Training -->> Epoch: 543,(no reg loss)standard loss: 0.0042237, inverse loss: 0.0008816
Valid-->> Epoch [543/3000], Standardized Loss: 0.0144495, Inverse Loss: 0.0030162
Training -->> Epoch: 544,(no reg loss)standard loss: 0.0044073, inverse loss: 0.0009200
Valid-->> Epoch [544/3000], Standardized Loss: 0.0143634, Inverse Loss: 0.0029982
Training -->> Epoch: 545,(no reg loss)standard loss: 0.0044450, inverse loss: 0.0009278
Valid-->> Epoch [545/3000], Standardized Loss: 0.0144400, Inverse Loss: 0.0030142
Training -->> Epoch: 546,(no reg loss)standard loss: 0.0043392, inverse loss: 0.0009058
Valid-->> Epoch [546/3000], Standardized Loss: 0.0144240, Inverse Loss: 0.0030108
Training -->> Epoch: 547,(no reg loss)standard loss: 0.0045266, inverse loss: 0.0009449
Valid-->> Epoch [547/3000], Standardized Loss: 0.0143149, Inverse Loss: 0.0029881
Valid-->> Lowest loss found at epoch 547, loss: 0.0029881
Epoch 547, Masked params (inverse standardized): tensor([3.229258e+01, 4.430069e+01, 7.170296e-02, 2.634056e+01, 2.767892e+01,
        1.781373e+01, 2.143257e+01, 1.642860e+00, 1.066122e+02, 2.754004e+01,
        2.614414e+01, 4.461915e+01, 2.443193e+01, 1.504098e+01, 8.461566e+01,
        2.834950e+01, 5.670309e+00, 2.981931e+01, 1.197273e+01, 3.204002e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 548,(no reg loss)standard loss: 0.0043903, inverse loss: 0.0009164
Valid-->> Epoch [548/3000], Standardized Loss: 0.0143943, Inverse Loss: 0.0030047
Training -->> Epoch: 549,(no reg loss)standard loss: 0.0042849, inverse loss: 0.0008944
Valid-->> Epoch [549/3000], Standardized Loss: 0.0144340, Inverse Loss: 0.0030129
Training -->> Epoch: 550,(no reg loss)standard loss: 0.0043860, inverse loss: 0.0009155
Valid-->> Epoch [550/3000], Standardized Loss: 0.0143958, Inverse Loss: 0.0030050
Training -->> Epoch: 551,(no reg loss)standard loss: 0.0044373, inverse loss: 0.0009262
Valid-->> Epoch [551/3000], Standardized Loss: 0.0143286, Inverse Loss: 0.0029909
Training -->> Epoch: 552,(no reg loss)standard loss: 0.0042535, inverse loss: 0.0008879
Valid-->> Epoch [552/3000], Standardized Loss: 0.0144317, Inverse Loss: 0.0030124
Training -->> Epoch: 553,(no reg loss)standard loss: 0.0044666, inverse loss: 0.0009323
Valid-->> Epoch [553/3000], Standardized Loss: 0.0144909, Inverse Loss: 0.0030248
Training -->> Epoch: 554,(no reg loss)standard loss: 0.0044041, inverse loss: 0.0009193
Valid-->> Epoch [554/3000], Standardized Loss: 0.0143226, Inverse Loss: 0.0029897
Training -->> Epoch: 555,(no reg loss)standard loss: 0.0043680, inverse loss: 0.0009118
Valid-->> Epoch [555/3000], Standardized Loss: 0.0144810, Inverse Loss: 0.0030227
Training -->> Epoch: 556,(no reg loss)standard loss: 0.0044289, inverse loss: 0.0009245
Valid-->> Epoch [556/3000], Standardized Loss: 0.0143871, Inverse Loss: 0.0030031
Training -->> Epoch: 557,(no reg loss)standard loss: 0.0045046, inverse loss: 0.0009403
Valid-->> Epoch [557/3000], Standardized Loss: 0.0142634, Inverse Loss: 0.0029773
Valid-->> Lowest loss found at epoch 557, loss: 0.0029773
Epoch 557, Masked params (inverse standardized): tensor([3.229392e+01, 4.430078e+01, 7.279968e-02, 2.634190e+01, 2.768030e+01,
        1.781435e+01, 2.143391e+01, 1.643583e+00, 1.066125e+02, 2.754144e+01,
        2.614550e+01, 4.461960e+01, 2.443312e+01, 1.504137e+01, 8.461577e+01,
        2.835086e+01, 5.671560e+00, 2.982047e+01, 1.197309e+01, 3.204133e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 558,(no reg loss)standard loss: 0.0042907, inverse loss: 0.0008956
Valid-->> Epoch [558/3000], Standardized Loss: 0.0144417, Inverse Loss: 0.0030145
Training -->> Epoch: 559,(no reg loss)standard loss: 0.0044835, inverse loss: 0.0009359
Valid-->> Epoch [559/3000], Standardized Loss: 0.0143143, Inverse Loss: 0.0029879
Training -->> Epoch: 560,(no reg loss)standard loss: 0.0042947, inverse loss: 0.0008965
Valid-->> Epoch [560/3000], Standardized Loss: 0.0144442, Inverse Loss: 0.0030151
Training -->> Epoch: 561,(no reg loss)standard loss: 0.0044634, inverse loss: 0.0009317
Valid-->> Epoch [561/3000], Standardized Loss: 0.0142932, Inverse Loss: 0.0029835
Training -->> Epoch: 562,(no reg loss)standard loss: 0.0043711, inverse loss: 0.0009124
Valid-->> Epoch [562/3000], Standardized Loss: 0.0143923, Inverse Loss: 0.0030042
Training -->> Epoch: 563,(no reg loss)standard loss: 0.0044334, inverse loss: 0.0009254
Valid-->> Epoch [563/3000], Standardized Loss: 0.0143281, Inverse Loss: 0.0029908
Training -->> Epoch: 564,(no reg loss)standard loss: 0.0043875, inverse loss: 0.0009158
Valid-->> Epoch [564/3000], Standardized Loss: 0.0143388, Inverse Loss: 0.0029931
Training -->> Epoch: 565,(no reg loss)standard loss: 0.0043498, inverse loss: 0.0009080
Valid-->> Epoch [565/3000], Standardized Loss: 0.0143659, Inverse Loss: 0.0029987
Training -->> Epoch: 566,(no reg loss)standard loss: 0.0043534, inverse loss: 0.0009087
Valid-->> Epoch [566/3000], Standardized Loss: 0.0144113, Inverse Loss: 0.0030082
Training -->> Epoch: 567,(no reg loss)standard loss: 0.0044043, inverse loss: 0.0009193
Valid-->> Epoch [567/3000], Standardized Loss: 0.0143058, Inverse Loss: 0.0029862
Training -->> Epoch: 568,(no reg loss)standard loss: 0.0044814, inverse loss: 0.0009354
Valid-->> Epoch [568/3000], Standardized Loss: 0.0143224, Inverse Loss: 0.0029896
Training -->> Epoch: 569,(no reg loss)standard loss: 0.0043677, inverse loss: 0.0009117
Valid-->> Epoch [569/3000], Standardized Loss: 0.0143243, Inverse Loss: 0.0029900
Training -->> Epoch: 570,(no reg loss)standard loss: 0.0043705, inverse loss: 0.0009123
Valid-->> Epoch [570/3000], Standardized Loss: 0.0143012, Inverse Loss: 0.0029852
Training -->> Epoch: 571,(no reg loss)standard loss: 0.0043858, inverse loss: 0.0009155
Valid-->> Epoch [571/3000], Standardized Loss: 0.0143590, Inverse Loss: 0.0029973
Training -->> Epoch: 572,(no reg loss)standard loss: 0.0044605, inverse loss: 0.0009311
Valid-->> Epoch [572/3000], Standardized Loss: 0.0142715, Inverse Loss: 0.0029790
Training -->> Epoch: 573,(no reg loss)standard loss: 0.0043023, inverse loss: 0.0008980
Valid-->> Epoch [573/3000], Standardized Loss: 0.0143998, Inverse Loss: 0.0030058
Training -->> Epoch: 574,(no reg loss)standard loss: 0.0044252, inverse loss: 0.0009237
Valid-->> Epoch [574/3000], Standardized Loss: 0.0142983, Inverse Loss: 0.0029846
Training -->> Epoch: 575,(no reg loss)standard loss: 0.0043539, inverse loss: 0.0009088
Valid-->> Epoch [575/3000], Standardized Loss: 0.0143178, Inverse Loss: 0.0029887
Training -->> Epoch: 576,(no reg loss)standard loss: 0.0044149, inverse loss: 0.0009216
Valid-->> Epoch [576/3000], Standardized Loss: 0.0143128, Inverse Loss: 0.0029876
Training -->> Epoch: 577,(no reg loss)standard loss: 0.0043232, inverse loss: 0.0009024
Valid-->> Epoch [577/3000], Standardized Loss: 0.0143382, Inverse Loss: 0.0029929
Training -->> Epoch: 578,(no reg loss)standard loss: 0.0044405, inverse loss: 0.0009269
Valid-->> Epoch [578/3000], Standardized Loss: 0.0143007, Inverse Loss: 0.0029851
Training -->> Epoch: 579,(no reg loss)standard loss: 0.0042937, inverse loss: 0.0008963
Valid-->> Epoch [579/3000], Standardized Loss: 0.0144249, Inverse Loss: 0.0030110
Training -->> Epoch: 580,(no reg loss)standard loss: 0.0044743, inverse loss: 0.0009340
Valid-->> Epoch [580/3000], Standardized Loss: 0.0142385, Inverse Loss: 0.0029721
Valid-->> Lowest loss found at epoch 580, loss: 0.0029721
Epoch 580, Masked params (inverse standardized): tensor([3.229269e+01, 4.430157e+01, 7.125854e-02, 2.634226e+01, 2.768035e+01,
        1.781420e+01, 2.143266e+01, 1.642302e+00, 1.066123e+02, 2.754184e+01,
        2.614555e+01, 4.461954e+01, 2.443195e+01, 1.504166e+01, 8.461618e+01,
        2.835009e+01, 5.669889e+00, 2.981915e+01, 1.197334e+01, 3.204011e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 581,(no reg loss)standard loss: 0.0042050, inverse loss: 0.0008778
Valid-->> Epoch [581/3000], Standardized Loss: 0.0143309, Inverse Loss: 0.0029914
Training -->> Epoch: 582,(no reg loss)standard loss: 0.0044996, inverse loss: 0.0009392
Valid-->> Epoch [582/3000], Standardized Loss: 0.0144886, Inverse Loss: 0.0030243
Training -->> Epoch: 583,(no reg loss)standard loss: 0.0044481, inverse loss: 0.0009285
Valid-->> Epoch [583/3000], Standardized Loss: 0.0142777, Inverse Loss: 0.0029803
Training -->> Epoch: 584,(no reg loss)standard loss: 0.0043017, inverse loss: 0.0008979
Valid-->> Epoch [584/3000], Standardized Loss: 0.0143706, Inverse Loss: 0.0029997
Training -->> Epoch: 585,(no reg loss)standard loss: 0.0044885, inverse loss: 0.0009369
Valid-->> Epoch [585/3000], Standardized Loss: 0.0142940, Inverse Loss: 0.0029837
Training -->> Epoch: 586,(no reg loss)standard loss: 0.0042079, inverse loss: 0.0008784
Valid-->> Epoch [586/3000], Standardized Loss: 0.0143201, Inverse Loss: 0.0029892
Training -->> Epoch: 587,(no reg loss)standard loss: 0.0044835, inverse loss: 0.0009359
Valid-->> Epoch [587/3000], Standardized Loss: 0.0143499, Inverse Loss: 0.0029954
Training -->> Epoch: 588,(no reg loss)standard loss: 0.0044977, inverse loss: 0.0009388
Valid-->> Epoch [588/3000], Standardized Loss: 0.0142548, Inverse Loss: 0.0029755
Training -->> Epoch: 589,(no reg loss)standard loss: 0.0042832, inverse loss: 0.0008941
Valid-->> Epoch [589/3000], Standardized Loss: 0.0144451, Inverse Loss: 0.0030153
Training -->> Epoch: 590,(no reg loss)standard loss: 0.0045199, inverse loss: 0.0009435
Valid-->> Epoch [590/3000], Standardized Loss: 0.0142796, Inverse Loss: 0.0029807
Training -->> Epoch: 591,(no reg loss)standard loss: 0.0044028, inverse loss: 0.0009190
Valid-->> Epoch [591/3000], Standardized Loss: 0.0143161, Inverse Loss: 0.0029883
Training -->> Epoch: 592,(no reg loss)standard loss: 0.0043901, inverse loss: 0.0009164
Valid-->> Epoch [592/3000], Standardized Loss: 0.0143298, Inverse Loss: 0.0029912
Training -->> Epoch: 593,(no reg loss)standard loss: 0.0043963, inverse loss: 0.0009177
Valid-->> Epoch [593/3000], Standardized Loss: 0.0143658, Inverse Loss: 0.0029987
Training -->> Epoch: 594,(no reg loss)standard loss: 0.0044402, inverse loss: 0.0009268
Valid-->> Epoch [594/3000], Standardized Loss: 0.0142444, Inverse Loss: 0.0029733
Training -->> Epoch: 595,(no reg loss)standard loss: 0.0042722, inverse loss: 0.0008918
Valid-->> Epoch [595/3000], Standardized Loss: 0.0144016, Inverse Loss: 0.0030062
Training -->> Epoch: 596,(no reg loss)standard loss: 0.0044888, inverse loss: 0.0009370
Valid-->> Epoch [596/3000], Standardized Loss: 0.0142898, Inverse Loss: 0.0029828
Training -->> Epoch: 597,(no reg loss)standard loss: 0.0043616, inverse loss: 0.0009104
Valid-->> Epoch [597/3000], Standardized Loss: 0.0143230, Inverse Loss: 0.0029898
Training -->> Epoch: 598,(no reg loss)standard loss: 0.0045221, inverse loss: 0.0009439
Valid-->> Epoch [598/3000], Standardized Loss: 0.0142774, Inverse Loss: 0.0029802
Training -->> Epoch: 599,(no reg loss)standard loss: 0.0043896, inverse loss: 0.0009163
Valid-->> Epoch [599/3000], Standardized Loss: 0.0143972, Inverse Loss: 0.0030053
Training -->> Epoch: 600,(no reg loss)standard loss: 0.0043939, inverse loss: 0.0009172
Valid-->> Epoch [600/3000], Standardized Loss: 0.0142506, Inverse Loss: 0.0029746
Training -->> Epoch: 601,(no reg loss)standard loss: 0.0044459, inverse loss: 0.0009280
Valid-->> Epoch [601/3000], Standardized Loss: 0.0143640, Inverse Loss: 0.0029983
Training -->> Epoch: 602,(no reg loss)standard loss: 0.0044940, inverse loss: 0.0009381
Valid-->> Epoch [602/3000], Standardized Loss: 0.0142302, Inverse Loss: 0.0029704
Valid-->> Lowest loss found at epoch 602, loss: 0.0029704
Epoch 602, Masked params (inverse standardized): tensor([3.229274e+01, 4.430219e+01, 7.032585e-02, 2.634179e+01, 2.767992e+01,
        1.781308e+01, 2.143269e+01, 1.640509e+00, 1.066132e+02, 2.754131e+01,
        2.614513e+01, 4.461703e+01, 2.443206e+01, 1.504000e+01, 8.461705e+01,
        2.834994e+01, 5.668722e+00, 2.981939e+01, 1.197113e+01, 3.204023e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 603,(no reg loss)standard loss: 0.0043600, inverse loss: 0.0009101
Valid-->> Epoch [603/3000], Standardized Loss: 0.0143002, Inverse Loss: 0.0029850
Training -->> Epoch: 604,(no reg loss)standard loss: 0.0042888, inverse loss: 0.0008952
Valid-->> Epoch [604/3000], Standardized Loss: 0.0143704, Inverse Loss: 0.0029996
Training -->> Epoch: 605,(no reg loss)standard loss: 0.0045608, inverse loss: 0.0009520
Valid-->> Epoch [605/3000], Standardized Loss: 0.0142884, Inverse Loss: 0.0029825
Training -->> Epoch: 606,(no reg loss)standard loss: 0.0043573, inverse loss: 0.0009095
Valid-->> Epoch [606/3000], Standardized Loss: 0.0142779, Inverse Loss: 0.0029803
Training -->> Epoch: 607,(no reg loss)standard loss: 0.0044298, inverse loss: 0.0009247
Valid-->> Epoch [607/3000], Standardized Loss: 0.0142895, Inverse Loss: 0.0029828
Training -->> Epoch: 608,(no reg loss)standard loss: 0.0044287, inverse loss: 0.0009244
Valid-->> Epoch [608/3000], Standardized Loss: 0.0142833, Inverse Loss: 0.0029815
Training -->> Epoch: 609,(no reg loss)standard loss: 0.0044745, inverse loss: 0.0009340
Valid-->> Epoch [609/3000], Standardized Loss: 0.0142311, Inverse Loss: 0.0029706
Training -->> Epoch: 610,(no reg loss)standard loss: 0.0044656, inverse loss: 0.0009321
Valid-->> Epoch [610/3000], Standardized Loss: 0.0142432, Inverse Loss: 0.0029731
Training -->> Epoch: 611,(no reg loss)standard loss: 0.0043306, inverse loss: 0.0009040
Valid-->> Epoch [611/3000], Standardized Loss: 0.0142796, Inverse Loss: 0.0029807
Training -->> Epoch: 612,(no reg loss)standard loss: 0.0043713, inverse loss: 0.0009124
Valid-->> Epoch [612/3000], Standardized Loss: 0.0142706, Inverse Loss: 0.0029788
Training -->> Epoch: 613,(no reg loss)standard loss: 0.0044285, inverse loss: 0.0009244
Valid-->> Epoch [613/3000], Standardized Loss: 0.0142652, Inverse Loss: 0.0029777
Training -->> Epoch: 614,(no reg loss)standard loss: 0.0044918, inverse loss: 0.0009376
Valid-->> Epoch [614/3000], Standardized Loss: 0.0142416, Inverse Loss: 0.0029728
Training -->> Epoch: 615,(no reg loss)standard loss: 0.0042766, inverse loss: 0.0008927
Valid-->> Epoch [615/3000], Standardized Loss: 0.0143960, Inverse Loss: 0.0030050
Training -->> Epoch: 616,(no reg loss)standard loss: 0.0046401, inverse loss: 0.0009686
Valid-->> Epoch [616/3000], Standardized Loss: 0.0142606, Inverse Loss: 0.0029767
Training -->> Epoch: 617,(no reg loss)standard loss: 0.0042656, inverse loss: 0.0008904
Valid-->> Epoch [617/3000], Standardized Loss: 0.0142880, Inverse Loss: 0.0029825
Training -->> Epoch: 618,(no reg loss)standard loss: 0.0045155, inverse loss: 0.0009426
Valid-->> Epoch [618/3000], Standardized Loss: 0.0142032, Inverse Loss: 0.0029648
Valid-->> Lowest loss found at epoch 618, loss: 0.0029648
Epoch 618, Masked params (inverse standardized): tensor([3.229111e+01, 4.430124e+01, 6.892586e-02, 2.634263e+01, 2.768055e+01,
        1.781263e+01, 2.143108e+01, 1.640554e+00, 1.066119e+02, 2.754218e+01,
        2.614573e+01, 4.461858e+01, 2.443145e+01, 1.504037e+01, 8.461575e+01,
        2.834991e+01, 5.667656e+00, 2.981844e+01, 1.197249e+01, 3.203854e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 619,(no reg loss)standard loss: 0.0044170, inverse loss: 0.0009220
Valid-->> Epoch [619/3000], Standardized Loss: 0.0143099, Inverse Loss: 0.0029870
Training -->> Epoch: 620,(no reg loss)standard loss: 0.0043411, inverse loss: 0.0009061
Valid-->> Epoch [620/3000], Standardized Loss: 0.0143244, Inverse Loss: 0.0029901
Training -->> Epoch: 621,(no reg loss)standard loss: 0.0044868, inverse loss: 0.0009366
Valid-->> Epoch [621/3000], Standardized Loss: 0.0141974, Inverse Loss: 0.0029635
Valid-->> Lowest loss found at epoch 621, loss: 0.0029635
Epoch 621, Masked params (inverse standardized): tensor([3.229166e+01, 4.430108e+01, 6.875038e-02, 2.634276e+01, 2.768060e+01,
        1.781248e+01, 2.143163e+01, 1.640499e+00, 1.066123e+02, 2.754229e+01,
        2.614580e+01, 4.461729e+01, 2.443152e+01, 1.503956e+01, 8.461574e+01,
        2.834986e+01, 5.668037e+00, 2.981866e+01, 1.197085e+01, 3.203912e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 622,(no reg loss)standard loss: 0.0045143, inverse loss: 0.0009423
Valid-->> Epoch [622/3000], Standardized Loss: 0.0142496, Inverse Loss: 0.0029744
Training -->> Epoch: 623,(no reg loss)standard loss: 0.0043645, inverse loss: 0.0009110
Valid-->> Epoch [623/3000], Standardized Loss: 0.0142448, Inverse Loss: 0.0029734
Training -->> Epoch: 624,(no reg loss)standard loss: 0.0043557, inverse loss: 0.0009092
Valid-->> Epoch [624/3000], Standardized Loss: 0.0142557, Inverse Loss: 0.0029757
Training -->> Epoch: 625,(no reg loss)standard loss: 0.0044712, inverse loss: 0.0009333
Valid-->> Epoch [625/3000], Standardized Loss: 0.0142451, Inverse Loss: 0.0029735
Training -->> Epoch: 626,(no reg loss)standard loss: 0.0043038, inverse loss: 0.0008984
Valid-->> Epoch [626/3000], Standardized Loss: 0.0143522, Inverse Loss: 0.0029959
Training -->> Epoch: 627,(no reg loss)standard loss: 0.0045193, inverse loss: 0.0009433
Valid-->> Epoch [627/3000], Standardized Loss: 0.0141835, Inverse Loss: 0.0029606
Valid-->> Lowest loss found at epoch 627, loss: 0.0029606
Epoch 627, Masked params (inverse standardized): tensor([3.229198e+01, 4.430108e+01, 6.937790e-02, 2.634294e+01, 2.768075e+01,
        1.781336e+01, 2.143195e+01, 1.640944e+00, 1.066126e+02, 2.754251e+01,
        2.614593e+01, 4.461838e+01, 2.443158e+01, 1.504076e+01, 8.461600e+01,
        2.834994e+01, 5.668331e+00, 2.981874e+01, 1.197233e+01, 3.203939e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 628,(no reg loss)standard loss: 0.0043623, inverse loss: 0.0009106
Valid-->> Epoch [628/3000], Standardized Loss: 0.0144309, Inverse Loss: 0.0030123
Training -->> Epoch: 629,(no reg loss)standard loss: 0.0045761, inverse loss: 0.0009552
Valid-->> Epoch [629/3000], Standardized Loss: 0.0141216, Inverse Loss: 0.0029477
Valid-->> Lowest loss found at epoch 629, loss: 0.0029477
Epoch 629, Masked params (inverse standardized): tensor([3.229509e+01, 4.430143e+01, 7.119370e-02, 2.634438e+01, 2.768227e+01,
        1.781630e+01, 2.143507e+01, 1.642624e+00, 1.066123e+02, 2.754393e+01,
        2.614747e+01, 4.462060e+01, 2.443407e+01, 1.504353e+01, 8.461640e+01,
        2.835193e+01, 5.670168e+00, 2.982145e+01, 1.197485e+01, 3.204244e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 630,(no reg loss)standard loss: 0.0042658, inverse loss: 0.0008904
Valid-->> Epoch [630/3000], Standardized Loss: 0.0143538, Inverse Loss: 0.0029962
Training -->> Epoch: 631,(no reg loss)standard loss: 0.0045377, inverse loss: 0.0009472
Valid-->> Epoch [631/3000], Standardized Loss: 0.0142779, Inverse Loss: 0.0029804
Training -->> Epoch: 632,(no reg loss)standard loss: 0.0044005, inverse loss: 0.0009185
Valid-->> Epoch [632/3000], Standardized Loss: 0.0141531, Inverse Loss: 0.0029543
Training -->> Epoch: 633,(no reg loss)standard loss: 0.0042169, inverse loss: 0.0008802
Valid-->> Epoch [633/3000], Standardized Loss: 0.0143095, Inverse Loss: 0.0029870
Training -->> Epoch: 634,(no reg loss)standard loss: 0.0045324, inverse loss: 0.0009461
Valid-->> Epoch [634/3000], Standardized Loss: 0.0141652, Inverse Loss: 0.0029568
Training -->> Epoch: 635,(no reg loss)standard loss: 0.0044930, inverse loss: 0.0009379
Valid-->> Epoch [635/3000], Standardized Loss: 0.0142131, Inverse Loss: 0.0029668
Training -->> Epoch: 636,(no reg loss)standard loss: 0.0044125, inverse loss: 0.0009211
Valid-->> Epoch [636/3000], Standardized Loss: 0.0142097, Inverse Loss: 0.0029661
Training -->> Epoch: 637,(no reg loss)standard loss: 0.0042990, inverse loss: 0.0008974
Valid-->> Epoch [637/3000], Standardized Loss: 0.0142493, Inverse Loss: 0.0029744
Training -->> Epoch: 638,(no reg loss)standard loss: 0.0046586, inverse loss: 0.0009724
Valid-->> Epoch [638/3000], Standardized Loss: 0.0141331, Inverse Loss: 0.0029501
Training -->> Epoch: 639,(no reg loss)standard loss: 0.0042438, inverse loss: 0.0008858
Valid-->> Epoch [639/3000], Standardized Loss: 0.0142789, Inverse Loss: 0.0029805
Training -->> Epoch: 640,(no reg loss)standard loss: 0.0045565, inverse loss: 0.0009511
Valid-->> Epoch [640/3000], Standardized Loss: 0.0142313, Inverse Loss: 0.0029706
Training -->> Epoch: 641,(no reg loss)standard loss: 0.0043731, inverse loss: 0.0009128
Valid-->> Epoch [641/3000], Standardized Loss: 0.0142587, Inverse Loss: 0.0029763
Training -->> Epoch: 642,(no reg loss)standard loss: 0.0045049, inverse loss: 0.0009404
Valid-->> Epoch [642/3000], Standardized Loss: 0.0142218, Inverse Loss: 0.0029686
Training -->> Epoch: 643,(no reg loss)standard loss: 0.0044814, inverse loss: 0.0009354
Valid-->> Epoch [643/3000], Standardized Loss: 0.0142442, Inverse Loss: 0.0029733
Training -->> Epoch: 644,(no reg loss)standard loss: 0.0043225, inverse loss: 0.0009023
Valid-->> Epoch [644/3000], Standardized Loss: 0.0142756, Inverse Loss: 0.0029799
Training -->> Epoch: 645,(no reg loss)standard loss: 0.0045101, inverse loss: 0.0009414
Valid-->> Epoch [645/3000], Standardized Loss: 0.0141716, Inverse Loss: 0.0029582
Training -->> Epoch: 646,(no reg loss)standard loss: 0.0043587, inverse loss: 0.0009098
Valid-->> Epoch [646/3000], Standardized Loss: 0.0142532, Inverse Loss: 0.0029752
Training -->> Epoch: 647,(no reg loss)standard loss: 0.0044111, inverse loss: 0.0009208
Valid-->> Epoch [647/3000], Standardized Loss: 0.0142087, Inverse Loss: 0.0029659
Training -->> Epoch: 648,(no reg loss)standard loss: 0.0045221, inverse loss: 0.0009439
Valid-->> Epoch [648/3000], Standardized Loss: 0.0142186, Inverse Loss: 0.0029680
Training -->> Epoch: 649,(no reg loss)standard loss: 0.0044274, inverse loss: 0.0009242
Valid-->> Epoch [649/3000], Standardized Loss: 0.0141561, Inverse Loss: 0.0029549
Training -->> Epoch: 650,(no reg loss)standard loss: 0.0044357, inverse loss: 0.0009259
Valid-->> Epoch [650/3000], Standardized Loss: 0.0142261, Inverse Loss: 0.0029695
Training -->> Epoch: 651,(no reg loss)standard loss: 0.0043399, inverse loss: 0.0009059
Valid-->> Epoch [651/3000], Standardized Loss: 0.0142284, Inverse Loss: 0.0029700
Training -->> Epoch: 652,(no reg loss)standard loss: 0.0044477, inverse loss: 0.0009284
Valid-->> Epoch [652/3000], Standardized Loss: 0.0142543, Inverse Loss: 0.0029754
Training -->> Epoch: 653,(no reg loss)standard loss: 0.0044028, inverse loss: 0.0009190
Valid-->> Epoch [653/3000], Standardized Loss: 0.0142839, Inverse Loss: 0.0029816
Training -->> Epoch: 654,(no reg loss)standard loss: 0.0043684, inverse loss: 0.0009119
Valid-->> Epoch [654/3000], Standardized Loss: 0.0142325, Inverse Loss: 0.0029709
Training -->> Epoch: 655,(no reg loss)standard loss: 0.0045741, inverse loss: 0.0009548
Valid-->> Epoch [655/3000], Standardized Loss: 0.0142155, Inverse Loss: 0.0029673
Training -->> Epoch: 656,(no reg loss)standard loss: 0.0043269, inverse loss: 0.0009032
Valid-->> Epoch [656/3000], Standardized Loss: 0.0142182, Inverse Loss: 0.0029679
Training -->> Epoch: 657,(no reg loss)standard loss: 0.0045555, inverse loss: 0.0009509
Valid-->> Epoch [657/3000], Standardized Loss: 0.0141710, Inverse Loss: 0.0029580
Training -->> Epoch: 658,(no reg loss)standard loss: 0.0042926, inverse loss: 0.0008960
Valid-->> Epoch [658/3000], Standardized Loss: 0.0142577, Inverse Loss: 0.0029761
Training -->> Epoch: 659,(no reg loss)standard loss: 0.0046893, inverse loss: 0.0009788
Valid-->> Epoch [659/3000], Standardized Loss: 0.0140854, Inverse Loss: 0.0029402
Valid-->> Lowest loss found at epoch 659, loss: 0.0029402
Epoch 659, Masked params (inverse standardized): tensor([3.229441e+01, 4.430192e+01, 7.481575e-02, 2.634457e+01, 2.768231e+01,
        1.781652e+01, 2.143438e+01, 1.646133e+00, 1.066121e+02, 2.754417e+01,
        2.614750e+01, 4.462317e+01, 2.443333e+01, 1.504426e+01, 8.461635e+01,
        2.835156e+01, 5.673313e+00, 2.982060e+01, 1.197681e+01, 3.204171e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 660,(no reg loss)standard loss: 0.0042320, inverse loss: 0.0008834
Valid-->> Epoch [660/3000], Standardized Loss: 0.0142472, Inverse Loss: 0.0029739
Training -->> Epoch: 661,(no reg loss)standard loss: 0.0045353, inverse loss: 0.0009467
Valid-->> Epoch [661/3000], Standardized Loss: 0.0141890, Inverse Loss: 0.0029618
Training -->> Epoch: 662,(no reg loss)standard loss: 0.0044390, inverse loss: 0.0009266
Valid-->> Epoch [662/3000], Standardized Loss: 0.0141064, Inverse Loss: 0.0029445
Training -->> Epoch: 663,(no reg loss)standard loss: 0.0043363, inverse loss: 0.0009052
Valid-->> Epoch [663/3000], Standardized Loss: 0.0141947, Inverse Loss: 0.0029630
Training -->> Epoch: 664,(no reg loss)standard loss: 0.0044766, inverse loss: 0.0009344
Valid-->> Epoch [664/3000], Standardized Loss: 0.0141657, Inverse Loss: 0.0029569
Training -->> Epoch: 665,(no reg loss)standard loss: 0.0043389, inverse loss: 0.0009057
Valid-->> Epoch [665/3000], Standardized Loss: 0.0141515, Inverse Loss: 0.0029540
Training -->> Epoch: 666,(no reg loss)standard loss: 0.0045066, inverse loss: 0.0009407
Valid-->> Epoch [666/3000], Standardized Loss: 0.0141704, Inverse Loss: 0.0029579
Training -->> Epoch: 667,(no reg loss)standard loss: 0.0043820, inverse loss: 0.0009147
Valid-->> Epoch [667/3000], Standardized Loss: 0.0142266, Inverse Loss: 0.0029696
Training -->> Epoch: 668,(no reg loss)standard loss: 0.0044229, inverse loss: 0.0009232
Valid-->> Epoch [668/3000], Standardized Loss: 0.0141190, Inverse Loss: 0.0029472
Training -->> Epoch: 669,(no reg loss)standard loss: 0.0044028, inverse loss: 0.0009190
Valid-->> Epoch [669/3000], Standardized Loss: 0.0142019, Inverse Loss: 0.0029645
Training -->> Epoch: 670,(no reg loss)standard loss: 0.0044283, inverse loss: 0.0009243
Valid-->> Epoch [670/3000], Standardized Loss: 0.0142055, Inverse Loss: 0.0029652
Training -->> Epoch: 671,(no reg loss)standard loss: 0.0044207, inverse loss: 0.0009228
Valid-->> Epoch [671/3000], Standardized Loss: 0.0141983, Inverse Loss: 0.0029637
Training -->> Epoch: 672,(no reg loss)standard loss: 0.0044053, inverse loss: 0.0009196
Valid-->> Epoch [672/3000], Standardized Loss: 0.0141518, Inverse Loss: 0.0029540
Training -->> Epoch: 673,(no reg loss)standard loss: 0.0044250, inverse loss: 0.0009237
Valid-->> Epoch [673/3000], Standardized Loss: 0.0142588, Inverse Loss: 0.0029764
Training -->> Epoch: 674,(no reg loss)standard loss: 0.0045270, inverse loss: 0.0009450
Valid-->> Epoch [674/3000], Standardized Loss: 0.0142495, Inverse Loss: 0.0029744
Training -->> Epoch: 675,(no reg loss)standard loss: 0.0045170, inverse loss: 0.0009429
Valid-->> Epoch [675/3000], Standardized Loss: 0.0141252, Inverse Loss: 0.0029485
Training -->> Epoch: 676,(no reg loss)standard loss: 0.0043318, inverse loss: 0.0009042
Valid-->> Epoch [676/3000], Standardized Loss: 0.0141418, Inverse Loss: 0.0029519
Training -->> Epoch: 677,(no reg loss)standard loss: 0.0044950, inverse loss: 0.0009383
Valid-->> Epoch [677/3000], Standardized Loss: 0.0141306, Inverse Loss: 0.0029496
Training -->> Epoch: 678,(no reg loss)standard loss: 0.0045157, inverse loss: 0.0009426
Valid-->> Epoch [678/3000], Standardized Loss: 0.0142181, Inverse Loss: 0.0029679
Training -->> Epoch: 679,(no reg loss)standard loss: 0.0045243, inverse loss: 0.0009444
Valid-->> Epoch [679/3000], Standardized Loss: 0.0140896, Inverse Loss: 0.0029410
Training -->> Epoch: 680,(no reg loss)standard loss: 0.0042078, inverse loss: 0.0008783
Valid-->> Epoch [680/3000], Standardized Loss: 0.0142582, Inverse Loss: 0.0029762
Training -->> Epoch: 681,(no reg loss)standard loss: 0.0048172, inverse loss: 0.0010055
Valid-->> Epoch [681/3000], Standardized Loss: 0.0140793, Inverse Loss: 0.0029389
Valid-->> Lowest loss found at epoch 681, loss: 0.0029389
Epoch 681, Masked params (inverse standardized): tensor([3.229384e+01, 4.430260e+01, 7.578087e-02, 2.634415e+01, 2.768181e+01,
        1.781756e+01, 2.143387e+01, 1.647558e+00, 1.066112e+02, 2.754386e+01,
        2.614704e+01, 4.462457e+01, 2.443232e+01, 1.504560e+01, 8.461681e+01,
        2.835082e+01, 5.674780e+00, 2.981956e+01, 1.197824e+01, 3.204122e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 682,(no reg loss)standard loss: 0.0042829, inverse loss: 0.0008940
Valid-->> Epoch [682/3000], Standardized Loss: 0.0141469, Inverse Loss: 0.0029530
Training -->> Epoch: 683,(no reg loss)standard loss: 0.0043845, inverse loss: 0.0009152
Valid-->> Epoch [683/3000], Standardized Loss: 0.0141958, Inverse Loss: 0.0029632
Training -->> Epoch: 684,(no reg loss)standard loss: 0.0045545, inverse loss: 0.0009507
Valid-->> Epoch [684/3000], Standardized Loss: 0.0141200, Inverse Loss: 0.0029474
Training -->> Epoch: 685,(no reg loss)standard loss: 0.0043688, inverse loss: 0.0009119
Valid-->> Epoch [685/3000], Standardized Loss: 0.0142389, Inverse Loss: 0.0029722
Training -->> Epoch: 686,(no reg loss)standard loss: 0.0045697, inverse loss: 0.0009539
Valid-->> Epoch [686/3000], Standardized Loss: 0.0140682, Inverse Loss: 0.0029366
Valid-->> Lowest loss found at epoch 686, loss: 0.0029366
Epoch 686, Masked params (inverse standardized): tensor([3.229303e+01, 4.430154e+01, 7.121086e-02, 2.634476e+01, 2.768236e+01,
        1.781431e+01, 2.143302e+01, 1.642527e+00, 1.066118e+02, 2.754440e+01,
        2.614755e+01, 4.461934e+01, 2.443293e+01, 1.504162e+01, 8.461629e+01,
        2.835130e+01, 5.670223e+00, 2.982006e+01, 1.197321e+01, 3.204051e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 687,(no reg loss)standard loss: 0.0043997, inverse loss: 0.0009184
Valid-->> Epoch [687/3000], Standardized Loss: 0.0141374, Inverse Loss: 0.0029510
Training -->> Epoch: 688,(no reg loss)standard loss: 0.0043660, inverse loss: 0.0009114
Valid-->> Epoch [688/3000], Standardized Loss: 0.0141098, Inverse Loss: 0.0029453
Training -->> Epoch: 689,(no reg loss)standard loss: 0.0044888, inverse loss: 0.0009370
Valid-->> Epoch [689/3000], Standardized Loss: 0.0141312, Inverse Loss: 0.0029497
Training -->> Epoch: 690,(no reg loss)standard loss: 0.0043174, inverse loss: 0.0009012
Valid-->> Epoch [690/3000], Standardized Loss: 0.0141408, Inverse Loss: 0.0029517
Training -->> Epoch: 691,(no reg loss)standard loss: 0.0044525, inverse loss: 0.0009294
Valid-->> Epoch [691/3000], Standardized Loss: 0.0141424, Inverse Loss: 0.0029521
Training -->> Epoch: 692,(no reg loss)standard loss: 0.0045620, inverse loss: 0.0009523
Valid-->> Epoch [692/3000], Standardized Loss: 0.0142263, Inverse Loss: 0.0029696
Training -->> Epoch: 693,(no reg loss)standard loss: 0.0045496, inverse loss: 0.0009497
Valid-->> Epoch [693/3000], Standardized Loss: 0.0140463, Inverse Loss: 0.0029320
Valid-->> Lowest loss found at epoch 693, loss: 0.0029320
Epoch 693, Masked params (inverse standardized): tensor([3.229384e+01, 4.430100e+01, 7.167435e-02, 2.634510e+01, 2.768271e+01,
        1.781386e+01, 2.143382e+01, 1.642000e+00, 1.066125e+02, 2.754479e+01,
        2.614795e+01, 4.461838e+01, 2.443364e+01, 1.504078e+01, 8.461595e+01,
        2.835187e+01, 5.670059e+00, 2.982086e+01, 1.197225e+01, 3.204128e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 694,(no reg loss)standard loss: 0.0043730, inverse loss: 0.0009128
Valid-->> Epoch [694/3000], Standardized Loss: 0.0141134, Inverse Loss: 0.0029460
Training -->> Epoch: 695,(no reg loss)standard loss: 0.0044435, inverse loss: 0.0009275
Valid-->> Epoch [695/3000], Standardized Loss: 0.0141110, Inverse Loss: 0.0029455
Training -->> Epoch: 696,(no reg loss)standard loss: 0.0043117, inverse loss: 0.0009000
Valid-->> Epoch [696/3000], Standardized Loss: 0.0141404, Inverse Loss: 0.0029517
Training -->> Epoch: 697,(no reg loss)standard loss: 0.0045366, inverse loss: 0.0009470
Valid-->> Epoch [697/3000], Standardized Loss: 0.0141196, Inverse Loss: 0.0029473
Training -->> Epoch: 698,(no reg loss)standard loss: 0.0043189, inverse loss: 0.0009015
Valid-->> Epoch [698/3000], Standardized Loss: 0.0141098, Inverse Loss: 0.0029453
Training -->> Epoch: 699,(no reg loss)standard loss: 0.0043297, inverse loss: 0.0009038
Valid-->> Epoch [699/3000], Standardized Loss: 0.0141322, Inverse Loss: 0.0029499
Training -->> Epoch: 700,(no reg loss)standard loss: 0.0045041, inverse loss: 0.0009402
Valid-->> Epoch [700/3000], Standardized Loss: 0.0140738, Inverse Loss: 0.0029377
Training -->> Epoch: 701,(no reg loss)standard loss: 0.0044633, inverse loss: 0.0009317
Valid-->> Epoch [701/3000], Standardized Loss: 0.0140678, Inverse Loss: 0.0029365
Training -->> Epoch: 702,(no reg loss)standard loss: 0.0042268, inverse loss: 0.0008823
Valid-->> Epoch [702/3000], Standardized Loss: 0.0141325, Inverse Loss: 0.0029500
Training -->> Epoch: 703,(no reg loss)standard loss: 0.0046419, inverse loss: 0.0009689
Valid-->> Epoch [703/3000], Standardized Loss: 0.0140689, Inverse Loss: 0.0029367
Training -->> Epoch: 704,(no reg loss)standard loss: 0.0044636, inverse loss: 0.0009317
Valid-->> Epoch [704/3000], Standardized Loss: 0.0140674, Inverse Loss: 0.0029364
Training -->> Epoch: 705,(no reg loss)standard loss: 0.0043099, inverse loss: 0.0008996
Valid-->> Epoch [705/3000], Standardized Loss: 0.0141029, Inverse Loss: 0.0029438
Training -->> Epoch: 706,(no reg loss)standard loss: 0.0045246, inverse loss: 0.0009445
Valid-->> Epoch [706/3000], Standardized Loss: 0.0140870, Inverse Loss: 0.0029405
Training -->> Epoch: 707,(no reg loss)standard loss: 0.0042958, inverse loss: 0.0008967
Valid-->> Epoch [707/3000], Standardized Loss: 0.0141201, Inverse Loss: 0.0029474
Training -->> Epoch: 708,(no reg loss)standard loss: 0.0045016, inverse loss: 0.0009397
Valid-->> Epoch [708/3000], Standardized Loss: 0.0141223, Inverse Loss: 0.0029479
Training -->> Epoch: 709,(no reg loss)standard loss: 0.0045379, inverse loss: 0.0009472
Valid-->> Epoch [709/3000], Standardized Loss: 0.0139990, Inverse Loss: 0.0029221
Valid-->> Lowest loss found at epoch 709, loss: 0.0029221
Epoch 709, Masked params (inverse standardized): tensor([3.229411e+01, 4.430170e+01, 7.201195e-02, 2.634647e+01, 2.768377e+01,
        1.781439e+01, 2.143410e+01, 1.642395e+00, 1.066130e+02, 2.754609e+01,
        2.614897e+01, 4.461822e+01, 2.443379e+01, 1.504110e+01, 8.461674e+01,
        2.835221e+01, 5.671089e+00, 2.982095e+01, 1.197200e+01, 3.204155e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 710,(no reg loss)standard loss: 0.0042686, inverse loss: 0.0008910
Valid-->> Epoch [710/3000], Standardized Loss: 0.0141827, Inverse Loss: 0.0029605
Training -->> Epoch: 711,(no reg loss)standard loss: 0.0045298, inverse loss: 0.0009455
Valid-->> Epoch [711/3000], Standardized Loss: 0.0140132, Inverse Loss: 0.0029251
Training -->> Epoch: 712,(no reg loss)standard loss: 0.0044511, inverse loss: 0.0009291
Valid-->> Epoch [712/3000], Standardized Loss: 0.0141567, Inverse Loss: 0.0029551
Training -->> Epoch: 713,(no reg loss)standard loss: 0.0043726, inverse loss: 0.0009127
Valid-->> Epoch [713/3000], Standardized Loss: 0.0140698, Inverse Loss: 0.0029369
Training -->> Epoch: 714,(no reg loss)standard loss: 0.0043684, inverse loss: 0.0009118
Valid-->> Epoch [714/3000], Standardized Loss: 0.0140968, Inverse Loss: 0.0029425
Training -->> Epoch: 715,(no reg loss)standard loss: 0.0045294, inverse loss: 0.0009455
Valid-->> Epoch [715/3000], Standardized Loss: 0.0140008, Inverse Loss: 0.0029225
Training -->> Epoch: 716,(no reg loss)standard loss: 0.0043645, inverse loss: 0.0009110
Valid-->> Epoch [716/3000], Standardized Loss: 0.0140480, Inverse Loss: 0.0029324
Training -->> Epoch: 717,(no reg loss)standard loss: 0.0043822, inverse loss: 0.0009147
Valid-->> Epoch [717/3000], Standardized Loss: 0.0141696, Inverse Loss: 0.0029578
Training -->> Epoch: 718,(no reg loss)standard loss: 0.0046426, inverse loss: 0.0009691
Valid-->> Epoch [718/3000], Standardized Loss: 0.0140033, Inverse Loss: 0.0029230
Training -->> Epoch: 719,(no reg loss)standard loss: 0.0044098, inverse loss: 0.0009205
Valid-->> Epoch [719/3000], Standardized Loss: 0.0140847, Inverse Loss: 0.0029400
Training -->> Epoch: 720,(no reg loss)standard loss: 0.0044653, inverse loss: 0.0009321
Valid-->> Epoch [720/3000], Standardized Loss: 0.0141086, Inverse Loss: 0.0029450
Training -->> Epoch: 721,(no reg loss)standard loss: 0.0044413, inverse loss: 0.0009271
Valid-->> Epoch [721/3000], Standardized Loss: 0.0140564, Inverse Loss: 0.0029341
Training -->> Epoch: 722,(no reg loss)standard loss: 0.0044562, inverse loss: 0.0009302
Valid-->> Epoch [722/3000], Standardized Loss: 0.0141574, Inverse Loss: 0.0029552
Training -->> Epoch: 723,(no reg loss)standard loss: 0.0044497, inverse loss: 0.0009288
Valid-->> Epoch [723/3000], Standardized Loss: 0.0141700, Inverse Loss: 0.0029578
Training -->> Epoch: 724,(no reg loss)standard loss: 0.0045837, inverse loss: 0.0009568
Valid-->> Epoch [724/3000], Standardized Loss: 0.0140091, Inverse Loss: 0.0029242
Training -->> Epoch: 725,(no reg loss)standard loss: 0.0043796, inverse loss: 0.0009142
Valid-->> Epoch [725/3000], Standardized Loss: 0.0140615, Inverse Loss: 0.0029352
Training -->> Epoch: 726,(no reg loss)standard loss: 0.0043429, inverse loss: 0.0009065
Valid-->> Epoch [726/3000], Standardized Loss: 0.0141512, Inverse Loss: 0.0029539
Training -->> Epoch: 727,(no reg loss)standard loss: 0.0045540, inverse loss: 0.0009506
Valid-->> Epoch [727/3000], Standardized Loss: 0.0140771, Inverse Loss: 0.0029384
Training -->> Epoch: 728,(no reg loss)standard loss: 0.0044347, inverse loss: 0.0009257
Valid-->> Epoch [728/3000], Standardized Loss: 0.0140134, Inverse Loss: 0.0029251
Training -->> Epoch: 729,(no reg loss)standard loss: 0.0044551, inverse loss: 0.0009300
Valid-->> Epoch [729/3000], Standardized Loss: 0.0140828, Inverse Loss: 0.0029396
Training -->> Epoch: 730,(no reg loss)standard loss: 0.0043672, inverse loss: 0.0009116
Valid-->> Epoch [730/3000], Standardized Loss: 0.0140384, Inverse Loss: 0.0029304
Training -->> Epoch: 731,(no reg loss)standard loss: 0.0043205, inverse loss: 0.0009018
Valid-->> Epoch [731/3000], Standardized Loss: 0.0140467, Inverse Loss: 0.0029321
Training -->> Epoch: 732,(no reg loss)standard loss: 0.0044941, inverse loss: 0.0009381
Valid-->> Epoch [732/3000], Standardized Loss: 0.0141087, Inverse Loss: 0.0029450
Training -->> Epoch: 733,(no reg loss)standard loss: 0.0043715, inverse loss: 0.0009125
Valid-->> Epoch [733/3000], Standardized Loss: 0.0140546, Inverse Loss: 0.0029337
Training -->> Epoch: 734,(no reg loss)standard loss: 0.0044873, inverse loss: 0.0009367
Valid-->> Epoch [734/3000], Standardized Loss: 0.0141227, Inverse Loss: 0.0029480
Training -->> Epoch: 735,(no reg loss)standard loss: 0.0043773, inverse loss: 0.0009137
Valid-->> Epoch [735/3000], Standardized Loss: 0.0139965, Inverse Loss: 0.0029216
Valid-->> Lowest loss found at epoch 735, loss: 0.0029216
Epoch 735, Masked params (inverse standardized): tensor([3.229144e+01, 4.430126e+01, 6.735802e-02, 2.634686e+01, 2.768382e+01,
        1.781099e+01, 2.143142e+01, 1.637756e+00, 1.066125e+02, 2.754638e+01,
        2.614901e+01, 4.461411e+01, 2.443249e+01, 1.503756e+01, 8.461588e+01,
        2.835150e+01, 5.666040e+00, 2.981928e+01, 1.196825e+01, 3.203894e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 736,(no reg loss)standard loss: 0.0044014, inverse loss: 0.0009187
Valid-->> Epoch [736/3000], Standardized Loss: 0.0140668, Inverse Loss: 0.0029363
Training -->> Epoch: 737,(no reg loss)standard loss: 0.0043513, inverse loss: 0.0009083
Valid-->> Epoch [737/3000], Standardized Loss: 0.0140061, Inverse Loss: 0.0029236
Training -->> Epoch: 738,(no reg loss)standard loss: 0.0044932, inverse loss: 0.0009379
Valid-->> Epoch [738/3000], Standardized Loss: 0.0139617, Inverse Loss: 0.0029143
Valid-->> Lowest loss found at epoch 738, loss: 0.0029143
Epoch 738, Masked params (inverse standardized): tensor([3.229250e+01, 4.430164e+01, 7.166100e-02, 2.634753e+01, 2.768444e+01,
        1.781296e+01, 2.143247e+01, 1.642344e+00, 1.066122e+02, 2.754715e+01,
        2.614961e+01, 4.461832e+01, 2.443279e+01, 1.504007e+01, 8.461632e+01,
        2.835191e+01, 5.670261e+00, 2.981969e+01, 1.197186e+01, 3.203997e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 739,(no reg loss)standard loss: 0.0043057, inverse loss: 0.0008988
Valid-->> Epoch [739/3000], Standardized Loss: 0.0140640, Inverse Loss: 0.0029357
Training -->> Epoch: 740,(no reg loss)standard loss: 0.0045239, inverse loss: 0.0009443
Valid-->> Epoch [740/3000], Standardized Loss: 0.0140217, Inverse Loss: 0.0029269
Training -->> Epoch: 741,(no reg loss)standard loss: 0.0044259, inverse loss: 0.0009239
Valid-->> Epoch [741/3000], Standardized Loss: 0.0140838, Inverse Loss: 0.0029398
Training -->> Epoch: 742,(no reg loss)standard loss: 0.0043947, inverse loss: 0.0009173
Valid-->> Epoch [742/3000], Standardized Loss: 0.0140988, Inverse Loss: 0.0029430
Training -->> Epoch: 743,(no reg loss)standard loss: 0.0045791, inverse loss: 0.0009558
Valid-->> Epoch [743/3000], Standardized Loss: 0.0140011, Inverse Loss: 0.0029226
Training -->> Epoch: 744,(no reg loss)standard loss: 0.0044622, inverse loss: 0.0009314
Valid-->> Epoch [744/3000], Standardized Loss: 0.0140295, Inverse Loss: 0.0029285
Training -->> Epoch: 745,(no reg loss)standard loss: 0.0043620, inverse loss: 0.0009105
Valid-->> Epoch [745/3000], Standardized Loss: 0.0140181, Inverse Loss: 0.0029261
Training -->> Epoch: 746,(no reg loss)standard loss: 0.0045662, inverse loss: 0.0009531
Valid-->> Epoch [746/3000], Standardized Loss: 0.0141010, Inverse Loss: 0.0029434
Training -->> Epoch: 747,(no reg loss)standard loss: 0.0043714, inverse loss: 0.0009125
Valid-->> Epoch [747/3000], Standardized Loss: 0.0140232, Inverse Loss: 0.0029272
Training -->> Epoch: 748,(no reg loss)standard loss: 0.0045306, inverse loss: 0.0009457
Valid-->> Epoch [748/3000], Standardized Loss: 0.0139936, Inverse Loss: 0.0029210
Training -->> Epoch: 749,(no reg loss)standard loss: 0.0043726, inverse loss: 0.0009127
Valid-->> Epoch [749/3000], Standardized Loss: 0.0141298, Inverse Loss: 0.0029494
Training -->> Epoch: 750,(no reg loss)standard loss: 0.0045996, inverse loss: 0.0009601
Valid-->> Epoch [750/3000], Standardized Loss: 0.0140337, Inverse Loss: 0.0029294
Training -->> Epoch: 751,(no reg loss)standard loss: 0.0045185, inverse loss: 0.0009432
Valid-->> Epoch [751/3000], Standardized Loss: 0.0139583, Inverse Loss: 0.0029136
Valid-->> Lowest loss found at epoch 751, loss: 0.0029136
Epoch 751, Masked params (inverse standardized): tensor([3.229333e+01, 4.430209e+01, 7.080841e-02, 2.634690e+01, 2.768410e+01,
        1.781290e+01, 2.143331e+01, 1.641321e+00, 1.066135e+02, 2.754659e+01,
        2.614928e+01, 4.461669e+01, 2.443367e+01, 1.503953e+01, 8.461698e+01,
        2.835235e+01, 5.670015e+00, 2.982071e+01, 1.197043e+01, 3.204086e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 752,(no reg loss)standard loss: 0.0042526, inverse loss: 0.0008877
Valid-->> Epoch [752/3000], Standardized Loss: 0.0140405, Inverse Loss: 0.0029308
Training -->> Epoch: 753,(no reg loss)standard loss: 0.0045151, inverse loss: 0.0009425
Valid-->> Epoch [753/3000], Standardized Loss: 0.0140810, Inverse Loss: 0.0029393
Training -->> Epoch: 754,(no reg loss)standard loss: 0.0043577, inverse loss: 0.0009096
Valid-->> Epoch [754/3000], Standardized Loss: 0.0140538, Inverse Loss: 0.0029336
Training -->> Epoch: 755,(no reg loss)standard loss: 0.0045009, inverse loss: 0.0009395
Valid-->> Epoch [755/3000], Standardized Loss: 0.0140123, Inverse Loss: 0.0029249
Training -->> Epoch: 756,(no reg loss)standard loss: 0.0044967, inverse loss: 0.0009386
Valid-->> Epoch [756/3000], Standardized Loss: 0.0140057, Inverse Loss: 0.0029235
Training -->> Epoch: 757,(no reg loss)standard loss: 0.0044856, inverse loss: 0.0009363
Valid-->> Epoch [757/3000], Standardized Loss: 0.0139912, Inverse Loss: 0.0029205
Training -->> Epoch: 758,(no reg loss)standard loss: 0.0043842, inverse loss: 0.0009151
Valid-->> Epoch [758/3000], Standardized Loss: 0.0140195, Inverse Loss: 0.0029264
Training -->> Epoch: 759,(no reg loss)standard loss: 0.0044552, inverse loss: 0.0009300
Valid-->> Epoch [759/3000], Standardized Loss: 0.0140501, Inverse Loss: 0.0029328
Training -->> Epoch: 760,(no reg loss)standard loss: 0.0045366, inverse loss: 0.0009470
Valid-->> Epoch [760/3000], Standardized Loss: 0.0140119, Inverse Loss: 0.0029248
Training -->> Epoch: 761,(no reg loss)standard loss: 0.0044940, inverse loss: 0.0009381
Valid-->> Epoch [761/3000], Standardized Loss: 0.0140030, Inverse Loss: 0.0029230
Training -->> Epoch: 762,(no reg loss)standard loss: 0.0044977, inverse loss: 0.0009388
Valid-->> Epoch [762/3000], Standardized Loss: 0.0140284, Inverse Loss: 0.0029283
Training -->> Epoch: 763,(no reg loss)standard loss: 0.0043627, inverse loss: 0.0009107
Valid-->> Epoch [763/3000], Standardized Loss: 0.0140438, Inverse Loss: 0.0029315
Training -->> Epoch: 764,(no reg loss)standard loss: 0.0044924, inverse loss: 0.0009377
Valid-->> Epoch [764/3000], Standardized Loss: 0.0140404, Inverse Loss: 0.0029308
Training -->> Epoch: 765,(no reg loss)standard loss: 0.0046273, inverse loss: 0.0009659
Valid-->> Epoch [765/3000], Standardized Loss: 0.0139274, Inverse Loss: 0.0029072
Valid-->> Lowest loss found at epoch 765, loss: 0.0029072
Epoch 765, Masked params (inverse standardized): tensor([3.229399e+01, 4.430117e+01, 7.333946e-02, 2.634745e+01, 2.768452e+01,
        1.781461e+01, 2.143394e+01, 1.643364e+00, 1.066119e+02, 2.754712e+01,
        2.614971e+01, 4.461911e+01, 2.443359e+01, 1.504157e+01, 8.461578e+01,
        2.835238e+01, 5.671404e+00, 2.982068e+01, 1.197296e+01, 3.204139e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 766,(no reg loss)standard loss: 0.0043367, inverse loss: 0.0009052
Valid-->> Epoch [766/3000], Standardized Loss: 0.0140456, Inverse Loss: 0.0029319
Training -->> Epoch: 767,(no reg loss)standard loss: 0.0045083, inverse loss: 0.0009411
Valid-->> Epoch [767/3000], Standardized Loss: 0.0140084, Inverse Loss: 0.0029241
Training -->> Epoch: 768,(no reg loss)standard loss: 0.0044198, inverse loss: 0.0009226
Valid-->> Epoch [768/3000], Standardized Loss: 0.0139358, Inverse Loss: 0.0029089
Training -->> Epoch: 769,(no reg loss)standard loss: 0.0043739, inverse loss: 0.0009130
Valid-->> Epoch [769/3000], Standardized Loss: 0.0140262, Inverse Loss: 0.0029278
Training -->> Epoch: 770,(no reg loss)standard loss: 0.0044862, inverse loss: 0.0009364
Valid-->> Epoch [770/3000], Standardized Loss: 0.0140159, Inverse Loss: 0.0029257
Training -->> Epoch: 771,(no reg loss)standard loss: 0.0045856, inverse loss: 0.0009572
Valid-->> Epoch [771/3000], Standardized Loss: 0.0139164, Inverse Loss: 0.0029049
Valid-->> Lowest loss found at epoch 771, loss: 0.0029049
Epoch 771, Masked params (inverse standardized): tensor([3.229385e+01, 4.430104e+01, 7.162666e-02, 2.634780e+01, 2.768475e+01,
        1.781404e+01, 2.143386e+01, 1.642328e+00, 1.066121e+02, 2.754746e+01,
        2.614994e+01, 4.461845e+01, 2.443373e+01, 1.504087e+01, 8.461589e+01,
        2.835247e+01, 5.670347e+00, 2.982080e+01, 1.197228e+01, 3.204130e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 772,(no reg loss)standard loss: 0.0043682, inverse loss: 0.0009118
Valid-->> Epoch [772/3000], Standardized Loss: 0.0139775, Inverse Loss: 0.0029176
Training -->> Epoch: 773,(no reg loss)standard loss: 0.0043532, inverse loss: 0.0009087
Valid-->> Epoch [773/3000], Standardized Loss: 0.0140784, Inverse Loss: 0.0029387
Training -->> Epoch: 774,(no reg loss)standard loss: 0.0047174, inverse loss: 0.0009847
Valid-->> Epoch [774/3000], Standardized Loss: 0.0139234, Inverse Loss: 0.0029064
Training -->> Epoch: 775,(no reg loss)standard loss: 0.0043490, inverse loss: 0.0009078
Valid-->> Epoch [775/3000], Standardized Loss: 0.0139469, Inverse Loss: 0.0029113
Training -->> Epoch: 776,(no reg loss)standard loss: 0.0043211, inverse loss: 0.0009020
Valid-->> Epoch [776/3000], Standardized Loss: 0.0141225, Inverse Loss: 0.0029479
Training -->> Epoch: 777,(no reg loss)standard loss: 0.0047367, inverse loss: 0.0009887
Valid-->> Epoch [777/3000], Standardized Loss: 0.0139144, Inverse Loss: 0.0029045
Valid-->> Lowest loss found at epoch 777, loss: 0.0029045
Epoch 777, Masked params (inverse standardized): tensor([3.229406e+01, 4.430167e+01, 7.497406e-02, 2.634771e+01, 2.768452e+01,
        1.781650e+01, 2.143407e+01, 1.645699e+00, 1.066127e+02, 2.754734e+01,
        2.614972e+01, 4.462169e+01, 2.443296e+01, 1.504369e+01, 8.461646e+01,
        2.835187e+01, 5.673470e+00, 2.982004e+01, 1.197552e+01, 3.204129e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 778,(no reg loss)standard loss: 0.0043434, inverse loss: 0.0009066
Valid-->> Epoch [778/3000], Standardized Loss: 0.0140220, Inverse Loss: 0.0029269
Training -->> Epoch: 779,(no reg loss)standard loss: 0.0045232, inverse loss: 0.0009442
Valid-->> Epoch [779/3000], Standardized Loss: 0.0139550, Inverse Loss: 0.0029129
Training -->> Epoch: 780,(no reg loss)standard loss: 0.0044507, inverse loss: 0.0009290
Valid-->> Epoch [780/3000], Standardized Loss: 0.0140156, Inverse Loss: 0.0029256
Training -->> Epoch: 781,(no reg loss)standard loss: 0.0044540, inverse loss: 0.0009297
Valid-->> Epoch [781/3000], Standardized Loss: 0.0139522, Inverse Loss: 0.0029124
Training -->> Epoch: 782,(no reg loss)standard loss: 0.0043653, inverse loss: 0.0009112
Valid-->> Epoch [782/3000], Standardized Loss: 0.0139357, Inverse Loss: 0.0029089
Training -->> Epoch: 783,(no reg loss)standard loss: 0.0045624, inverse loss: 0.0009524
Valid-->> Epoch [783/3000], Standardized Loss: 0.0139495, Inverse Loss: 0.0029118
Training -->> Epoch: 784,(no reg loss)standard loss: 0.0043352, inverse loss: 0.0009049
Valid-->> Epoch [784/3000], Standardized Loss: 0.0139808, Inverse Loss: 0.0029183
Training -->> Epoch: 785,(no reg loss)standard loss: 0.0045684, inverse loss: 0.0009536
Valid-->> Epoch [785/3000], Standardized Loss: 0.0139558, Inverse Loss: 0.0029131
Training -->> Epoch: 786,(no reg loss)standard loss: 0.0044807, inverse loss: 0.0009353
Valid-->> Epoch [786/3000], Standardized Loss: 0.0139853, Inverse Loss: 0.0029193
Training -->> Epoch: 787,(no reg loss)standard loss: 0.0044306, inverse loss: 0.0009248
Valid-->> Epoch [787/3000], Standardized Loss: 0.0140020, Inverse Loss: 0.0029228
Training -->> Epoch: 788,(no reg loss)standard loss: 0.0045051, inverse loss: 0.0009404
Valid-->> Epoch [788/3000], Standardized Loss: 0.0139337, Inverse Loss: 0.0029085
Training -->> Epoch: 789,(no reg loss)standard loss: 0.0044242, inverse loss: 0.0009235
Valid-->> Epoch [789/3000], Standardized Loss: 0.0139828, Inverse Loss: 0.0029188
Training -->> Epoch: 790,(no reg loss)standard loss: 0.0044503, inverse loss: 0.0009290
Valid-->> Epoch [790/3000], Standardized Loss: 0.0139871, Inverse Loss: 0.0029196
Training -->> Epoch: 791,(no reg loss)standard loss: 0.0045266, inverse loss: 0.0009449
Valid-->> Epoch [791/3000], Standardized Loss: 0.0139273, Inverse Loss: 0.0029072
Training -->> Epoch: 792,(no reg loss)standard loss: 0.0044153, inverse loss: 0.0009216
Valid-->> Epoch [792/3000], Standardized Loss: 0.0140130, Inverse Loss: 0.0029250
Training -->> Epoch: 793,(no reg loss)standard loss: 0.0044590, inverse loss: 0.0009308
Valid-->> Epoch [793/3000], Standardized Loss: 0.0138581, Inverse Loss: 0.0028927
Valid-->> Lowest loss found at epoch 793, loss: 0.0028927
Epoch 793, Masked params (inverse standardized): tensor([3.229290e+01, 4.430056e+01, 6.930542e-02, 2.634948e+01, 2.768627e+01,
        1.781306e+01, 2.143289e+01, 1.640465e+00, 1.066125e+02, 2.754916e+01,
        2.615144e+01, 4.461742e+01, 2.443406e+01, 1.504005e+01, 8.461557e+01,
        2.835345e+01, 5.668356e+00, 2.982077e+01, 1.197130e+01, 3.204037e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 794,(no reg loss)standard loss: 0.0043666, inverse loss: 0.0009115
Valid-->> Epoch [794/3000], Standardized Loss: 0.0140197, Inverse Loss: 0.0029264
Training -->> Epoch: 795,(no reg loss)standard loss: 0.0045274, inverse loss: 0.0009450
Valid-->> Epoch [795/3000], Standardized Loss: 0.0139735, Inverse Loss: 0.0029168
Training -->> Epoch: 796,(no reg loss)standard loss: 0.0044586, inverse loss: 0.0009307
Valid-->> Epoch [796/3000], Standardized Loss: 0.0139121, Inverse Loss: 0.0029040
Training -->> Epoch: 797,(no reg loss)standard loss: 0.0043263, inverse loss: 0.0009031
Valid-->> Epoch [797/3000], Standardized Loss: 0.0140167, Inverse Loss: 0.0029258
Training -->> Epoch: 798,(no reg loss)standard loss: 0.0045958, inverse loss: 0.0009593
Valid-->> Epoch [798/3000], Standardized Loss: 0.0139298, Inverse Loss: 0.0029077
Training -->> Epoch: 799,(no reg loss)standard loss: 0.0044915, inverse loss: 0.0009375
Valid-->> Epoch [799/3000], Standardized Loss: 0.0139235, Inverse Loss: 0.0029064
Training -->> Epoch: 800,(no reg loss)standard loss: 0.0044348, inverse loss: 0.0009257
Valid-->> Epoch [800/3000], Standardized Loss: 0.0140401, Inverse Loss: 0.0029307
Training -->> Epoch: 801,(no reg loss)standard loss: 0.0044897, inverse loss: 0.0009372
Valid-->> Epoch [801/3000], Standardized Loss: 0.0138789, Inverse Loss: 0.0028971
Training -->> Epoch: 802,(no reg loss)standard loss: 0.0043792, inverse loss: 0.0009141
Valid-->> Epoch [802/3000], Standardized Loss: 0.0141009, Inverse Loss: 0.0029434
Training -->> Epoch: 803,(no reg loss)standard loss: 0.0045895, inverse loss: 0.0009580
Valid-->> Epoch [803/3000], Standardized Loss: 0.0138973, Inverse Loss: 0.0029009
Training -->> Epoch: 804,(no reg loss)standard loss: 0.0044456, inverse loss: 0.0009280
Valid-->> Epoch [804/3000], Standardized Loss: 0.0139643, Inverse Loss: 0.0029149
Training -->> Epoch: 805,(no reg loss)standard loss: 0.0045594, inverse loss: 0.0009517
Valid-->> Epoch [805/3000], Standardized Loss: 0.0139604, Inverse Loss: 0.0029141
Training -->> Epoch: 806,(no reg loss)standard loss: 0.0044178, inverse loss: 0.0009222
Valid-->> Epoch [806/3000], Standardized Loss: 0.0140022, Inverse Loss: 0.0029228
Training -->> Epoch: 807,(no reg loss)standard loss: 0.0045305, inverse loss: 0.0009457
Valid-->> Epoch [807/3000], Standardized Loss: 0.0139190, Inverse Loss: 0.0029054
Training -->> Epoch: 808,(no reg loss)standard loss: 0.0044040, inverse loss: 0.0009193
Valid-->> Epoch [808/3000], Standardized Loss: 0.0139539, Inverse Loss: 0.0029127
Training -->> Epoch: 809,(no reg loss)standard loss: 0.0045612, inverse loss: 0.0009521
Valid-->> Epoch [809/3000], Standardized Loss: 0.0139636, Inverse Loss: 0.0029147
Training -->> Epoch: 810,(no reg loss)standard loss: 0.0043905, inverse loss: 0.0009165
Valid-->> Epoch [810/3000], Standardized Loss: 0.0139343, Inverse Loss: 0.0029086
Training -->> Epoch: 811,(no reg loss)standard loss: 0.0044894, inverse loss: 0.0009371
Valid-->> Epoch [811/3000], Standardized Loss: 0.0139425, Inverse Loss: 0.0029103
Training -->> Epoch: 812,(no reg loss)standard loss: 0.0045782, inverse loss: 0.0009556
Valid-->> Epoch [812/3000], Standardized Loss: 0.0138716, Inverse Loss: 0.0028955
Training -->> Epoch: 813,(no reg loss)standard loss: 0.0044484, inverse loss: 0.0009286
Valid-->> Epoch [813/3000], Standardized Loss: 0.0140129, Inverse Loss: 0.0029250
Training -->> Epoch: 814,(no reg loss)standard loss: 0.0046165, inverse loss: 0.0009636
Valid-->> Epoch [814/3000], Standardized Loss: 0.0138394, Inverse Loss: 0.0028888
Valid-->> Lowest loss found at epoch 814, loss: 0.0028888
Epoch 814, Masked params (inverse standardized): tensor([3.229396e+01, 4.430210e+01, 7.209778e-02, 2.634922e+01, 2.768602e+01,
        1.781488e+01, 2.143393e+01, 1.643011e+00, 1.066122e+02, 2.754894e+01,
        2.615119e+01, 4.461976e+01, 2.443435e+01, 1.504210e+01, 8.461687e+01,
        2.835339e+01, 5.670727e+00, 2.982123e+01, 1.197361e+01, 3.204142e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 815,(no reg loss)standard loss: 0.0042572, inverse loss: 0.0008886
Valid-->> Epoch [815/3000], Standardized Loss: 0.0139217, Inverse Loss: 0.0029060
Training -->> Epoch: 816,(no reg loss)standard loss: 0.0045728, inverse loss: 0.0009545
Valid-->> Epoch [816/3000], Standardized Loss: 0.0139772, Inverse Loss: 0.0029176
Training -->> Epoch: 817,(no reg loss)standard loss: 0.0045190, inverse loss: 0.0009433
Valid-->> Epoch [817/3000], Standardized Loss: 0.0138311, Inverse Loss: 0.0028871
Valid-->> Lowest loss found at epoch 817, loss: 0.0028871
Epoch 817, Masked params (inverse standardized): tensor([3.229411e+01, 4.430222e+01, 7.019615e-02, 2.634973e+01, 2.768638e+01,
        1.781336e+01, 2.143408e+01, 1.640226e+00, 1.066125e+02, 2.754945e+01,
        2.615155e+01, 4.461597e+01, 2.443418e+01, 1.503959e+01, 8.461674e+01,
        2.835334e+01, 5.668688e+00, 2.982116e+01, 1.196991e+01, 3.204166e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 818,(no reg loss)standard loss: 0.0043238, inverse loss: 0.0009025
Valid-->> Epoch [818/3000], Standardized Loss: 0.0139330, Inverse Loss: 0.0029083
Training -->> Epoch: 819,(no reg loss)standard loss: 0.0045841, inverse loss: 0.0009569
Valid-->> Epoch [819/3000], Standardized Loss: 0.0138731, Inverse Loss: 0.0028959
Training -->> Epoch: 820,(no reg loss)standard loss: 0.0044997, inverse loss: 0.0009393
Valid-->> Epoch [820/3000], Standardized Loss: 0.0138231, Inverse Loss: 0.0028854
Valid-->> Lowest loss found at epoch 820, loss: 0.0028854
Epoch 820, Masked params (inverse standardized): tensor([3.229376e+01, 4.430100e+01, 7.063293e-02, 2.635001e+01, 2.768658e+01,
        1.781277e+01, 2.143373e+01, 1.640974e+00, 1.066116e+02, 2.754973e+01,
        2.615175e+01, 4.461629e+01, 2.443422e+01, 1.503911e+01, 8.461560e+01,
        2.835343e+01, 5.669680e+00, 2.982112e+01, 1.197001e+01, 3.204128e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 821,(no reg loss)standard loss: 0.0043694, inverse loss: 0.0009121
Valid-->> Epoch [821/3000], Standardized Loss: 0.0139509, Inverse Loss: 0.0029121
Training -->> Epoch: 822,(no reg loss)standard loss: 0.0045863, inverse loss: 0.0009573
Valid-->> Epoch [822/3000], Standardized Loss: 0.0138941, Inverse Loss: 0.0029002
Training -->> Epoch: 823,(no reg loss)standard loss: 0.0044018, inverse loss: 0.0009188
Valid-->> Epoch [823/3000], Standardized Loss: 0.0139163, Inverse Loss: 0.0029049
Training -->> Epoch: 824,(no reg loss)standard loss: 0.0045519, inverse loss: 0.0009502
Valid-->> Epoch [824/3000], Standardized Loss: 0.0139369, Inverse Loss: 0.0029092
Training -->> Epoch: 825,(no reg loss)standard loss: 0.0044704, inverse loss: 0.0009332
Valid-->> Epoch [825/3000], Standardized Loss: 0.0139024, Inverse Loss: 0.0029020
Training -->> Epoch: 826,(no reg loss)standard loss: 0.0044894, inverse loss: 0.0009371
Valid-->> Epoch [826/3000], Standardized Loss: 0.0138811, Inverse Loss: 0.0028975
Training -->> Epoch: 827,(no reg loss)standard loss: 0.0044869, inverse loss: 0.0009366
Valid-->> Epoch [827/3000], Standardized Loss: 0.0138826, Inverse Loss: 0.0028978
Training -->> Epoch: 828,(no reg loss)standard loss: 0.0043496, inverse loss: 0.0009079
Valid-->> Epoch [828/3000], Standardized Loss: 0.0139541, Inverse Loss: 0.0029128
Training -->> Epoch: 829,(no reg loss)standard loss: 0.0045268, inverse loss: 0.0009449
Valid-->> Epoch [829/3000], Standardized Loss: 0.0139190, Inverse Loss: 0.0029054
Training -->> Epoch: 830,(no reg loss)standard loss: 0.0045506, inverse loss: 0.0009499
Valid-->> Epoch [830/3000], Standardized Loss: 0.0139026, Inverse Loss: 0.0029020
Training -->> Epoch: 831,(no reg loss)standard loss: 0.0044284, inverse loss: 0.0009244
Valid-->> Epoch [831/3000], Standardized Loss: 0.0139164, Inverse Loss: 0.0029049
Training -->> Epoch: 832,(no reg loss)standard loss: 0.0045360, inverse loss: 0.0009468
Valid-->> Epoch [832/3000], Standardized Loss: 0.0138861, Inverse Loss: 0.0028986
Training -->> Epoch: 833,(no reg loss)standard loss: 0.0044191, inverse loss: 0.0009224
Valid-->> Epoch [833/3000], Standardized Loss: 0.0138891, Inverse Loss: 0.0028992
Training -->> Epoch: 834,(no reg loss)standard loss: 0.0043931, inverse loss: 0.0009170
Valid-->> Epoch [834/3000], Standardized Loss: 0.0138862, Inverse Loss: 0.0028986
Training -->> Epoch: 835,(no reg loss)standard loss: 0.0045344, inverse loss: 0.0009465
Valid-->> Epoch [835/3000], Standardized Loss: 0.0139948, Inverse Loss: 0.0029213
Training -->> Epoch: 836,(no reg loss)standard loss: 0.0044692, inverse loss: 0.0009329
Valid-->> Epoch [836/3000], Standardized Loss: 0.0139310, Inverse Loss: 0.0029079
Training -->> Epoch: 837,(no reg loss)standard loss: 0.0044410, inverse loss: 0.0009270
Valid-->> Epoch [837/3000], Standardized Loss: 0.0140118, Inverse Loss: 0.0029248
Training -->> Epoch: 838,(no reg loss)standard loss: 0.0047180, inverse loss: 0.0009848
Valid-->> Epoch [838/3000], Standardized Loss: 0.0138786, Inverse Loss: 0.0028970
Training -->> Epoch: 839,(no reg loss)standard loss: 0.0043516, inverse loss: 0.0009083
Valid-->> Epoch [839/3000], Standardized Loss: 0.0138764, Inverse Loss: 0.0028965
Training -->> Epoch: 840,(no reg loss)standard loss: 0.0044636, inverse loss: 0.0009317
Valid-->> Epoch [840/3000], Standardized Loss: 0.0138761, Inverse Loss: 0.0028965
Training -->> Epoch: 841,(no reg loss)standard loss: 0.0044781, inverse loss: 0.0009347
Valid-->> Epoch [841/3000], Standardized Loss: 0.0139309, Inverse Loss: 0.0029079
Training -->> Epoch: 842,(no reg loss)standard loss: 0.0045215, inverse loss: 0.0009438
Valid-->> Epoch [842/3000], Standardized Loss: 0.0140150, Inverse Loss: 0.0029255
Training -->> Epoch: 843,(no reg loss)standard loss: 0.0045736, inverse loss: 0.0009547
Valid-->> Epoch [843/3000], Standardized Loss: 0.0138070, Inverse Loss: 0.0028821
Valid-->> Lowest loss found at epoch 843, loss: 0.0028821
Epoch 843, Masked params (inverse standardized): tensor([3.229375e+01, 4.430178e+01, 7.007408e-02, 2.634997e+01, 2.768652e+01,
        1.781397e+01, 2.143373e+01, 1.641035e+00, 1.066124e+02, 2.754970e+01,
        2.615169e+01, 4.461778e+01, 2.443431e+01, 1.504080e+01, 8.461658e+01,
        2.835344e+01, 5.668644e+00, 2.982121e+01, 1.197182e+01, 3.204126e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 844,(no reg loss)standard loss: 0.0044348, inverse loss: 0.0009257
Valid-->> Epoch [844/3000], Standardized Loss: 0.0139104, Inverse Loss: 0.0029036
Training -->> Epoch: 845,(no reg loss)standard loss: 0.0045433, inverse loss: 0.0009484
Valid-->> Epoch [845/3000], Standardized Loss: 0.0138838, Inverse Loss: 0.0028981
Training -->> Epoch: 846,(no reg loss)standard loss: 0.0044234, inverse loss: 0.0009233
Valid-->> Epoch [846/3000], Standardized Loss: 0.0139700, Inverse Loss: 0.0029161
Training -->> Epoch: 847,(no reg loss)standard loss: 0.0046457, inverse loss: 0.0009697
Valid-->> Epoch [847/3000], Standardized Loss: 0.0137805, Inverse Loss: 0.0028765
Valid-->> Lowest loss found at epoch 847, loss: 0.0028765
Epoch 847, Masked params (inverse standardized): tensor([3.229421e+01, 4.430119e+01, 7.225227e-02, 2.635053e+01, 2.768704e+01,
        1.781518e+01, 2.143417e+01, 1.643433e+00, 1.066112e+02, 2.755020e+01,
        2.615224e+01, 4.462038e+01, 2.443454e+01, 1.504259e+01, 8.461588e+01,
        2.835382e+01, 5.671007e+00, 2.982142e+01, 1.197431e+01, 3.204172e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 848,(no reg loss)standard loss: 0.0044177, inverse loss: 0.0009222
Valid-->> Epoch [848/3000], Standardized Loss: 0.0138863, Inverse Loss: 0.0028986
Training -->> Epoch: 849,(no reg loss)standard loss: 0.0044807, inverse loss: 0.0009353
Valid-->> Epoch [849/3000], Standardized Loss: 0.0138544, Inverse Loss: 0.0028919
Training -->> Epoch: 850,(no reg loss)standard loss: 0.0044139, inverse loss: 0.0009213
Valid-->> Epoch [850/3000], Standardized Loss: 0.0139094, Inverse Loss: 0.0029034
Training -->> Epoch: 851,(no reg loss)standard loss: 0.0045721, inverse loss: 0.0009544
Valid-->> Epoch [851/3000], Standardized Loss: 0.0138928, Inverse Loss: 0.0029000
Training -->> Epoch: 852,(no reg loss)standard loss: 0.0045531, inverse loss: 0.0009504
Valid-->> Epoch [852/3000], Standardized Loss: 0.0137973, Inverse Loss: 0.0028800
Training -->> Epoch: 853,(no reg loss)standard loss: 0.0044453, inverse loss: 0.0009279
Valid-->> Epoch [853/3000], Standardized Loss: 0.0139050, Inverse Loss: 0.0029025
Training -->> Epoch: 854,(no reg loss)standard loss: 0.0044174, inverse loss: 0.0009221
Valid-->> Epoch [854/3000], Standardized Loss: 0.0138230, Inverse Loss: 0.0028854
Training -->> Epoch: 855,(no reg loss)standard loss: 0.0045273, inverse loss: 0.0009450
Valid-->> Epoch [855/3000], Standardized Loss: 0.0137521, Inverse Loss: 0.0028706
Valid-->> Lowest loss found at epoch 855, loss: 0.0028706
Epoch 855, Masked params (inverse standardized): tensor([3.229343e+01, 4.430070e+01, 7.165718e-02, 2.635163e+01, 2.768800e+01,
        1.781363e+01, 2.143339e+01, 1.642277e+00, 1.066122e+02, 2.755136e+01,
        2.615320e+01, 4.461834e+01, 2.443446e+01, 1.504060e+01, 8.461570e+01,
        2.835428e+01, 5.670290e+00, 2.982108e+01, 1.197215e+01, 3.204090e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 856,(no reg loss)standard loss: 0.0043815, inverse loss: 0.0009146
Valid-->> Epoch [856/3000], Standardized Loss: 0.0140964, Inverse Loss: 0.0029425
Training -->> Epoch: 857,(no reg loss)standard loss: 0.0048442, inverse loss: 0.0010112
Valid-->> Epoch [857/3000], Standardized Loss: 0.0136994, Inverse Loss: 0.0028596
Valid-->> Lowest loss found at epoch 857, loss: 0.0028596
Epoch 857, Masked params (inverse standardized): tensor([3.229807e+01, 4.430064e+01, 7.511139e-02, 2.635267e+01, 2.768910e+01,
        1.781997e+01, 2.143806e+01, 1.646584e+00, 1.066125e+02, 2.755235e+01,
        2.615429e+01, 4.462457e+01, 2.443706e+01, 1.504721e+01, 8.461534e+01,
        2.835592e+01, 5.673618e+00, 2.982425e+01, 1.197883e+01, 3.204543e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 858,(no reg loss)standard loss: 0.0044097, inverse loss: 0.0009205
Valid-->> Epoch [858/3000], Standardized Loss: 0.0139854, Inverse Loss: 0.0029193
Training -->> Epoch: 859,(no reg loss)standard loss: 0.0045732, inverse loss: 0.0009546
Valid-->> Epoch [859/3000], Standardized Loss: 0.0138277, Inverse Loss: 0.0028864
Training -->> Epoch: 860,(no reg loss)standard loss: 0.0043399, inverse loss: 0.0009059
Valid-->> Epoch [860/3000], Standardized Loss: 0.0138575, Inverse Loss: 0.0028926
Training -->> Epoch: 861,(no reg loss)standard loss: 0.0045198, inverse loss: 0.0009434
Valid-->> Epoch [861/3000], Standardized Loss: 0.0139264, Inverse Loss: 0.0029070
Training -->> Epoch: 862,(no reg loss)standard loss: 0.0045215, inverse loss: 0.0009438
Valid-->> Epoch [862/3000], Standardized Loss: 0.0138344, Inverse Loss: 0.0028878
Training -->> Epoch: 863,(no reg loss)standard loss: 0.0044483, inverse loss: 0.0009285
Valid-->> Epoch [863/3000], Standardized Loss: 0.0138666, Inverse Loss: 0.0028945
Training -->> Epoch: 864,(no reg loss)standard loss: 0.0045049, inverse loss: 0.0009403
Valid-->> Epoch [864/3000], Standardized Loss: 0.0138222, Inverse Loss: 0.0028852
Training -->> Epoch: 865,(no reg loss)standard loss: 0.0044727, inverse loss: 0.0009336
Valid-->> Epoch [865/3000], Standardized Loss: 0.0138698, Inverse Loss: 0.0028952
Training -->> Epoch: 866,(no reg loss)standard loss: 0.0044251, inverse loss: 0.0009237
Valid-->> Epoch [866/3000], Standardized Loss: 0.0138177, Inverse Loss: 0.0028843
Training -->> Epoch: 867,(no reg loss)standard loss: 0.0044732, inverse loss: 0.0009337
Valid-->> Epoch [867/3000], Standardized Loss: 0.0138713, Inverse Loss: 0.0028955
Training -->> Epoch: 868,(no reg loss)standard loss: 0.0044970, inverse loss: 0.0009387
Valid-->> Epoch [868/3000], Standardized Loss: 0.0139474, Inverse Loss: 0.0029114
Training -->> Epoch: 869,(no reg loss)standard loss: 0.0045207, inverse loss: 0.0009436
Valid-->> Epoch [869/3000], Standardized Loss: 0.0138871, Inverse Loss: 0.0028988
Training -->> Epoch: 870,(no reg loss)standard loss: 0.0045554, inverse loss: 0.0009509
Valid-->> Epoch [870/3000], Standardized Loss: 0.0138096, Inverse Loss: 0.0028826
Training -->> Epoch: 871,(no reg loss)standard loss: 0.0044331, inverse loss: 0.0009254
Valid-->> Epoch [871/3000], Standardized Loss: 0.0138452, Inverse Loss: 0.0028900
Training -->> Epoch: 872,(no reg loss)standard loss: 0.0045927, inverse loss: 0.0009587
Valid-->> Epoch [872/3000], Standardized Loss: 0.0138267, Inverse Loss: 0.0028862
Training -->> Epoch: 873,(no reg loss)standard loss: 0.0044688, inverse loss: 0.0009328
Valid-->> Epoch [873/3000], Standardized Loss: 0.0138813, Inverse Loss: 0.0028976
Training -->> Epoch: 874,(no reg loss)standard loss: 0.0044703, inverse loss: 0.0009331
Valid-->> Epoch [874/3000], Standardized Loss: 0.0138896, Inverse Loss: 0.0028993
Training -->> Epoch: 875,(no reg loss)standard loss: 0.0045937, inverse loss: 0.0009589
Valid-->> Epoch [875/3000], Standardized Loss: 0.0138039, Inverse Loss: 0.0028814
Training -->> Epoch: 876,(no reg loss)standard loss: 0.0044011, inverse loss: 0.0009187
Valid-->> Epoch [876/3000], Standardized Loss: 0.0138866, Inverse Loss: 0.0028987
Training -->> Epoch: 877,(no reg loss)standard loss: 0.0044602, inverse loss: 0.0009310
Valid-->> Epoch [877/3000], Standardized Loss: 0.0137985, Inverse Loss: 0.0028803
Training -->> Epoch: 878,(no reg loss)standard loss: 0.0044864, inverse loss: 0.0009365
Valid-->> Epoch [878/3000], Standardized Loss: 0.0139765, Inverse Loss: 0.0029174
Training -->> Epoch: 879,(no reg loss)standard loss: 0.0047189, inverse loss: 0.0009850
Valid-->> Epoch [879/3000], Standardized Loss: 0.0137504, Inverse Loss: 0.0028702
Training -->> Epoch: 880,(no reg loss)standard loss: 0.0042925, inverse loss: 0.0008960
Valid-->> Epoch [880/3000], Standardized Loss: 0.0139270, Inverse Loss: 0.0029071
Training -->> Epoch: 881,(no reg loss)standard loss: 0.0047367, inverse loss: 0.0009887
Valid-->> Epoch [881/3000], Standardized Loss: 0.0137549, Inverse Loss: 0.0028712
Training -->> Epoch: 882,(no reg loss)standard loss: 0.0043978, inverse loss: 0.0009180
Valid-->> Epoch [882/3000], Standardized Loss: 0.0138625, Inverse Loss: 0.0028936
Training -->> Epoch: 883,(no reg loss)standard loss: 0.0045262, inverse loss: 0.0009448
Valid-->> Epoch [883/3000], Standardized Loss: 0.0137852, Inverse Loss: 0.0028775
Training -->> Epoch: 884,(no reg loss)standard loss: 0.0044021, inverse loss: 0.0009189
Valid-->> Epoch [884/3000], Standardized Loss: 0.0138010, Inverse Loss: 0.0028808
Training -->> Epoch: 885,(no reg loss)standard loss: 0.0046154, inverse loss: 0.0009634
Valid-->> Epoch [885/3000], Standardized Loss: 0.0138106, Inverse Loss: 0.0028828
Training -->> Epoch: 886,(no reg loss)standard loss: 0.0044645, inverse loss: 0.0009319
Valid-->> Epoch [886/3000], Standardized Loss: 0.0139008, Inverse Loss: 0.0029016
Training -->> Epoch: 887,(no reg loss)standard loss: 0.0045027, inverse loss: 0.0009399
Valid-->> Epoch [887/3000], Standardized Loss: 0.0137514, Inverse Loss: 0.0028704
Training -->> Epoch: 888,(no reg loss)standard loss: 0.0044257, inverse loss: 0.0009238
Valid-->> Epoch [888/3000], Standardized Loss: 0.0138386, Inverse Loss: 0.0028887
Training -->> Epoch: 889,(no reg loss)standard loss: 0.0046226, inverse loss: 0.0009649
Valid-->> Epoch [889/3000], Standardized Loss: 0.0137769, Inverse Loss: 0.0028758
Training -->> Epoch: 890,(no reg loss)standard loss: 0.0045503, inverse loss: 0.0009498
Valid-->> Epoch [890/3000], Standardized Loss: 0.0137195, Inverse Loss: 0.0028638
Training -->> Epoch: 891,(no reg loss)standard loss: 0.0044139, inverse loss: 0.0009213
Valid-->> Epoch [891/3000], Standardized Loss: 0.0138595, Inverse Loss: 0.0028930
Training -->> Epoch: 892,(no reg loss)standard loss: 0.0045393, inverse loss: 0.0009475
Valid-->> Epoch [892/3000], Standardized Loss: 0.0138898, Inverse Loss: 0.0028993
Training -->> Epoch: 893,(no reg loss)standard loss: 0.0045850, inverse loss: 0.0009571
Valid-->> Epoch [893/3000], Standardized Loss: 0.0138071, Inverse Loss: 0.0028821
Training -->> Epoch: 894,(no reg loss)standard loss: 0.0045004, inverse loss: 0.0009394
Valid-->> Epoch [894/3000], Standardized Loss: 0.0137700, Inverse Loss: 0.0028743
Training -->> Epoch: 895,(no reg loss)standard loss: 0.0044721, inverse loss: 0.0009335
Valid-->> Epoch [895/3000], Standardized Loss: 0.0138450, Inverse Loss: 0.0028900
Training -->> Epoch: 896,(no reg loss)standard loss: 0.0045222, inverse loss: 0.0009440
Valid-->> Epoch [896/3000], Standardized Loss: 0.0137771, Inverse Loss: 0.0028758
Training -->> Epoch: 897,(no reg loss)standard loss: 0.0045830, inverse loss: 0.0009566
Valid-->> Epoch [897/3000], Standardized Loss: 0.0136701, Inverse Loss: 0.0028535
Valid-->> Lowest loss found at epoch 897, loss: 0.0028535
Epoch 897, Masked params (inverse standardized): tensor([3.229460e+01, 4.430090e+01, 7.225418e-02, 2.635326e+01, 2.768945e+01,
        1.781444e+01, 2.143459e+01, 1.642685e+00, 1.066119e+02, 2.755303e+01,
        2.615460e+01, 4.461876e+01, 2.443575e+01, 1.504126e+01, 8.461566e+01,
        2.835551e+01, 5.670748e+00, 2.982241e+01, 1.197264e+01, 3.204213e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 898,(no reg loss)standard loss: 0.0044140, inverse loss: 0.0009214
Valid-->> Epoch [898/3000], Standardized Loss: 0.0138362, Inverse Loss: 0.0028881
Training -->> Epoch: 899,(no reg loss)standard loss: 0.0047005, inverse loss: 0.0009812
Valid-->> Epoch [899/3000], Standardized Loss: 0.0137331, Inverse Loss: 0.0028666
Training -->> Epoch: 900,(no reg loss)standard loss: 0.0045061, inverse loss: 0.0009406
Valid-->> Epoch [900/3000], Standardized Loss: 0.0137749, Inverse Loss: 0.0028754
Training -->> Epoch: 901,(no reg loss)standard loss: 0.0044545, inverse loss: 0.0009298
Valid-->> Epoch [901/3000], Standardized Loss: 0.0138388, Inverse Loss: 0.0028887
Training -->> Epoch: 902,(no reg loss)standard loss: 0.0045270, inverse loss: 0.0009450
Valid-->> Epoch [902/3000], Standardized Loss: 0.0138017, Inverse Loss: 0.0028810
Training -->> Epoch: 903,(no reg loss)standard loss: 0.0044586, inverse loss: 0.0009307
Valid-->> Epoch [903/3000], Standardized Loss: 0.0138014, Inverse Loss: 0.0028809
Training -->> Epoch: 904,(no reg loss)standard loss: 0.0043839, inverse loss: 0.0009151
Valid-->> Epoch [904/3000], Standardized Loss: 0.0137709, Inverse Loss: 0.0028745
Training -->> Epoch: 905,(no reg loss)standard loss: 0.0045258, inverse loss: 0.0009447
Valid-->> Epoch [905/3000], Standardized Loss: 0.0137798, Inverse Loss: 0.0028764
Training -->> Epoch: 906,(no reg loss)standard loss: 0.0045342, inverse loss: 0.0009465
Valid-->> Epoch [906/3000], Standardized Loss: 0.0137483, Inverse Loss: 0.0028698
Training -->> Epoch: 907,(no reg loss)standard loss: 0.0045649, inverse loss: 0.0009529
Valid-->> Epoch [907/3000], Standardized Loss: 0.0137327, Inverse Loss: 0.0028665
Training -->> Epoch: 908,(no reg loss)standard loss: 0.0045150, inverse loss: 0.0009425
Valid-->> Epoch [908/3000], Standardized Loss: 0.0138250, Inverse Loss: 0.0028858
Training -->> Epoch: 909,(no reg loss)standard loss: 0.0045120, inverse loss: 0.0009418
Valid-->> Epoch [909/3000], Standardized Loss: 0.0137792, Inverse Loss: 0.0028763
Training -->> Epoch: 910,(no reg loss)standard loss: 0.0045007, inverse loss: 0.0009395
Valid-->> Epoch [910/3000], Standardized Loss: 0.0138273, Inverse Loss: 0.0028863
Training -->> Epoch: 911,(no reg loss)standard loss: 0.0045829, inverse loss: 0.0009566
Valid-->> Epoch [911/3000], Standardized Loss: 0.0137531, Inverse Loss: 0.0028708
Training -->> Epoch: 912,(no reg loss)standard loss: 0.0045481, inverse loss: 0.0009494
Valid-->> Epoch [912/3000], Standardized Loss: 0.0137481, Inverse Loss: 0.0028698
Training -->> Epoch: 913,(no reg loss)standard loss: 0.0045584, inverse loss: 0.0009515
Valid-->> Epoch [913/3000], Standardized Loss: 0.0136839, Inverse Loss: 0.0028564
Training -->> Epoch: 914,(no reg loss)standard loss: 0.0044971, inverse loss: 0.0009387
Valid-->> Epoch [914/3000], Standardized Loss: 0.0138289, Inverse Loss: 0.0028866
Training -->> Epoch: 915,(no reg loss)standard loss: 0.0044594, inverse loss: 0.0009308
Valid-->> Epoch [915/3000], Standardized Loss: 0.0137279, Inverse Loss: 0.0028655
Training -->> Epoch: 916,(no reg loss)standard loss: 0.0044993, inverse loss: 0.0009392
Valid-->> Epoch [916/3000], Standardized Loss: 0.0137614, Inverse Loss: 0.0028725
Training -->> Epoch: 917,(no reg loss)standard loss: 0.0045320, inverse loss: 0.0009460
Valid-->> Epoch [917/3000], Standardized Loss: 0.0137386, Inverse Loss: 0.0028678
Training -->> Epoch: 918,(no reg loss)standard loss: 0.0044424, inverse loss: 0.0009273
Valid-->> Epoch [918/3000], Standardized Loss: 0.0138375, Inverse Loss: 0.0028884
Training -->> Epoch: 919,(no reg loss)standard loss: 0.0045120, inverse loss: 0.0009418
Valid-->> Epoch [919/3000], Standardized Loss: 0.0137816, Inverse Loss: 0.0028768
Training -->> Epoch: 920,(no reg loss)standard loss: 0.0045462, inverse loss: 0.0009490
Valid-->> Epoch [920/3000], Standardized Loss: 0.0137531, Inverse Loss: 0.0028708
Training -->> Epoch: 921,(no reg loss)standard loss: 0.0045787, inverse loss: 0.0009557
Valid-->> Epoch [921/3000], Standardized Loss: 0.0137669, Inverse Loss: 0.0028737
Training -->> Epoch: 922,(no reg loss)standard loss: 0.0043766, inverse loss: 0.0009136
Valid-->> Epoch [922/3000], Standardized Loss: 0.0137935, Inverse Loss: 0.0028792
Training -->> Epoch: 923,(no reg loss)standard loss: 0.0046031, inverse loss: 0.0009609
Valid-->> Epoch [923/3000], Standardized Loss: 0.0137343, Inverse Loss: 0.0028669
Training -->> Epoch: 924,(no reg loss)standard loss: 0.0043861, inverse loss: 0.0009155
Valid-->> Epoch [924/3000], Standardized Loss: 0.0138177, Inverse Loss: 0.0028843
Training -->> Epoch: 925,(no reg loss)standard loss: 0.0046576, inverse loss: 0.0009722
Valid-->> Epoch [925/3000], Standardized Loss: 0.0136733, Inverse Loss: 0.0028541
Training -->> Epoch: 926,(no reg loss)standard loss: 0.0044516, inverse loss: 0.0009292
Valid-->> Epoch [926/3000], Standardized Loss: 0.0138169, Inverse Loss: 0.0028841
Training -->> Epoch: 927,(no reg loss)standard loss: 0.0045764, inverse loss: 0.0009553
Valid-->> Epoch [927/3000], Standardized Loss: 0.0137418, Inverse Loss: 0.0028684
Training -->> Epoch: 928,(no reg loss)standard loss: 0.0044886, inverse loss: 0.0009370
Valid-->> Epoch [928/3000], Standardized Loss: 0.0138776, Inverse Loss: 0.0028968
Training -->> Epoch: 929,(no reg loss)standard loss: 0.0046220, inverse loss: 0.0009648
Valid-->> Epoch [929/3000], Standardized Loss: 0.0137652, Inverse Loss: 0.0028733
Training -->> Epoch: 930,(no reg loss)standard loss: 0.0045788, inverse loss: 0.0009558
Valid-->> Epoch [930/3000], Standardized Loss: 0.0138177, Inverse Loss: 0.0028843
Training -->> Epoch: 931,(no reg loss)standard loss: 0.0044520, inverse loss: 0.0009293
Valid-->> Epoch [931/3000], Standardized Loss: 0.0138198, Inverse Loss: 0.0028847
Training -->> Epoch: 932,(no reg loss)standard loss: 0.0046439, inverse loss: 0.0009694
Valid-->> Epoch [932/3000], Standardized Loss: 0.0137412, Inverse Loss: 0.0028683
Training -->> Epoch: 933,(no reg loss)standard loss: 0.0044627, inverse loss: 0.0009315
Valid-->> Epoch [933/3000], Standardized Loss: 0.0137956, Inverse Loss: 0.0028797
Training -->> Epoch: 934,(no reg loss)standard loss: 0.0045356, inverse loss: 0.0009468
Valid-->> Epoch [934/3000], Standardized Loss: 0.0136435, Inverse Loss: 0.0028479
Valid-->> Lowest loss found at epoch 934, loss: 0.0028479
Epoch 934, Masked params (inverse standardized): tensor([3.229322e+01, 4.430211e+01, 6.990433e-02, 2.635384e+01, 2.768974e+01,
        1.781334e+01, 2.143320e+01, 1.641254e+00, 1.066119e+02, 2.755354e+01,
        2.615493e+01, 4.461853e+01, 2.443513e+01, 1.504058e+01, 8.461639e+01,
        2.835516e+01, 5.668747e+00, 2.982158e+01, 1.197220e+01, 3.204097e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 935,(no reg loss)standard loss: 0.0043730, inverse loss: 0.0009128
Valid-->> Epoch [935/3000], Standardized Loss: 0.0137282, Inverse Loss: 0.0028656
Training -->> Epoch: 936,(no reg loss)standard loss: 0.0045133, inverse loss: 0.0009421
Valid-->> Epoch [936/3000], Standardized Loss: 0.0137672, Inverse Loss: 0.0028737
Training -->> Epoch: 937,(no reg loss)standard loss: 0.0045893, inverse loss: 0.0009580
Valid-->> Epoch [937/3000], Standardized Loss: 0.0137709, Inverse Loss: 0.0028745
Training -->> Epoch: 938,(no reg loss)standard loss: 0.0046408, inverse loss: 0.0009687
Valid-->> Epoch [938/3000], Standardized Loss: 0.0137308, Inverse Loss: 0.0028662
Training -->> Epoch: 939,(no reg loss)standard loss: 0.0044430, inverse loss: 0.0009274
Valid-->> Epoch [939/3000], Standardized Loss: 0.0137120, Inverse Loss: 0.0028622
Training -->> Epoch: 940,(no reg loss)standard loss: 0.0044969, inverse loss: 0.0009387
Valid-->> Epoch [940/3000], Standardized Loss: 0.0136817, Inverse Loss: 0.0028559
Training -->> Epoch: 941,(no reg loss)standard loss: 0.0046375, inverse loss: 0.0009680
Valid-->> Epoch [941/3000], Standardized Loss: 0.0137272, Inverse Loss: 0.0028654
Training -->> Epoch: 942,(no reg loss)standard loss: 0.0045154, inverse loss: 0.0009425
Valid-->> Epoch [942/3000], Standardized Loss: 0.0136749, Inverse Loss: 0.0028545
Training -->> Epoch: 943,(no reg loss)standard loss: 0.0044553, inverse loss: 0.0009300
Valid-->> Epoch [943/3000], Standardized Loss: 0.0137903, Inverse Loss: 0.0028786
Training -->> Epoch: 944,(no reg loss)standard loss: 0.0046183, inverse loss: 0.0009640
Valid-->> Epoch [944/3000], Standardized Loss: 0.0136807, Inverse Loss: 0.0028557
Training -->> Epoch: 945,(no reg loss)standard loss: 0.0043266, inverse loss: 0.0009031
Valid-->> Epoch [945/3000], Standardized Loss: 0.0137060, Inverse Loss: 0.0028610
Training -->> Epoch: 946,(no reg loss)standard loss: 0.0045887, inverse loss: 0.0009578
Valid-->> Epoch [946/3000], Standardized Loss: 0.0137479, Inverse Loss: 0.0028697
Training -->> Epoch: 947,(no reg loss)standard loss: 0.0045478, inverse loss: 0.0009493
Valid-->> Epoch [947/3000], Standardized Loss: 0.0136818, Inverse Loss: 0.0028559
Training -->> Epoch: 948,(no reg loss)standard loss: 0.0043877, inverse loss: 0.0009159
Valid-->> Epoch [948/3000], Standardized Loss: 0.0136856, Inverse Loss: 0.0028567
Training -->> Epoch: 949,(no reg loss)standard loss: 0.0044311, inverse loss: 0.0009249
Valid-->> Epoch [949/3000], Standardized Loss: 0.0136915, Inverse Loss: 0.0028580
Training -->> Epoch: 950,(no reg loss)standard loss: 0.0046219, inverse loss: 0.0009648
Valid-->> Epoch [950/3000], Standardized Loss: 0.0138901, Inverse Loss: 0.0028994
Training -->> Epoch: 951,(no reg loss)standard loss: 0.0043903, inverse loss: 0.0009164
Valid-->> Epoch [951/3000], Standardized Loss: 0.0136608, Inverse Loss: 0.0028515
Training -->> Epoch: 952,(no reg loss)standard loss: 0.0046461, inverse loss: 0.0009698
Valid-->> Epoch [952/3000], Standardized Loss: 0.0137511, Inverse Loss: 0.0028704
Training -->> Epoch: 953,(no reg loss)standard loss: 0.0043934, inverse loss: 0.0009171
Valid-->> Epoch [953/3000], Standardized Loss: 0.0136975, Inverse Loss: 0.0028592
Training -->> Epoch: 954,(no reg loss)standard loss: 0.0046678, inverse loss: 0.0009744
Valid-->> Epoch [954/3000], Standardized Loss: 0.0136224, Inverse Loss: 0.0028435
Valid-->> Lowest loss found at epoch 954, loss: 0.0028435
Epoch 954, Masked params (inverse standardized): tensor([3.229344e+01, 4.430126e+01, 7.293510e-02, 2.635419e+01, 2.768985e+01,
        1.781414e+01, 2.143345e+01, 1.644218e+00, 1.066133e+02, 2.755401e+01,
        2.615499e+01, 4.462067e+01, 2.443487e+01, 1.504159e+01, 8.461640e+01,
        2.835501e+01, 5.671654e+00, 2.982137e+01, 1.197415e+01, 3.204093e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 955,(no reg loss)standard loss: 0.0043527, inverse loss: 0.0009086
Valid-->> Epoch [955/3000], Standardized Loss: 0.0136865, Inverse Loss: 0.0028569
Training -->> Epoch: 956,(no reg loss)standard loss: 0.0046185, inverse loss: 0.0009640
Valid-->> Epoch [956/3000], Standardized Loss: 0.0137210, Inverse Loss: 0.0028641
Training -->> Epoch: 957,(no reg loss)standard loss: 0.0045128, inverse loss: 0.0009420
Valid-->> Epoch [957/3000], Standardized Loss: 0.0137303, Inverse Loss: 0.0028660
Training -->> Epoch: 958,(no reg loss)standard loss: 0.0045198, inverse loss: 0.0009435
Valid-->> Epoch [958/3000], Standardized Loss: 0.0136543, Inverse Loss: 0.0028502
Training -->> Epoch: 959,(no reg loss)standard loss: 0.0045217, inverse loss: 0.0009439
Valid-->> Epoch [959/3000], Standardized Loss: 0.0137515, Inverse Loss: 0.0028705
Training -->> Epoch: 960,(no reg loss)standard loss: 0.0046284, inverse loss: 0.0009661
Valid-->> Epoch [960/3000], Standardized Loss: 0.0137100, Inverse Loss: 0.0028618
Training -->> Epoch: 961,(no reg loss)standard loss: 0.0045015, inverse loss: 0.0009396
Valid-->> Epoch [961/3000], Standardized Loss: 0.0137405, Inverse Loss: 0.0028682
Training -->> Epoch: 962,(no reg loss)standard loss: 0.0047100, inverse loss: 0.0009832
Valid-->> Epoch [962/3000], Standardized Loss: 0.0136397, Inverse Loss: 0.0028471
Training -->> Epoch: 963,(no reg loss)standard loss: 0.0044473, inverse loss: 0.0009283
Valid-->> Epoch [963/3000], Standardized Loss: 0.0137533, Inverse Loss: 0.0028708
Training -->> Epoch: 964,(no reg loss)standard loss: 0.0046949, inverse loss: 0.0009800
Valid-->> Epoch [964/3000], Standardized Loss: 0.0135937, Inverse Loss: 0.0028375
Valid-->> Lowest loss found at epoch 964, loss: 0.0028375
Epoch 964, Masked params (inverse standardized): tensor([3.229435e+01, 4.430175e+01, 7.255936e-02, 2.635454e+01, 2.769040e+01,
        1.781509e+01, 2.143436e+01, 1.643761e+00, 1.066123e+02, 2.755439e+01,
        2.615551e+01, 4.462061e+01, 2.443580e+01, 1.504236e+01, 8.461633e+01,
        2.835589e+01, 5.671177e+00, 2.982226e+01, 1.197440e+01, 3.204183e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 965,(no reg loss)standard loss: 0.0045362, inverse loss: 0.0009469
Valid-->> Epoch [965/3000], Standardized Loss: 0.0136806, Inverse Loss: 0.0028557
Training -->> Epoch: 966,(no reg loss)standard loss: 0.0044115, inverse loss: 0.0009209
Valid-->> Epoch [966/3000], Standardized Loss: 0.0137933, Inverse Loss: 0.0028792
Training -->> Epoch: 967,(no reg loss)standard loss: 0.0046909, inverse loss: 0.0009792
Valid-->> Epoch [967/3000], Standardized Loss: 0.0136898, Inverse Loss: 0.0028576
Training -->> Epoch: 968,(no reg loss)standard loss: 0.0045208, inverse loss: 0.0009437
Valid-->> Epoch [968/3000], Standardized Loss: 0.0138157, Inverse Loss: 0.0028839
Training -->> Epoch: 969,(no reg loss)standard loss: 0.0045562, inverse loss: 0.0009511
Valid-->> Epoch [969/3000], Standardized Loss: 0.0136531, Inverse Loss: 0.0028499
Training -->> Epoch: 970,(no reg loss)standard loss: 0.0044887, inverse loss: 0.0009370
Valid-->> Epoch [970/3000], Standardized Loss: 0.0138333, Inverse Loss: 0.0028875
Training -->> Epoch: 971,(no reg loss)standard loss: 0.0046603, inverse loss: 0.0009728
Valid-->> Epoch [971/3000], Standardized Loss: 0.0137022, Inverse Loss: 0.0028602
Training -->> Epoch: 972,(no reg loss)standard loss: 0.0044818, inverse loss: 0.0009355
Valid-->> Epoch [972/3000], Standardized Loss: 0.0136967, Inverse Loss: 0.0028590
Training -->> Epoch: 973,(no reg loss)standard loss: 0.0045115, inverse loss: 0.0009417
Valid-->> Epoch [973/3000], Standardized Loss: 0.0137122, Inverse Loss: 0.0028623
Training -->> Epoch: 974,(no reg loss)standard loss: 0.0045640, inverse loss: 0.0009527
Valid-->> Epoch [974/3000], Standardized Loss: 0.0137409, Inverse Loss: 0.0028683
Training -->> Epoch: 975,(no reg loss)standard loss: 0.0046305, inverse loss: 0.0009666
Valid-->> Epoch [975/3000], Standardized Loss: 0.0137142, Inverse Loss: 0.0028627
Training -->> Epoch: 976,(no reg loss)standard loss: 0.0044638, inverse loss: 0.0009318
Valid-->> Epoch [976/3000], Standardized Loss: 0.0136639, Inverse Loss: 0.0028522
Training -->> Epoch: 977,(no reg loss)standard loss: 0.0045621, inverse loss: 0.0009523
Valid-->> Epoch [977/3000], Standardized Loss: 0.0137785, Inverse Loss: 0.0028761
Training -->> Epoch: 978,(no reg loss)standard loss: 0.0045406, inverse loss: 0.0009478
Valid-->> Epoch [978/3000], Standardized Loss: 0.0135784, Inverse Loss: 0.0028343
Valid-->> Lowest loss found at epoch 978, loss: 0.0028343
Epoch 978, Masked params (inverse standardized): tensor([3.229395e+01, 4.430195e+01, 6.944275e-02, 2.635522e+01, 2.769090e+01,
        1.781355e+01, 2.143391e+01, 1.640083e+00, 1.066136e+02, 2.755504e+01,
        2.615602e+01, 4.461679e+01, 2.443559e+01, 1.504016e+01, 8.461673e+01,
        2.835585e+01, 5.668186e+00, 2.982203e+01, 1.197085e+01, 3.204152e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 979,(no reg loss)standard loss: 0.0043634, inverse loss: 0.0009108
Valid-->> Epoch [979/3000], Standardized Loss: 0.0136733, Inverse Loss: 0.0028542
Training -->> Epoch: 980,(no reg loss)standard loss: 0.0047086, inverse loss: 0.0009829
Valid-->> Epoch [980/3000], Standardized Loss: 0.0135720, Inverse Loss: 0.0028330
Valid-->> Lowest loss found at epoch 980, loss: 0.0028330
Epoch 980, Masked params (inverse standardized): tensor([3.229377e+01, 4.430065e+01, 7.157898e-02, 2.635552e+01, 2.769096e+01,
        1.781515e+01, 2.143377e+01, 1.642838e+00, 1.066129e+02, 2.755522e+01,
        2.615611e+01, 4.461996e+01, 2.443500e+01, 1.504228e+01, 8.461562e+01,
        2.835539e+01, 5.670534e+00, 2.982143e+01, 1.197404e+01, 3.204108e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 981,(no reg loss)standard loss: 0.0044935, inverse loss: 0.0009380
Valid-->> Epoch [981/3000], Standardized Loss: 0.0137567, Inverse Loss: 0.0028716
Training -->> Epoch: 982,(no reg loss)standard loss: 0.0045939, inverse loss: 0.0009589
Valid-->> Epoch [982/3000], Standardized Loss: 0.0136033, Inverse Loss: 0.0028395
Training -->> Epoch: 983,(no reg loss)standard loss: 0.0045497, inverse loss: 0.0009497
Valid-->> Epoch [983/3000], Standardized Loss: 0.0137855, Inverse Loss: 0.0028776
Training -->> Epoch: 984,(no reg loss)standard loss: 0.0044867, inverse loss: 0.0009365
Valid-->> Epoch [984/3000], Standardized Loss: 0.0136303, Inverse Loss: 0.0028452
Training -->> Epoch: 985,(no reg loss)standard loss: 0.0045530, inverse loss: 0.0009504
Valid-->> Epoch [985/3000], Standardized Loss: 0.0136532, Inverse Loss: 0.0028499
Training -->> Epoch: 986,(no reg loss)standard loss: 0.0046026, inverse loss: 0.0009607
Valid-->> Epoch [986/3000], Standardized Loss: 0.0136744, Inverse Loss: 0.0028544
Training -->> Epoch: 987,(no reg loss)standard loss: 0.0045154, inverse loss: 0.0009425
Valid-->> Epoch [987/3000], Standardized Loss: 0.0136399, Inverse Loss: 0.0028472
Training -->> Epoch: 988,(no reg loss)standard loss: 0.0045780, inverse loss: 0.0009556
Valid-->> Epoch [988/3000], Standardized Loss: 0.0136619, Inverse Loss: 0.0028518
Training -->> Epoch: 989,(no reg loss)standard loss: 0.0045605, inverse loss: 0.0009519
Valid-->> Epoch [989/3000], Standardized Loss: 0.0136805, Inverse Loss: 0.0028557
Training -->> Epoch: 990,(no reg loss)standard loss: 0.0045161, inverse loss: 0.0009427
Valid-->> Epoch [990/3000], Standardized Loss: 0.0136929, Inverse Loss: 0.0028582
Training -->> Epoch: 991,(no reg loss)standard loss: 0.0046296, inverse loss: 0.0009664
Valid-->> Epoch [991/3000], Standardized Loss: 0.0137257, Inverse Loss: 0.0028651
Training -->> Epoch: 992,(no reg loss)standard loss: 0.0046489, inverse loss: 0.0009704
Valid-->> Epoch [992/3000], Standardized Loss: 0.0136737, Inverse Loss: 0.0028542
Training -->> Epoch: 993,(no reg loss)standard loss: 0.0045310, inverse loss: 0.0009458
Valid-->> Epoch [993/3000], Standardized Loss: 0.0136034, Inverse Loss: 0.0028396
Training -->> Epoch: 994,(no reg loss)standard loss: 0.0044622, inverse loss: 0.0009314
Valid-->> Epoch [994/3000], Standardized Loss: 0.0137504, Inverse Loss: 0.0028702
Training -->> Epoch: 995,(no reg loss)standard loss: 0.0046124, inverse loss: 0.0009628
Valid-->> Epoch [995/3000], Standardized Loss: 0.0137170, Inverse Loss: 0.0028633
Training -->> Epoch: 996,(no reg loss)standard loss: 0.0045599, inverse loss: 0.0009518
Valid-->> Epoch [996/3000], Standardized Loss: 0.0136244, Inverse Loss: 0.0028439
Training -->> Epoch: 997,(no reg loss)standard loss: 0.0044933, inverse loss: 0.0009379
Valid-->> Epoch [997/3000], Standardized Loss: 0.0136046, Inverse Loss: 0.0028398
Training -->> Epoch: 998,(no reg loss)standard loss: 0.0045073, inverse loss: 0.0009409
Valid-->> Epoch [998/3000], Standardized Loss: 0.0136711, Inverse Loss: 0.0028537
Training -->> Epoch: 999,(no reg loss)standard loss: 0.0045872, inverse loss: 0.0009575
Valid-->> Epoch [999/3000], Standardized Loss: 0.0136735, Inverse Loss: 0.0028542
Training -->> Epoch: 1000,(no reg loss)standard loss: 0.0045660, inverse loss: 0.0009531
Valid-->> Epoch [1000/3000], Standardized Loss: 0.0136395, Inverse Loss: 0.0028471
Training -->> Epoch: 1001,(no reg loss)standard loss: 0.0045838, inverse loss: 0.0009568
Valid-->> Epoch [1001/3000], Standardized Loss: 0.0136423, Inverse Loss: 0.0028477
Training -->> Epoch: 1002,(no reg loss)standard loss: 0.0044695, inverse loss: 0.0009329
Valid-->> Epoch [1002/3000], Standardized Loss: 0.0136850, Inverse Loss: 0.0028566
Training -->> Epoch: 1003,(no reg loss)standard loss: 0.0045566, inverse loss: 0.0009511
Valid-->> Epoch [1003/3000], Standardized Loss: 0.0136664, Inverse Loss: 0.0028527
Training -->> Epoch: 1004,(no reg loss)standard loss: 0.0045906, inverse loss: 0.0009582
Valid-->> Epoch [1004/3000], Standardized Loss: 0.0136686, Inverse Loss: 0.0028532
Training -->> Epoch: 1005,(no reg loss)standard loss: 0.0045046, inverse loss: 0.0009403
Valid-->> Epoch [1005/3000], Standardized Loss: 0.0136938, Inverse Loss: 0.0028584
Training -->> Epoch: 1006,(no reg loss)standard loss: 0.0046196, inverse loss: 0.0009643
Valid-->> Epoch [1006/3000], Standardized Loss: 0.0136880, Inverse Loss: 0.0028572
Training -->> Epoch: 1007,(no reg loss)standard loss: 0.0045828, inverse loss: 0.0009566
Valid-->> Epoch [1007/3000], Standardized Loss: 0.0136085, Inverse Loss: 0.0028406
Training -->> Epoch: 1008,(no reg loss)standard loss: 0.0045510, inverse loss: 0.0009500
Valid-->> Epoch [1008/3000], Standardized Loss: 0.0136412, Inverse Loss: 0.0028474
Training -->> Epoch: 1009,(no reg loss)standard loss: 0.0046529, inverse loss: 0.0009712
Valid-->> Epoch [1009/3000], Standardized Loss: 0.0136312, Inverse Loss: 0.0028454
Training -->> Epoch: 1010,(no reg loss)standard loss: 0.0045910, inverse loss: 0.0009583
Valid-->> Epoch [1010/3000], Standardized Loss: 0.0136875, Inverse Loss: 0.0028571
Training -->> Epoch: 1011,(no reg loss)standard loss: 0.0045027, inverse loss: 0.0009399
Valid-->> Epoch [1011/3000], Standardized Loss: 0.0136509, Inverse Loss: 0.0028495
Training -->> Epoch: 1012,(no reg loss)standard loss: 0.0046411, inverse loss: 0.0009688
Valid-->> Epoch [1012/3000], Standardized Loss: 0.0135672, Inverse Loss: 0.0028320
Valid-->> Lowest loss found at epoch 1012, loss: 0.0028320
Epoch 1012, Masked params (inverse standardized): tensor([3.229316e+01, 4.430139e+01, 7.176399e-02, 2.635536e+01, 2.769090e+01,
        1.781351e+01, 2.143312e+01, 1.642410e+00, 1.066119e+02, 2.755523e+01,
        2.615602e+01, 4.461849e+01, 2.443504e+01, 1.504051e+01, 8.461575e+01,
        2.835554e+01, 5.670044e+00, 2.982137e+01, 1.197222e+01, 3.204066e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1013,(no reg loss)standard loss: 0.0045531, inverse loss: 0.0009504
Valid-->> Epoch [1013/3000], Standardized Loss: 0.0135844, Inverse Loss: 0.0028356
Training -->> Epoch: 1014,(no reg loss)standard loss: 0.0044947, inverse loss: 0.0009382
Valid-->> Epoch [1014/3000], Standardized Loss: 0.0137290, Inverse Loss: 0.0028658
Training -->> Epoch: 1015,(no reg loss)standard loss: 0.0046578, inverse loss: 0.0009723
Valid-->> Epoch [1015/3000], Standardized Loss: 0.0136508, Inverse Loss: 0.0028494
Training -->> Epoch: 1016,(no reg loss)standard loss: 0.0044599, inverse loss: 0.0009310
Valid-->> Epoch [1016/3000], Standardized Loss: 0.0136242, Inverse Loss: 0.0028439
Training -->> Epoch: 1017,(no reg loss)standard loss: 0.0045993, inverse loss: 0.0009601
Valid-->> Epoch [1017/3000], Standardized Loss: 0.0136333, Inverse Loss: 0.0028458
Training -->> Epoch: 1018,(no reg loss)standard loss: 0.0045574, inverse loss: 0.0009513
Valid-->> Epoch [1018/3000], Standardized Loss: 0.0135800, Inverse Loss: 0.0028347
Training -->> Epoch: 1019,(no reg loss)standard loss: 0.0045444, inverse loss: 0.0009486
Valid-->> Epoch [1019/3000], Standardized Loss: 0.0137825, Inverse Loss: 0.0028769
Training -->> Epoch: 1020,(no reg loss)standard loss: 0.0046148, inverse loss: 0.0009633
Valid-->> Epoch [1020/3000], Standardized Loss: 0.0135930, Inverse Loss: 0.0028374
Training -->> Epoch: 1021,(no reg loss)standard loss: 0.0044764, inverse loss: 0.0009344
Valid-->> Epoch [1021/3000], Standardized Loss: 0.0135986, Inverse Loss: 0.0028385
Training -->> Epoch: 1022,(no reg loss)standard loss: 0.0045926, inverse loss: 0.0009586
Valid-->> Epoch [1022/3000], Standardized Loss: 0.0136138, Inverse Loss: 0.0028417
Training -->> Epoch: 1023,(no reg loss)standard loss: 0.0045607, inverse loss: 0.0009520
Valid-->> Epoch [1023/3000], Standardized Loss: 0.0136245, Inverse Loss: 0.0028440
Training -->> Epoch: 1024,(no reg loss)standard loss: 0.0044942, inverse loss: 0.0009381
Valid-->> Epoch [1024/3000], Standardized Loss: 0.0137035, Inverse Loss: 0.0028605
Training -->> Epoch: 1025,(no reg loss)standard loss: 0.0046519, inverse loss: 0.0009710
Valid-->> Epoch [1025/3000], Standardized Loss: 0.0136551, Inverse Loss: 0.0028503
Training -->> Epoch: 1026,(no reg loss)standard loss: 0.0047114, inverse loss: 0.0009834
Valid-->> Epoch [1026/3000], Standardized Loss: 0.0136453, Inverse Loss: 0.0028483
Training -->> Epoch: 1027,(no reg loss)standard loss: 0.0044128, inverse loss: 0.0009211
Valid-->> Epoch [1027/3000], Standardized Loss: 0.0136152, Inverse Loss: 0.0028420
Training -->> Epoch: 1028,(no reg loss)standard loss: 0.0046934, inverse loss: 0.0009797
Valid-->> Epoch [1028/3000], Standardized Loss: 0.0135593, Inverse Loss: 0.0028304
Valid-->> Lowest loss found at epoch 1028, loss: 0.0028304
Epoch 1028, Masked params (inverse standardized): tensor([3.229275e+01, 4.430192e+01, 7.296371e-02, 2.635561e+01, 2.769097e+01,
        1.781422e+01, 2.143273e+01, 1.644129e+00, 1.066115e+02, 2.755548e+01,
        2.615609e+01, 4.462059e+01, 2.443458e+01, 1.504186e+01, 8.461649e+01,
        2.835525e+01, 5.671404e+00, 2.982084e+01, 1.197418e+01, 3.204027e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1029,(no reg loss)standard loss: 0.0044996, inverse loss: 0.0009392
Valid-->> Epoch [1029/3000], Standardized Loss: 0.0134808, Inverse Loss: 0.0028140
Valid-->> Lowest loss found at epoch 1029, loss: 0.0028140
Epoch 1029, Masked params (inverse standardized): tensor([3.229432e+01, 4.430125e+01, 6.962395e-02, 2.635765e+01, 2.769301e+01,
        1.781277e+01, 2.143426e+01, 1.639647e+00, 1.066112e+02, 2.755755e+01,
        2.615815e+01, 4.461517e+01, 2.443659e+01, 1.503885e+01, 8.461581e+01,
        2.835728e+01, 5.668518e+00, 2.982288e+01, 1.196915e+01, 3.204191e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1030,(no reg loss)standard loss: 0.0043601, inverse loss: 0.0009101
Valid-->> Epoch [1030/3000], Standardized Loss: 0.0136910, Inverse Loss: 0.0028578
Training -->> Epoch: 1031,(no reg loss)standard loss: 0.0045840, inverse loss: 0.0009568
Valid-->> Epoch [1031/3000], Standardized Loss: 0.0136819, Inverse Loss: 0.0028559
Training -->> Epoch: 1032,(no reg loss)standard loss: 0.0046623, inverse loss: 0.0009732
Valid-->> Epoch [1032/3000], Standardized Loss: 0.0136698, Inverse Loss: 0.0028534
Training -->> Epoch: 1033,(no reg loss)standard loss: 0.0046525, inverse loss: 0.0009712
Valid-->> Epoch [1033/3000], Standardized Loss: 0.0135129, Inverse Loss: 0.0028207
Training -->> Epoch: 1034,(no reg loss)standard loss: 0.0045607, inverse loss: 0.0009520
Valid-->> Epoch [1034/3000], Standardized Loss: 0.0135346, Inverse Loss: 0.0028252
Training -->> Epoch: 1035,(no reg loss)standard loss: 0.0044733, inverse loss: 0.0009338
Valid-->> Epoch [1035/3000], Standardized Loss: 0.0135918, Inverse Loss: 0.0028371
Training -->> Epoch: 1036,(no reg loss)standard loss: 0.0047063, inverse loss: 0.0009824
Valid-->> Epoch [1036/3000], Standardized Loss: 0.0135698, Inverse Loss: 0.0028325
Training -->> Epoch: 1037,(no reg loss)standard loss: 0.0044235, inverse loss: 0.0009234
Valid-->> Epoch [1037/3000], Standardized Loss: 0.0136019, Inverse Loss: 0.0028392
Training -->> Epoch: 1038,(no reg loss)standard loss: 0.0046169, inverse loss: 0.0009637
Valid-->> Epoch [1038/3000], Standardized Loss: 0.0136218, Inverse Loss: 0.0028434
Training -->> Epoch: 1039,(no reg loss)standard loss: 0.0044898, inverse loss: 0.0009372
Valid-->> Epoch [1039/3000], Standardized Loss: 0.0135987, Inverse Loss: 0.0028386
Training -->> Epoch: 1040,(no reg loss)standard loss: 0.0047526, inverse loss: 0.0009920
Valid-->> Epoch [1040/3000], Standardized Loss: 0.0135737, Inverse Loss: 0.0028334
Training -->> Epoch: 1041,(no reg loss)standard loss: 0.0045236, inverse loss: 0.0009442
Valid-->> Epoch [1041/3000], Standardized Loss: 0.0136387, Inverse Loss: 0.0028469
Training -->> Epoch: 1042,(no reg loss)standard loss: 0.0045533, inverse loss: 0.0009504
Valid-->> Epoch [1042/3000], Standardized Loss: 0.0135604, Inverse Loss: 0.0028306
Training -->> Epoch: 1043,(no reg loss)standard loss: 0.0045842, inverse loss: 0.0009569
Valid-->> Epoch [1043/3000], Standardized Loss: 0.0136189, Inverse Loss: 0.0028428
Training -->> Epoch: 1044,(no reg loss)standard loss: 0.0046338, inverse loss: 0.0009673
Valid-->> Epoch [1044/3000], Standardized Loss: 0.0135619, Inverse Loss: 0.0028309
Training -->> Epoch: 1045,(no reg loss)standard loss: 0.0045184, inverse loss: 0.0009432
Valid-->> Epoch [1045/3000], Standardized Loss: 0.0137006, Inverse Loss: 0.0028598
Training -->> Epoch: 1046,(no reg loss)standard loss: 0.0046130, inverse loss: 0.0009629
Valid-->> Epoch [1046/3000], Standardized Loss: 0.0135971, Inverse Loss: 0.0028382
Training -->> Epoch: 1047,(no reg loss)standard loss: 0.0045903, inverse loss: 0.0009582
Valid-->> Epoch [1047/3000], Standardized Loss: 0.0136236, Inverse Loss: 0.0028438
Training -->> Epoch: 1048,(no reg loss)standard loss: 0.0045799, inverse loss: 0.0009560
Valid-->> Epoch [1048/3000], Standardized Loss: 0.0135950, Inverse Loss: 0.0028378
Training -->> Epoch: 1049,(no reg loss)standard loss: 0.0045684, inverse loss: 0.0009536
Valid-->> Epoch [1049/3000], Standardized Loss: 0.0135667, Inverse Loss: 0.0028319
Training -->> Epoch: 1050,(no reg loss)standard loss: 0.0045852, inverse loss: 0.0009571
Valid-->> Epoch [1050/3000], Standardized Loss: 0.0135775, Inverse Loss: 0.0028341
Training -->> Epoch: 1051,(no reg loss)standard loss: 0.0045700, inverse loss: 0.0009539
Valid-->> Epoch [1051/3000], Standardized Loss: 0.0135836, Inverse Loss: 0.0028354
Training -->> Epoch: 1052,(no reg loss)standard loss: 0.0044596, inverse loss: 0.0009309
Valid-->> Epoch [1052/3000], Standardized Loss: 0.0134781, Inverse Loss: 0.0028134
Valid-->> Lowest loss found at epoch 1052, loss: 0.0028134
Epoch 1052, Masked params (inverse standardized): tensor([3.229254e+01, 4.430030e+01, 6.657410e-02, 2.635801e+01, 2.769315e+01,
        1.781112e+01, 2.143254e+01, 1.637486e+00, 1.066119e+02, 2.755787e+01,
        2.615824e+01, 4.461431e+01, 2.443582e+01, 1.503763e+01, 8.461538e+01,
        2.835685e+01, 5.665241e+00, 2.982187e+01, 1.196838e+01, 3.204020e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1053,(no reg loss)standard loss: 0.0046647, inverse loss: 0.0009737
Valid-->> Epoch [1053/3000], Standardized Loss: 0.0135688, Inverse Loss: 0.0028323
Training -->> Epoch: 1054,(no reg loss)standard loss: 0.0044698, inverse loss: 0.0009330
Valid-->> Epoch [1054/3000], Standardized Loss: 0.0135660, Inverse Loss: 0.0028317
Training -->> Epoch: 1055,(no reg loss)standard loss: 0.0045669, inverse loss: 0.0009533
Valid-->> Epoch [1055/3000], Standardized Loss: 0.0135421, Inverse Loss: 0.0028268
Training -->> Epoch: 1056,(no reg loss)standard loss: 0.0045646, inverse loss: 0.0009528
Valid-->> Epoch [1056/3000], Standardized Loss: 0.0136918, Inverse Loss: 0.0028580
Training -->> Epoch: 1057,(no reg loss)standard loss: 0.0046828, inverse loss: 0.0009775
Valid-->> Epoch [1057/3000], Standardized Loss: 0.0134824, Inverse Loss: 0.0028143
Training -->> Epoch: 1058,(no reg loss)standard loss: 0.0045596, inverse loss: 0.0009518
Valid-->> Epoch [1058/3000], Standardized Loss: 0.0135630, Inverse Loss: 0.0028311
Training -->> Epoch: 1059,(no reg loss)standard loss: 0.0046409, inverse loss: 0.0009687
Valid-->> Epoch [1059/3000], Standardized Loss: 0.0134813, Inverse Loss: 0.0028141
Training -->> Epoch: 1060,(no reg loss)standard loss: 0.0045300, inverse loss: 0.0009456
Valid-->> Epoch [1060/3000], Standardized Loss: 0.0134745, Inverse Loss: 0.0028127
Valid-->> Lowest loss found at epoch 1060, loss: 0.0028127
Epoch 1060, Masked params (inverse standardized): tensor([3.229317e+01, 4.430121e+01, 6.973267e-02, 2.635776e+01, 2.769292e+01,
        1.781203e+01, 2.143311e+01, 1.640413e+00, 1.066113e+02, 2.755766e+01,
        2.615806e+01, 4.461603e+01, 2.443603e+01, 1.503861e+01, 8.461575e+01,
        2.835692e+01, 5.668694e+00, 2.982216e+01, 1.196968e+01, 3.204079e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1061,(no reg loss)standard loss: 0.0046316, inverse loss: 0.0009668
Valid-->> Epoch [1061/3000], Standardized Loss: 0.0136179, Inverse Loss: 0.0028426
Training -->> Epoch: 1062,(no reg loss)standard loss: 0.0045553, inverse loss: 0.0009509
Valid-->> Epoch [1062/3000], Standardized Loss: 0.0135189, Inverse Loss: 0.0028219
Training -->> Epoch: 1063,(no reg loss)standard loss: 0.0046243, inverse loss: 0.0009653
Valid-->> Epoch [1063/3000], Standardized Loss: 0.0135647, Inverse Loss: 0.0028315
Training -->> Epoch: 1064,(no reg loss)standard loss: 0.0045544, inverse loss: 0.0009507
Valid-->> Epoch [1064/3000], Standardized Loss: 0.0134877, Inverse Loss: 0.0028154
Training -->> Epoch: 1065,(no reg loss)standard loss: 0.0045442, inverse loss: 0.0009485
Valid-->> Epoch [1065/3000], Standardized Loss: 0.0136366, Inverse Loss: 0.0028465
Training -->> Epoch: 1066,(no reg loss)standard loss: 0.0045573, inverse loss: 0.0009513
Valid-->> Epoch [1066/3000], Standardized Loss: 0.0135084, Inverse Loss: 0.0028197
Training -->> Epoch: 1067,(no reg loss)standard loss: 0.0045363, inverse loss: 0.0009469
Valid-->> Epoch [1067/3000], Standardized Loss: 0.0135336, Inverse Loss: 0.0028250
Training -->> Epoch: 1068,(no reg loss)standard loss: 0.0045816, inverse loss: 0.0009564
Valid-->> Epoch [1068/3000], Standardized Loss: 0.0135592, Inverse Loss: 0.0028303
Training -->> Epoch: 1069,(no reg loss)standard loss: 0.0046515, inverse loss: 0.0009710
Valid-->> Epoch [1069/3000], Standardized Loss: 0.0135000, Inverse Loss: 0.0028180
Training -->> Epoch: 1070,(no reg loss)standard loss: 0.0044761, inverse loss: 0.0009343
Valid-->> Epoch [1070/3000], Standardized Loss: 0.0135925, Inverse Loss: 0.0028373
Training -->> Epoch: 1071,(no reg loss)standard loss: 0.0046856, inverse loss: 0.0009781
Valid-->> Epoch [1071/3000], Standardized Loss: 0.0134765, Inverse Loss: 0.0028131
Training -->> Epoch: 1072,(no reg loss)standard loss: 0.0044879, inverse loss: 0.0009368
Valid-->> Epoch [1072/3000], Standardized Loss: 0.0136277, Inverse Loss: 0.0028446
Training -->> Epoch: 1073,(no reg loss)standard loss: 0.0046394, inverse loss: 0.0009684
Valid-->> Epoch [1073/3000], Standardized Loss: 0.0135387, Inverse Loss: 0.0028261
Training -->> Epoch: 1074,(no reg loss)standard loss: 0.0047017, inverse loss: 0.0009814
Valid-->> Epoch [1074/3000], Standardized Loss: 0.0134900, Inverse Loss: 0.0028159
Training -->> Epoch: 1075,(no reg loss)standard loss: 0.0043643, inverse loss: 0.0009110
Valid-->> Epoch [1075/3000], Standardized Loss: 0.0135470, Inverse Loss: 0.0028278
Training -->> Epoch: 1076,(no reg loss)standard loss: 0.0046731, inverse loss: 0.0009755
Valid-->> Epoch [1076/3000], Standardized Loss: 0.0135288, Inverse Loss: 0.0028240
Training -->> Epoch: 1077,(no reg loss)standard loss: 0.0046723, inverse loss: 0.0009753
Valid-->> Epoch [1077/3000], Standardized Loss: 0.0134675, Inverse Loss: 0.0028112
Valid-->> Lowest loss found at epoch 1077, loss: 0.0028112
Epoch 1077, Masked params (inverse standardized): tensor([3.229441e+01, 4.430239e+01, 7.279015e-02, 2.635776e+01, 2.769272e+01,
        1.781382e+01, 2.143440e+01, 1.642435e+00, 1.066122e+02, 2.755764e+01,
        2.615784e+01, 4.461727e+01, 2.443572e+01, 1.504021e+01, 8.461729e+01,
        2.835636e+01, 5.671312e+00, 2.982218e+01, 1.197110e+01, 3.204185e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1078,(no reg loss)standard loss: 0.0043117, inverse loss: 0.0009000
Valid-->> Epoch [1078/3000], Standardized Loss: 0.0135739, Inverse Loss: 0.0028334
Training -->> Epoch: 1079,(no reg loss)standard loss: 0.0049012, inverse loss: 0.0010231
Valid-->> Epoch [1079/3000], Standardized Loss: 0.0135561, Inverse Loss: 0.0028297
Training -->> Epoch: 1080,(no reg loss)standard loss: 0.0044987, inverse loss: 0.0009390
Valid-->> Epoch [1080/3000], Standardized Loss: 0.0134755, Inverse Loss: 0.0028129
Training -->> Epoch: 1081,(no reg loss)standard loss: 0.0046215, inverse loss: 0.0009647
Valid-->> Epoch [1081/3000], Standardized Loss: 0.0135576, Inverse Loss: 0.0028300
Training -->> Epoch: 1082,(no reg loss)standard loss: 0.0044573, inverse loss: 0.0009304
Valid-->> Epoch [1082/3000], Standardized Loss: 0.0135039, Inverse Loss: 0.0028188
Training -->> Epoch: 1083,(no reg loss)standard loss: 0.0046172, inverse loss: 0.0009638
Valid-->> Epoch [1083/3000], Standardized Loss: 0.0135575, Inverse Loss: 0.0028300
Training -->> Epoch: 1084,(no reg loss)standard loss: 0.0046011, inverse loss: 0.0009604
Valid-->> Epoch [1084/3000], Standardized Loss: 0.0135218, Inverse Loss: 0.0028225
Training -->> Epoch: 1085,(no reg loss)standard loss: 0.0046686, inverse loss: 0.0009745
Valid-->> Epoch [1085/3000], Standardized Loss: 0.0134936, Inverse Loss: 0.0028166
Training -->> Epoch: 1086,(no reg loss)standard loss: 0.0044392, inverse loss: 0.0009266
Valid-->> Epoch [1086/3000], Standardized Loss: 0.0134643, Inverse Loss: 0.0028105
Valid-->> Lowest loss found at epoch 1086, loss: 0.0028105
Epoch 1086, Masked params (inverse standardized): tensor([3.229110e+01, 4.430173e+01, 6.468010e-02, 2.635868e+01, 2.769356e+01,
        1.780910e+01, 2.143100e+01, 1.635424e+00, 1.066122e+02, 2.755862e+01,
        2.615869e+01, 4.461175e+01, 2.443543e+01, 1.503537e+01, 8.461647e+01,
        2.835684e+01, 5.663895e+00, 2.982122e+01, 1.196589e+01, 3.203877e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1087,(no reg loss)standard loss: 0.0046124, inverse loss: 0.0009628
Valid-->> Epoch [1087/3000], Standardized Loss: 0.0135232, Inverse Loss: 0.0028228
Training -->> Epoch: 1088,(no reg loss)standard loss: 0.0046385, inverse loss: 0.0009682
Valid-->> Epoch [1088/3000], Standardized Loss: 0.0135365, Inverse Loss: 0.0028256
Training -->> Epoch: 1089,(no reg loss)standard loss: 0.0044800, inverse loss: 0.0009351
Valid-->> Epoch [1089/3000], Standardized Loss: 0.0134252, Inverse Loss: 0.0028024
Valid-->> Lowest loss found at epoch 1089, loss: 0.0028024
Epoch 1089, Masked params (inverse standardized): tensor([3.229319e+01, 4.430152e+01, 6.786919e-02, 2.635918e+01, 2.769402e+01,
        1.781162e+01, 2.143314e+01, 1.638674e+00, 1.066119e+02, 2.755910e+01,
        2.615914e+01, 4.461549e+01, 2.443625e+01, 1.503830e+01, 8.461604e+01,
        2.835726e+01, 5.666494e+00, 2.982240e+01, 1.196941e+01, 3.204089e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1090,(no reg loss)standard loss: 0.0044592, inverse loss: 0.0009308
Valid-->> Epoch [1090/3000], Standardized Loss: 0.0136036, Inverse Loss: 0.0028396
Training -->> Epoch: 1091,(no reg loss)standard loss: 0.0046784, inverse loss: 0.0009766
Valid-->> Epoch [1091/3000], Standardized Loss: 0.0135085, Inverse Loss: 0.0028197
Training -->> Epoch: 1092,(no reg loss)standard loss: 0.0045453, inverse loss: 0.0009488
Valid-->> Epoch [1092/3000], Standardized Loss: 0.0135214, Inverse Loss: 0.0028224
Training -->> Epoch: 1093,(no reg loss)standard loss: 0.0046548, inverse loss: 0.0009716
Valid-->> Epoch [1093/3000], Standardized Loss: 0.0134384, Inverse Loss: 0.0028051
Training -->> Epoch: 1094,(no reg loss)standard loss: 0.0045000, inverse loss: 0.0009393
Valid-->> Epoch [1094/3000], Standardized Loss: 0.0135753, Inverse Loss: 0.0028337
Training -->> Epoch: 1095,(no reg loss)standard loss: 0.0045609, inverse loss: 0.0009520
Valid-->> Epoch [1095/3000], Standardized Loss: 0.0135546, Inverse Loss: 0.0028294
Training -->> Epoch: 1096,(no reg loss)standard loss: 0.0047365, inverse loss: 0.0009887
Valid-->> Epoch [1096/3000], Standardized Loss: 0.0133931, Inverse Loss: 0.0027957
Valid-->> Lowest loss found at epoch 1096, loss: 0.0027957
Epoch 1096, Masked params (inverse standardized): tensor([3.229522e+01, 4.430076e+01, 7.295799e-02, 2.635943e+01, 2.769429e+01,
        1.781529e+01, 2.143519e+01, 1.643559e+00, 1.066124e+02, 2.755934e+01,
        2.615941e+01, 4.461957e+01, 2.443700e+01, 1.504215e+01, 8.461554e+01,
        2.835779e+01, 5.671255e+00, 2.982332e+01, 1.197350e+01, 3.204273e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1097,(no reg loss)standard loss: 0.0046659, inverse loss: 0.0009740
Valid-->> Epoch [1097/3000], Standardized Loss: 0.0135522, Inverse Loss: 0.0028289
Training -->> Epoch: 1098,(no reg loss)standard loss: 0.0045977, inverse loss: 0.0009597
Valid-->> Epoch [1098/3000], Standardized Loss: 0.0134941, Inverse Loss: 0.0028167
Training -->> Epoch: 1099,(no reg loss)standard loss: 0.0044064, inverse loss: 0.0009198
Valid-->> Epoch [1099/3000], Standardized Loss: 0.0134912, Inverse Loss: 0.0028161
Training -->> Epoch: 1100,(no reg loss)standard loss: 0.0046175, inverse loss: 0.0009638
Valid-->> Epoch [1100/3000], Standardized Loss: 0.0135644, Inverse Loss: 0.0028314
Training -->> Epoch: 1101,(no reg loss)standard loss: 0.0046178, inverse loss: 0.0009639
Valid-->> Epoch [1101/3000], Standardized Loss: 0.0135601, Inverse Loss: 0.0028305
Training -->> Epoch: 1102,(no reg loss)standard loss: 0.0046336, inverse loss: 0.0009672
Valid-->> Epoch [1102/3000], Standardized Loss: 0.0134655, Inverse Loss: 0.0028108
Training -->> Epoch: 1103,(no reg loss)standard loss: 0.0045733, inverse loss: 0.0009546
Valid-->> Epoch [1103/3000], Standardized Loss: 0.0134491, Inverse Loss: 0.0028074
Training -->> Epoch: 1104,(no reg loss)standard loss: 0.0045014, inverse loss: 0.0009396
Valid-->> Epoch [1104/3000], Standardized Loss: 0.0135204, Inverse Loss: 0.0028222
Training -->> Epoch: 1105,(no reg loss)standard loss: 0.0046991, inverse loss: 0.0009809
Valid-->> Epoch [1105/3000], Standardized Loss: 0.0134949, Inverse Loss: 0.0028169
Training -->> Epoch: 1106,(no reg loss)standard loss: 0.0045335, inverse loss: 0.0009463
Valid-->> Epoch [1106/3000], Standardized Loss: 0.0136207, Inverse Loss: 0.0028432
Training -->> Epoch: 1107,(no reg loss)standard loss: 0.0046286, inverse loss: 0.0009662
Valid-->> Epoch [1107/3000], Standardized Loss: 0.0134966, Inverse Loss: 0.0028173
Training -->> Epoch: 1108,(no reg loss)standard loss: 0.0047004, inverse loss: 0.0009812
Valid-->> Epoch [1108/3000], Standardized Loss: 0.0134073, Inverse Loss: 0.0027986
Training -->> Epoch: 1109,(no reg loss)standard loss: 0.0046003, inverse loss: 0.0009603
Valid-->> Epoch [1109/3000], Standardized Loss: 0.0134641, Inverse Loss: 0.0028105
Training -->> Epoch: 1110,(no reg loss)standard loss: 0.0046105, inverse loss: 0.0009624
Valid-->> Epoch [1110/3000], Standardized Loss: 0.0134453, Inverse Loss: 0.0028065
Training -->> Epoch: 1111,(no reg loss)standard loss: 0.0045678, inverse loss: 0.0009535
Valid-->> Epoch [1111/3000], Standardized Loss: 0.0135237, Inverse Loss: 0.0028229
Training -->> Epoch: 1112,(no reg loss)standard loss: 0.0046613, inverse loss: 0.0009730
Valid-->> Epoch [1112/3000], Standardized Loss: 0.0134049, Inverse Loss: 0.0027981
Training -->> Epoch: 1113,(no reg loss)standard loss: 0.0045234, inverse loss: 0.0009442
Valid-->> Epoch [1113/3000], Standardized Loss: 0.0135830, Inverse Loss: 0.0028353
Training -->> Epoch: 1114,(no reg loss)standard loss: 0.0047735, inverse loss: 0.0009964
Valid-->> Epoch [1114/3000], Standardized Loss: 0.0133561, Inverse Loss: 0.0027879
Valid-->> Lowest loss found at epoch 1114, loss: 0.0027879
Epoch 1114, Masked params (inverse standardized): tensor([3.229600e+01, 4.430056e+01, 7.268333e-02, 2.636016e+01, 2.769501e+01,
        1.781648e+01, 2.143597e+01, 1.643646e+00, 1.066119e+02, 2.756005e+01,
        2.616014e+01, 4.462094e+01, 2.443775e+01, 1.504360e+01, 8.461555e+01,
        2.835850e+01, 5.671104e+00, 2.982409e+01, 1.197502e+01, 3.204351e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1115,(no reg loss)standard loss: 0.0045327, inverse loss: 0.0009461
Valid-->> Epoch [1115/3000], Standardized Loss: 0.0135223, Inverse Loss: 0.0028226
Training -->> Epoch: 1116,(no reg loss)standard loss: 0.0045840, inverse loss: 0.0009569
Valid-->> Epoch [1116/3000], Standardized Loss: 0.0135334, Inverse Loss: 0.0028249
Training -->> Epoch: 1117,(no reg loss)standard loss: 0.0046701, inverse loss: 0.0009748
Valid-->> Epoch [1117/3000], Standardized Loss: 0.0133899, Inverse Loss: 0.0027950
Training -->> Epoch: 1118,(no reg loss)standard loss: 0.0045062, inverse loss: 0.0009406
Valid-->> Epoch [1118/3000], Standardized Loss: 0.0135439, Inverse Loss: 0.0028271
Training -->> Epoch: 1119,(no reg loss)standard loss: 0.0046681, inverse loss: 0.0009744
Valid-->> Epoch [1119/3000], Standardized Loss: 0.0134580, Inverse Loss: 0.0028092
Training -->> Epoch: 1120,(no reg loss)standard loss: 0.0044936, inverse loss: 0.0009380
Valid-->> Epoch [1120/3000], Standardized Loss: 0.0134506, Inverse Loss: 0.0028077
Training -->> Epoch: 1121,(no reg loss)standard loss: 0.0046610, inverse loss: 0.0009729
Valid-->> Epoch [1121/3000], Standardized Loss: 0.0135524, Inverse Loss: 0.0028289
Training -->> Epoch: 1122,(no reg loss)standard loss: 0.0045838, inverse loss: 0.0009568
Valid-->> Epoch [1122/3000], Standardized Loss: 0.0134978, Inverse Loss: 0.0028175
Training -->> Epoch: 1123,(no reg loss)standard loss: 0.0046371, inverse loss: 0.0009679
Valid-->> Epoch [1123/3000], Standardized Loss: 0.0134708, Inverse Loss: 0.0028119
Training -->> Epoch: 1124,(no reg loss)standard loss: 0.0046062, inverse loss: 0.0009615
Valid-->> Epoch [1124/3000], Standardized Loss: 0.0134573, Inverse Loss: 0.0028091
Training -->> Epoch: 1125,(no reg loss)standard loss: 0.0045527, inverse loss: 0.0009503
Valid-->> Epoch [1125/3000], Standardized Loss: 0.0135411, Inverse Loss: 0.0028266
Training -->> Epoch: 1126,(no reg loss)standard loss: 0.0046693, inverse loss: 0.0009747
Valid-->> Epoch [1126/3000], Standardized Loss: 0.0134471, Inverse Loss: 0.0028069
Training -->> Epoch: 1127,(no reg loss)standard loss: 0.0046340, inverse loss: 0.0009673
Valid-->> Epoch [1127/3000], Standardized Loss: 0.0134323, Inverse Loss: 0.0028038
Training -->> Epoch: 1128,(no reg loss)standard loss: 0.0045158, inverse loss: 0.0009426
Valid-->> Epoch [1128/3000], Standardized Loss: 0.0135640, Inverse Loss: 0.0028313
Training -->> Epoch: 1129,(no reg loss)standard loss: 0.0046751, inverse loss: 0.0009759
Valid-->> Epoch [1129/3000], Standardized Loss: 0.0134148, Inverse Loss: 0.0028002
Training -->> Epoch: 1130,(no reg loss)standard loss: 0.0046034, inverse loss: 0.0009609
Valid-->> Epoch [1130/3000], Standardized Loss: 0.0135842, Inverse Loss: 0.0028355
Training -->> Epoch: 1131,(no reg loss)standard loss: 0.0047654, inverse loss: 0.0009947
Valid-->> Epoch [1131/3000], Standardized Loss: 0.0133437, Inverse Loss: 0.0027853
Valid-->> Lowest loss found at epoch 1131, loss: 0.0027853
Epoch 1131, Masked params (inverse standardized): tensor([3.229624e+01, 4.430073e+01, 7.183266e-02, 2.636037e+01, 2.769514e+01,
        1.781627e+01, 2.143619e+01, 1.642675e+00, 1.066114e+02, 2.756029e+01,
        2.616026e+01, 4.462012e+01, 2.443793e+01, 1.504319e+01, 8.461537e+01,
        2.835858e+01, 5.670242e+00, 2.982434e+01, 1.197437e+01, 3.204377e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1132,(no reg loss)standard loss: 0.0045529, inverse loss: 0.0009504
Valid-->> Epoch [1132/3000], Standardized Loss: 0.0134675, Inverse Loss: 0.0028112
Training -->> Epoch: 1133,(no reg loss)standard loss: 0.0046041, inverse loss: 0.0009611
Valid-->> Epoch [1133/3000], Standardized Loss: 0.0135107, Inverse Loss: 0.0028202
Training -->> Epoch: 1134,(no reg loss)standard loss: 0.0046190, inverse loss: 0.0009642
Valid-->> Epoch [1134/3000], Standardized Loss: 0.0134562, Inverse Loss: 0.0028088
Training -->> Epoch: 1135,(no reg loss)standard loss: 0.0045368, inverse loss: 0.0009470
Valid-->> Epoch [1135/3000], Standardized Loss: 0.0134993, Inverse Loss: 0.0028178
Training -->> Epoch: 1136,(no reg loss)standard loss: 0.0047093, inverse loss: 0.0009830
Valid-->> Epoch [1136/3000], Standardized Loss: 0.0133734, Inverse Loss: 0.0027916
Training -->> Epoch: 1137,(no reg loss)standard loss: 0.0044745, inverse loss: 0.0009340
Valid-->> Epoch [1137/3000], Standardized Loss: 0.0135902, Inverse Loss: 0.0028368
Training -->> Epoch: 1138,(no reg loss)standard loss: 0.0047529, inverse loss: 0.0009921
Valid-->> Epoch [1138/3000], Standardized Loss: 0.0134203, Inverse Loss: 0.0028013
Training -->> Epoch: 1139,(no reg loss)standard loss: 0.0046369, inverse loss: 0.0009679
Valid-->> Epoch [1139/3000], Standardized Loss: 0.0134175, Inverse Loss: 0.0028008
Training -->> Epoch: 1140,(no reg loss)standard loss: 0.0045435, inverse loss: 0.0009484
Valid-->> Epoch [1140/3000], Standardized Loss: 0.0134746, Inverse Loss: 0.0028127
Training -->> Epoch: 1141,(no reg loss)standard loss: 0.0046278, inverse loss: 0.0009660
Valid-->> Epoch [1141/3000], Standardized Loss: 0.0134593, Inverse Loss: 0.0028095
Training -->> Epoch: 1142,(no reg loss)standard loss: 0.0046318, inverse loss: 0.0009668
Valid-->> Epoch [1142/3000], Standardized Loss: 0.0134687, Inverse Loss: 0.0028114
Training -->> Epoch: 1143,(no reg loss)standard loss: 0.0045507, inverse loss: 0.0009499
Valid-->> Epoch [1143/3000], Standardized Loss: 0.0134337, Inverse Loss: 0.0028041
Training -->> Epoch: 1144,(no reg loss)standard loss: 0.0046075, inverse loss: 0.0009618
Valid-->> Epoch [1144/3000], Standardized Loss: 0.0134890, Inverse Loss: 0.0028157
Training -->> Epoch: 1145,(no reg loss)standard loss: 0.0046602, inverse loss: 0.0009728
Valid-->> Epoch [1145/3000], Standardized Loss: 0.0135136, Inverse Loss: 0.0028208
Training -->> Epoch: 1146,(no reg loss)standard loss: 0.0045710, inverse loss: 0.0009541
Valid-->> Epoch [1146/3000], Standardized Loss: 0.0134321, Inverse Loss: 0.0028038
Training -->> Epoch: 1147,(no reg loss)standard loss: 0.0047183, inverse loss: 0.0009849
Valid-->> Epoch [1147/3000], Standardized Loss: 0.0134093, Inverse Loss: 0.0027990
Training -->> Epoch: 1148,(no reg loss)standard loss: 0.0044884, inverse loss: 0.0009369
Valid-->> Epoch [1148/3000], Standardized Loss: 0.0134439, Inverse Loss: 0.0028063
Training -->> Epoch: 1149,(no reg loss)standard loss: 0.0046992, inverse loss: 0.0009809
Valid-->> Epoch [1149/3000], Standardized Loss: 0.0135484, Inverse Loss: 0.0028281
Training -->> Epoch: 1150,(no reg loss)standard loss: 0.0046317, inverse loss: 0.0009668
Valid-->> Epoch [1150/3000], Standardized Loss: 0.0133391, Inverse Loss: 0.0027844
Valid-->> Lowest loss found at epoch 1150, loss: 0.0027844
Epoch 1150, Masked params (inverse standardized): tensor([3.229506e+01, 4.430028e+01, 7.029533e-02, 2.636083e+01, 2.769546e+01,
        1.781411e+01, 2.143503e+01, 1.640211e+00, 1.066112e+02, 2.756078e+01,
        2.616058e+01, 4.461654e+01, 2.443742e+01, 1.504035e+01, 8.461470e+01,
        2.835843e+01, 5.668373e+00, 2.982362e+01, 1.197093e+01, 3.204267e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1151,(no reg loss)standard loss: 0.0046521, inverse loss: 0.0009711
Valid-->> Epoch [1151/3000], Standardized Loss: 0.0134080, Inverse Loss: 0.0027988
Training -->> Epoch: 1152,(no reg loss)standard loss: 0.0044431, inverse loss: 0.0009275
Valid-->> Epoch [1152/3000], Standardized Loss: 0.0135320, Inverse Loss: 0.0028247
Training -->> Epoch: 1153,(no reg loss)standard loss: 0.0048631, inverse loss: 0.0010151
Valid-->> Epoch [1153/3000], Standardized Loss: 0.0134261, Inverse Loss: 0.0028025
Training -->> Epoch: 1154,(no reg loss)standard loss: 0.0045852, inverse loss: 0.0009571
Valid-->> Epoch [1154/3000], Standardized Loss: 0.0133948, Inverse Loss: 0.0027960
Training -->> Epoch: 1155,(no reg loss)standard loss: 0.0044017, inverse loss: 0.0009188
Valid-->> Epoch [1155/3000], Standardized Loss: 0.0135182, Inverse Loss: 0.0028218
Training -->> Epoch: 1156,(no reg loss)standard loss: 0.0048990, inverse loss: 0.0010226
Valid-->> Epoch [1156/3000], Standardized Loss: 0.0133502, Inverse Loss: 0.0027867
Training -->> Epoch: 1157,(no reg loss)standard loss: 0.0045336, inverse loss: 0.0009463
Valid-->> Epoch [1157/3000], Standardized Loss: 0.0134306, Inverse Loss: 0.0028035
Training -->> Epoch: 1158,(no reg loss)standard loss: 0.0045393, inverse loss: 0.0009475
Valid-->> Epoch [1158/3000], Standardized Loss: 0.0135415, Inverse Loss: 0.0028266
Training -->> Epoch: 1159,(no reg loss)standard loss: 0.0047229, inverse loss: 0.0009859
Valid-->> Epoch [1159/3000], Standardized Loss: 0.0134218, Inverse Loss: 0.0028016
Training -->> Epoch: 1160,(no reg loss)standard loss: 0.0045152, inverse loss: 0.0009425
Valid-->> Epoch [1160/3000], Standardized Loss: 0.0133696, Inverse Loss: 0.0027908
Training -->> Epoch: 1161,(no reg loss)standard loss: 0.0047961, inverse loss: 0.0010011
Valid-->> Epoch [1161/3000], Standardized Loss: 0.0135023, Inverse Loss: 0.0028184
Training -->> Epoch: 1162,(no reg loss)standard loss: 0.0044163, inverse loss: 0.0009219
Valid-->> Epoch [1162/3000], Standardized Loss: 0.0134240, Inverse Loss: 0.0028021
Training -->> Epoch: 1163,(no reg loss)standard loss: 0.0047930, inverse loss: 0.0010005
Valid-->> Epoch [1163/3000], Standardized Loss: 0.0133562, Inverse Loss: 0.0027880
Training -->> Epoch: 1164,(no reg loss)standard loss: 0.0045031, inverse loss: 0.0009400
Valid-->> Epoch [1164/3000], Standardized Loss: 0.0134848, Inverse Loss: 0.0028148
Training -->> Epoch: 1165,(no reg loss)standard loss: 0.0047467, inverse loss: 0.0009908
Valid-->> Epoch [1165/3000], Standardized Loss: 0.0133138, Inverse Loss: 0.0027791
Valid-->> Lowest loss found at epoch 1165, loss: 0.0027791
Epoch 1165, Masked params (inverse standardized): tensor([3.229500e+01, 4.430133e+01, 7.213020e-02, 2.636145e+01, 2.769586e+01,
        1.781499e+01, 2.143495e+01, 1.643463e+00, 1.066120e+02, 2.756138e+01,
        2.616100e+01, 4.462084e+01, 2.443734e+01, 1.504232e+01, 8.461579e+01,
        2.835845e+01, 5.670309e+00, 2.982357e+01, 1.197446e+01, 3.204255e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1166,(no reg loss)standard loss: 0.0044714, inverse loss: 0.0009333
Valid-->> Epoch [1166/3000], Standardized Loss: 0.0135257, Inverse Loss: 0.0028233
Training -->> Epoch: 1167,(no reg loss)standard loss: 0.0046877, inverse loss: 0.0009785
Valid-->> Epoch [1167/3000], Standardized Loss: 0.0133894, Inverse Loss: 0.0027949
Training -->> Epoch: 1168,(no reg loss)standard loss: 0.0046507, inverse loss: 0.0009708
Valid-->> Epoch [1168/3000], Standardized Loss: 0.0134086, Inverse Loss: 0.0027989
Training -->> Epoch: 1169,(no reg loss)standard loss: 0.0046568, inverse loss: 0.0009721
Valid-->> Epoch [1169/3000], Standardized Loss: 0.0134386, Inverse Loss: 0.0028052
Training -->> Epoch: 1170,(no reg loss)standard loss: 0.0046079, inverse loss: 0.0009618
Valid-->> Epoch [1170/3000], Standardized Loss: 0.0134045, Inverse Loss: 0.0027980
Training -->> Epoch: 1171,(no reg loss)standard loss: 0.0047031, inverse loss: 0.0009817
Valid-->> Epoch [1171/3000], Standardized Loss: 0.0133278, Inverse Loss: 0.0027820
Training -->> Epoch: 1172,(no reg loss)standard loss: 0.0045055, inverse loss: 0.0009405
Valid-->> Epoch [1172/3000], Standardized Loss: 0.0134279, Inverse Loss: 0.0028029
Training -->> Epoch: 1173,(no reg loss)standard loss: 0.0046425, inverse loss: 0.0009691
Valid-->> Epoch [1173/3000], Standardized Loss: 0.0134117, Inverse Loss: 0.0027995
Training -->> Epoch: 1174,(no reg loss)standard loss: 0.0046411, inverse loss: 0.0009688
Valid-->> Epoch [1174/3000], Standardized Loss: 0.0134267, Inverse Loss: 0.0028027
Training -->> Epoch: 1175,(no reg loss)standard loss: 0.0045188, inverse loss: 0.0009433
Valid-->> Epoch [1175/3000], Standardized Loss: 0.0134127, Inverse Loss: 0.0027998
Training -->> Epoch: 1176,(no reg loss)standard loss: 0.0047826, inverse loss: 0.0009983
Valid-->> Epoch [1176/3000], Standardized Loss: 0.0133645, Inverse Loss: 0.0027897
Training -->> Epoch: 1177,(no reg loss)standard loss: 0.0045422, inverse loss: 0.0009481
Valid-->> Epoch [1177/3000], Standardized Loss: 0.0133469, Inverse Loss: 0.0027860
Training -->> Epoch: 1178,(no reg loss)standard loss: 0.0047172, inverse loss: 0.0009847
Valid-->> Epoch [1178/3000], Standardized Loss: 0.0132967, Inverse Loss: 0.0027755
Valid-->> Lowest loss found at epoch 1178, loss: 0.0027755
Epoch 1178, Masked params (inverse standardized): tensor([3.229436e+01, 4.430045e+01, 7.192039e-02, 2.636200e+01, 2.769639e+01,
        1.781415e+01, 2.143431e+01, 1.642855e+00, 1.066123e+02, 2.756198e+01,
        2.616148e+01, 4.461954e+01, 2.443745e+01, 1.504132e+01, 8.461510e+01,
        2.835885e+01, 5.670441e+00, 2.982344e+01, 1.197317e+01, 3.204198e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1179,(no reg loss)standard loss: 0.0047187, inverse loss: 0.0009850
Valid-->> Epoch [1179/3000], Standardized Loss: 0.0133870, Inverse Loss: 0.0027944
Training -->> Epoch: 1180,(no reg loss)standard loss: 0.0044504, inverse loss: 0.0009290
Valid-->> Epoch [1180/3000], Standardized Loss: 0.0133636, Inverse Loss: 0.0027895
Training -->> Epoch: 1181,(no reg loss)standard loss: 0.0047504, inverse loss: 0.0009916
Valid-->> Epoch [1181/3000], Standardized Loss: 0.0133412, Inverse Loss: 0.0027848
Training -->> Epoch: 1182,(no reg loss)standard loss: 0.0045776, inverse loss: 0.0009555
Valid-->> Epoch [1182/3000], Standardized Loss: 0.0133409, Inverse Loss: 0.0027848
Training -->> Epoch: 1183,(no reg loss)standard loss: 0.0046078, inverse loss: 0.0009618
Valid-->> Epoch [1183/3000], Standardized Loss: 0.0134151, Inverse Loss: 0.0028002
Training -->> Epoch: 1184,(no reg loss)standard loss: 0.0045892, inverse loss: 0.0009579
Valid-->> Epoch [1184/3000], Standardized Loss: 0.0134275, Inverse Loss: 0.0028028
Training -->> Epoch: 1185,(no reg loss)standard loss: 0.0046242, inverse loss: 0.0009652
Valid-->> Epoch [1185/3000], Standardized Loss: 0.0134886, Inverse Loss: 0.0028156
Training -->> Epoch: 1186,(no reg loss)standard loss: 0.0046373, inverse loss: 0.0009680
Valid-->> Epoch [1186/3000], Standardized Loss: 0.0133302, Inverse Loss: 0.0027825
Training -->> Epoch: 1187,(no reg loss)standard loss: 0.0045073, inverse loss: 0.0009408
Valid-->> Epoch [1187/3000], Standardized Loss: 0.0134704, Inverse Loss: 0.0028118
Training -->> Epoch: 1188,(no reg loss)standard loss: 0.0047941, inverse loss: 0.0010007
Valid-->> Epoch [1188/3000], Standardized Loss: 0.0134294, Inverse Loss: 0.0028032
Training -->> Epoch: 1189,(no reg loss)standard loss: 0.0044972, inverse loss: 0.0009387
Valid-->> Epoch [1189/3000], Standardized Loss: 0.0134032, Inverse Loss: 0.0027978
Training -->> Epoch: 1190,(no reg loss)standard loss: 0.0047099, inverse loss: 0.0009831
Valid-->> Epoch [1190/3000], Standardized Loss: 0.0134504, Inverse Loss: 0.0028076
Training -->> Epoch: 1191,(no reg loss)standard loss: 0.0046760, inverse loss: 0.0009761
Valid-->> Epoch [1191/3000], Standardized Loss: 0.0133479, Inverse Loss: 0.0027862
Training -->> Epoch: 1192,(no reg loss)standard loss: 0.0044767, inverse loss: 0.0009345
Valid-->> Epoch [1192/3000], Standardized Loss: 0.0133275, Inverse Loss: 0.0027820
Training -->> Epoch: 1193,(no reg loss)standard loss: 0.0045733, inverse loss: 0.0009546
Valid-->> Epoch [1193/3000], Standardized Loss: 0.0134147, Inverse Loss: 0.0028002
Training -->> Epoch: 1194,(no reg loss)standard loss: 0.0048129, inverse loss: 0.0010046
Valid-->> Epoch [1194/3000], Standardized Loss: 0.0132795, Inverse Loss: 0.0027720
Valid-->> Lowest loss found at epoch 1194, loss: 0.0027720
Epoch 1194, Masked params (inverse standardized): tensor([3.229601e+01, 4.430152e+01, 7.330894e-02, 2.636218e+01, 2.769647e+01,
        1.781625e+01, 2.143599e+01, 1.643927e+00, 1.066119e+02, 2.756222e+01,
        2.616154e+01, 4.462029e+01, 2.443745e+01, 1.504297e+01, 8.461608e+01,
        2.835870e+01, 5.671913e+00, 2.982373e+01, 1.197417e+01, 3.204351e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1195,(no reg loss)standard loss: 0.0044179, inverse loss: 0.0009222
Valid-->> Epoch [1195/3000], Standardized Loss: 0.0134859, Inverse Loss: 0.0028150
Training -->> Epoch: 1196,(no reg loss)standard loss: 0.0048243, inverse loss: 0.0010070
Valid-->> Epoch [1196/3000], Standardized Loss: 0.0134563, Inverse Loss: 0.0028088
Training -->> Epoch: 1197,(no reg loss)standard loss: 0.0045438, inverse loss: 0.0009485
Valid-->> Epoch [1197/3000], Standardized Loss: 0.0133935, Inverse Loss: 0.0027957
Training -->> Epoch: 1198,(no reg loss)standard loss: 0.0046295, inverse loss: 0.0009664
Valid-->> Epoch [1198/3000], Standardized Loss: 0.0135344, Inverse Loss: 0.0028251
Training -->> Epoch: 1199,(no reg loss)standard loss: 0.0047629, inverse loss: 0.0009942
Valid-->> Epoch [1199/3000], Standardized Loss: 0.0133097, Inverse Loss: 0.0027783
Training -->> Epoch: 1200,(no reg loss)standard loss: 0.0045823, inverse loss: 0.0009565
Valid-->> Epoch [1200/3000], Standardized Loss: 0.0133512, Inverse Loss: 0.0027869
Training -->> Epoch: 1201,(no reg loss)standard loss: 0.0045433, inverse loss: 0.0009484
Valid-->> Epoch [1201/3000], Standardized Loss: 0.0133933, Inverse Loss: 0.0027957
Training -->> Epoch: 1202,(no reg loss)standard loss: 0.0045739, inverse loss: 0.0009548
Valid-->> Epoch [1202/3000], Standardized Loss: 0.0132889, Inverse Loss: 0.0027739
Training -->> Epoch: 1203,(no reg loss)standard loss: 0.0045762, inverse loss: 0.0009552
Valid-->> Epoch [1203/3000], Standardized Loss: 0.0134348, Inverse Loss: 0.0028044
Training -->> Epoch: 1204,(no reg loss)standard loss: 0.0046274, inverse loss: 0.0009659
Valid-->> Epoch [1204/3000], Standardized Loss: 0.0133263, Inverse Loss: 0.0027817
Training -->> Epoch: 1205,(no reg loss)standard loss: 0.0046842, inverse loss: 0.0009778
Valid-->> Epoch [1205/3000], Standardized Loss: 0.0133449, Inverse Loss: 0.0027856
Training -->> Epoch: 1206,(no reg loss)standard loss: 0.0047491, inverse loss: 0.0009913
Valid-->> Epoch [1206/3000], Standardized Loss: 0.0132865, Inverse Loss: 0.0027734
Training -->> Epoch: 1207,(no reg loss)standard loss: 0.0044634, inverse loss: 0.0009317
Valid-->> Epoch [1207/3000], Standardized Loss: 0.0133819, Inverse Loss: 0.0027933
Training -->> Epoch: 1208,(no reg loss)standard loss: 0.0046443, inverse loss: 0.0009694
Valid-->> Epoch [1208/3000], Standardized Loss: 0.0134070, Inverse Loss: 0.0027986
Training -->> Epoch: 1209,(no reg loss)standard loss: 0.0046959, inverse loss: 0.0009802
Valid-->> Epoch [1209/3000], Standardized Loss: 0.0133289, Inverse Loss: 0.0027823
Training -->> Epoch: 1210,(no reg loss)standard loss: 0.0045401, inverse loss: 0.0009477
Valid-->> Epoch [1210/3000], Standardized Loss: 0.0133818, Inverse Loss: 0.0027933
Training -->> Epoch: 1211,(no reg loss)standard loss: 0.0047300, inverse loss: 0.0009873
Valid-->> Epoch [1211/3000], Standardized Loss: 0.0132757, Inverse Loss: 0.0027711
Valid-->> Lowest loss found at epoch 1211, loss: 0.0027711
Epoch 1211, Masked params (inverse standardized): tensor([3.229429e+01, 4.430068e+01, 7.151222e-02, 2.636264e+01, 2.769678e+01,
        1.781452e+01, 2.143429e+01, 1.642332e+00, 1.066124e+02, 2.756263e+01,
        2.616189e+01, 4.461906e+01, 2.443728e+01, 1.504154e+01, 8.461574e+01,
        2.835880e+01, 5.669823e+00, 2.982325e+01, 1.197301e+01, 3.204194e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1212,(no reg loss)standard loss: 0.0046658, inverse loss: 0.0009739
Valid-->> Epoch [1212/3000], Standardized Loss: 0.0134265, Inverse Loss: 0.0028026
Training -->> Epoch: 1213,(no reg loss)standard loss: 0.0045946, inverse loss: 0.0009591
Valid-->> Epoch [1213/3000], Standardized Loss: 0.0133106, Inverse Loss: 0.0027784
Training -->> Epoch: 1214,(no reg loss)standard loss: 0.0045066, inverse loss: 0.0009407
Valid-->> Epoch [1214/3000], Standardized Loss: 0.0134486, Inverse Loss: 0.0028072
Training -->> Epoch: 1215,(no reg loss)standard loss: 0.0047467, inverse loss: 0.0009908
Valid-->> Epoch [1215/3000], Standardized Loss: 0.0132831, Inverse Loss: 0.0027727
Training -->> Epoch: 1216,(no reg loss)standard loss: 0.0045851, inverse loss: 0.0009571
Valid-->> Epoch [1216/3000], Standardized Loss: 0.0133630, Inverse Loss: 0.0027894
Training -->> Epoch: 1217,(no reg loss)standard loss: 0.0046001, inverse loss: 0.0009602
Valid-->> Epoch [1217/3000], Standardized Loss: 0.0133521, Inverse Loss: 0.0027871
Training -->> Epoch: 1218,(no reg loss)standard loss: 0.0046988, inverse loss: 0.0009808
Valid-->> Epoch [1218/3000], Standardized Loss: 0.0134770, Inverse Loss: 0.0028132
Training -->> Epoch: 1219,(no reg loss)standard loss: 0.0047057, inverse loss: 0.0009823
Valid-->> Epoch [1219/3000], Standardized Loss: 0.0133529, Inverse Loss: 0.0027873
Training -->> Epoch: 1220,(no reg loss)standard loss: 0.0045383, inverse loss: 0.0009473
Valid-->> Epoch [1220/3000], Standardized Loss: 0.0134096, Inverse Loss: 0.0027991
Training -->> Epoch: 1221,(no reg loss)standard loss: 0.0047103, inverse loss: 0.0009832
Valid-->> Epoch [1221/3000], Standardized Loss: 0.0133906, Inverse Loss: 0.0027951
Training -->> Epoch: 1222,(no reg loss)standard loss: 0.0047184, inverse loss: 0.0009849
Valid-->> Epoch [1222/3000], Standardized Loss: 0.0132331, Inverse Loss: 0.0027623
Valid-->> Lowest loss found at epoch 1222, loss: 0.0027623
Epoch 1222, Masked params (inverse standardized): tensor([3.229593e+01, 4.430070e+01, 7.130623e-02, 2.636349e+01, 2.769761e+01,
        1.781505e+01, 2.143591e+01, 1.641918e+00, 1.066120e+02, 2.756346e+01,
        2.616271e+01, 4.461818e+01, 2.443842e+01, 1.504141e+01, 8.461531e+01,
        2.835979e+01, 5.670292e+00, 2.982452e+01, 1.197221e+01, 3.204350e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1223,(no reg loss)standard loss: 0.0045650, inverse loss: 0.0009529
Valid-->> Epoch [1223/3000], Standardized Loss: 0.0134204, Inverse Loss: 0.0028013
Training -->> Epoch: 1224,(no reg loss)standard loss: 0.0047774, inverse loss: 0.0009972
Valid-->> Epoch [1224/3000], Standardized Loss: 0.0132613, Inverse Loss: 0.0027682
Training -->> Epoch: 1225,(no reg loss)standard loss: 0.0045358, inverse loss: 0.0009468
Valid-->> Epoch [1225/3000], Standardized Loss: 0.0133526, Inverse Loss: 0.0027872
Training -->> Epoch: 1226,(no reg loss)standard loss: 0.0047696, inverse loss: 0.0009956
Valid-->> Epoch [1226/3000], Standardized Loss: 0.0133462, Inverse Loss: 0.0027859
Training -->> Epoch: 1227,(no reg loss)standard loss: 0.0045210, inverse loss: 0.0009437
Valid-->> Epoch [1227/3000], Standardized Loss: 0.0133392, Inverse Loss: 0.0027844
Training -->> Epoch: 1228,(no reg loss)standard loss: 0.0046829, inverse loss: 0.0009775
Valid-->> Epoch [1228/3000], Standardized Loss: 0.0134609, Inverse Loss: 0.0028098
Training -->> Epoch: 1229,(no reg loss)standard loss: 0.0046608, inverse loss: 0.0009729
Valid-->> Epoch [1229/3000], Standardized Loss: 0.0133422, Inverse Loss: 0.0027850
Training -->> Epoch: 1230,(no reg loss)standard loss: 0.0046340, inverse loss: 0.0009673
Valid-->> Epoch [1230/3000], Standardized Loss: 0.0133013, Inverse Loss: 0.0027765
Training -->> Epoch: 1231,(no reg loss)standard loss: 0.0046277, inverse loss: 0.0009660
Valid-->> Epoch [1231/3000], Standardized Loss: 0.0133271, Inverse Loss: 0.0027819
Training -->> Epoch: 1232,(no reg loss)standard loss: 0.0046570, inverse loss: 0.0009721
Valid-->> Epoch [1232/3000], Standardized Loss: 0.0133330, Inverse Loss: 0.0027831
Training -->> Epoch: 1233,(no reg loss)standard loss: 0.0044758, inverse loss: 0.0009343
Valid-->> Epoch [1233/3000], Standardized Loss: 0.0133259, Inverse Loss: 0.0027816
Training -->> Epoch: 1234,(no reg loss)standard loss: 0.0047818, inverse loss: 0.0009982
Valid-->> Epoch [1234/3000], Standardized Loss: 0.0133149, Inverse Loss: 0.0027793
Training -->> Epoch: 1235,(no reg loss)standard loss: 0.0045439, inverse loss: 0.0009485
Valid-->> Epoch [1235/3000], Standardized Loss: 0.0133962, Inverse Loss: 0.0027963
Training -->> Epoch: 1236,(no reg loss)standard loss: 0.0047303, inverse loss: 0.0009874
Valid-->> Epoch [1236/3000], Standardized Loss: 0.0133058, Inverse Loss: 0.0027774
Training -->> Epoch: 1237,(no reg loss)standard loss: 0.0046256, inverse loss: 0.0009655
Valid-->> Epoch [1237/3000], Standardized Loss: 0.0132701, Inverse Loss: 0.0027700
Training -->> Epoch: 1238,(no reg loss)standard loss: 0.0045576, inverse loss: 0.0009514
Valid-->> Epoch [1238/3000], Standardized Loss: 0.0133173, Inverse Loss: 0.0027798
Training -->> Epoch: 1239,(no reg loss)standard loss: 0.0048461, inverse loss: 0.0010116
Valid-->> Epoch [1239/3000], Standardized Loss: 0.0132613, Inverse Loss: 0.0027681
Training -->> Epoch: 1240,(no reg loss)standard loss: 0.0045763, inverse loss: 0.0009552
Valid-->> Epoch [1240/3000], Standardized Loss: 0.0133781, Inverse Loss: 0.0027925
Training -->> Epoch: 1241,(no reg loss)standard loss: 0.0046810, inverse loss: 0.0009771
Valid-->> Epoch [1241/3000], Standardized Loss: 0.0132736, Inverse Loss: 0.0027707
Training -->> Epoch: 1242,(no reg loss)standard loss: 0.0046124, inverse loss: 0.0009628
Valid-->> Epoch [1242/3000], Standardized Loss: 0.0134136, Inverse Loss: 0.0027999
Training -->> Epoch: 1243,(no reg loss)standard loss: 0.0047729, inverse loss: 0.0009963
Valid-->> Epoch [1243/3000], Standardized Loss: 0.0133034, Inverse Loss: 0.0027769
Training -->> Epoch: 1244,(no reg loss)standard loss: 0.0045460, inverse loss: 0.0009489
Valid-->> Epoch [1244/3000], Standardized Loss: 0.0134621, Inverse Loss: 0.0028101
Training -->> Epoch: 1245,(no reg loss)standard loss: 0.0046143, inverse loss: 0.0009632
Valid-->> Epoch [1245/3000], Standardized Loss: 0.0132526, Inverse Loss: 0.0027663
Training -->> Epoch: 1246,(no reg loss)standard loss: 0.0046438, inverse loss: 0.0009693
Valid-->> Epoch [1246/3000], Standardized Loss: 0.0133506, Inverse Loss: 0.0027868
Training -->> Epoch: 1247,(no reg loss)standard loss: 0.0046195, inverse loss: 0.0009643
Valid-->> Epoch [1247/3000], Standardized Loss: 0.0134060, Inverse Loss: 0.0027984
Training -->> Epoch: 1248,(no reg loss)standard loss: 0.0046773, inverse loss: 0.0009763
Valid-->> Epoch [1248/3000], Standardized Loss: 0.0133872, Inverse Loss: 0.0027944
Training -->> Epoch: 1249,(no reg loss)standard loss: 0.0046905, inverse loss: 0.0009791
Valid-->> Epoch [1249/3000], Standardized Loss: 0.0132896, Inverse Loss: 0.0027740
Training -->> Epoch: 1250,(no reg loss)standard loss: 0.0047449, inverse loss: 0.0009904
Valid-->> Epoch [1250/3000], Standardized Loss: 0.0133346, Inverse Loss: 0.0027835
Training -->> Epoch: 1251,(no reg loss)standard loss: 0.0045901, inverse loss: 0.0009581
Valid-->> Epoch [1251/3000], Standardized Loss: 0.0132300, Inverse Loss: 0.0027616
Valid-->> Lowest loss found at epoch 1251, loss: 0.0027616
Epoch 1251, Masked params (inverse standardized): tensor([3.229419e+01, 4.430120e+01, 6.809616e-02, 2.636404e+01, 2.769801e+01,
        1.781268e+01, 2.143412e+01, 1.638666e+00, 1.066119e+02, 2.756407e+01,
        2.616311e+01, 4.461556e+01, 2.443797e+01, 1.503910e+01, 8.461577e+01,
        2.835971e+01, 5.666498e+00, 2.982381e+01, 1.196976e+01, 3.204187e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1252,(no reg loss)standard loss: 0.0046036, inverse loss: 0.0009609
Valid-->> Epoch [1252/3000], Standardized Loss: 0.0132993, Inverse Loss: 0.0027761
Training -->> Epoch: 1253,(no reg loss)standard loss: 0.0047469, inverse loss: 0.0009909
Valid-->> Epoch [1253/3000], Standardized Loss: 0.0132299, Inverse Loss: 0.0027616
Valid-->> Lowest loss found at epoch 1253, loss: 0.0027616
Epoch 1253, Masked params (inverse standardized): tensor([3.229454e+01, 4.430092e+01, 7.156754e-02, 2.636394e+01, 2.769785e+01,
        1.781446e+01, 2.143453e+01, 1.642159e+00, 1.066122e+02, 2.756397e+01,
        2.616294e+01, 4.461861e+01, 2.443743e+01, 1.504137e+01, 8.461526e+01,
        2.835929e+01, 5.670258e+00, 2.982333e+01, 1.197258e+01, 3.204210e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1254,(no reg loss)standard loss: 0.0045897, inverse loss: 0.0009581
Valid-->> Epoch [1254/3000], Standardized Loss: 0.0133933, Inverse Loss: 0.0027957
Training -->> Epoch: 1255,(no reg loss)standard loss: 0.0046344, inverse loss: 0.0009674
Valid-->> Epoch [1255/3000], Standardized Loss: 0.0132733, Inverse Loss: 0.0027706
Training -->> Epoch: 1256,(no reg loss)standard loss: 0.0045346, inverse loss: 0.0009465
Valid-->> Epoch [1256/3000], Standardized Loss: 0.0133244, Inverse Loss: 0.0027813
Training -->> Epoch: 1257,(no reg loss)standard loss: 0.0046728, inverse loss: 0.0009754
Valid-->> Epoch [1257/3000], Standardized Loss: 0.0133755, Inverse Loss: 0.0027920
Training -->> Epoch: 1258,(no reg loss)standard loss: 0.0045978, inverse loss: 0.0009597
Valid-->> Epoch [1258/3000], Standardized Loss: 0.0133299, Inverse Loss: 0.0027825
Training -->> Epoch: 1259,(no reg loss)standard loss: 0.0047169, inverse loss: 0.0009846
Valid-->> Epoch [1259/3000], Standardized Loss: 0.0132441, Inverse Loss: 0.0027645
Training -->> Epoch: 1260,(no reg loss)standard loss: 0.0046212, inverse loss: 0.0009646
Valid-->> Epoch [1260/3000], Standardized Loss: 0.0132396, Inverse Loss: 0.0027636
Training -->> Epoch: 1261,(no reg loss)standard loss: 0.0047755, inverse loss: 0.0009968
Valid-->> Epoch [1261/3000], Standardized Loss: 0.0133741, Inverse Loss: 0.0027917
Training -->> Epoch: 1262,(no reg loss)standard loss: 0.0044643, inverse loss: 0.0009319
Valid-->> Epoch [1262/3000], Standardized Loss: 0.0132401, Inverse Loss: 0.0027637
Training -->> Epoch: 1263,(no reg loss)standard loss: 0.0048431, inverse loss: 0.0010109
Valid-->> Epoch [1263/3000], Standardized Loss: 0.0132678, Inverse Loss: 0.0027695
Training -->> Epoch: 1264,(no reg loss)standard loss: 0.0045334, inverse loss: 0.0009463
Valid-->> Epoch [1264/3000], Standardized Loss: 0.0133143, Inverse Loss: 0.0027792
Training -->> Epoch: 1265,(no reg loss)standard loss: 0.0048003, inverse loss: 0.0010020
Valid-->> Epoch [1265/3000], Standardized Loss: 0.0132566, Inverse Loss: 0.0027672
Training -->> Epoch: 1266,(no reg loss)standard loss: 0.0045508, inverse loss: 0.0009499
Valid-->> Epoch [1266/3000], Standardized Loss: 0.0132779, Inverse Loss: 0.0027716
Training -->> Epoch: 1267,(no reg loss)standard loss: 0.0045546, inverse loss: 0.0009507
Valid-->> Epoch [1267/3000], Standardized Loss: 0.0133205, Inverse Loss: 0.0027805
Training -->> Epoch: 1268,(no reg loss)standard loss: 0.0046797, inverse loss: 0.0009768
Valid-->> Epoch [1268/3000], Standardized Loss: 0.0133431, Inverse Loss: 0.0027852
Training -->> Epoch: 1269,(no reg loss)standard loss: 0.0047232, inverse loss: 0.0009859
Valid-->> Epoch [1269/3000], Standardized Loss: 0.0133231, Inverse Loss: 0.0027810
Training -->> Epoch: 1270,(no reg loss)standard loss: 0.0045464, inverse loss: 0.0009490
Valid-->> Epoch [1270/3000], Standardized Loss: 0.0131990, Inverse Loss: 0.0027551
Valid-->> Lowest loss found at epoch 1270, loss: 0.0027551
Epoch 1270, Masked params (inverse standardized): tensor([3.229375e+01, 4.430164e+01, 6.788445e-02, 2.636512e+01, 2.769883e+01,
        1.781185e+01, 2.143372e+01, 1.638371e+00, 1.066122e+02, 2.756516e+01,
        2.616393e+01, 4.461498e+01, 2.443801e+01, 1.503832e+01, 8.461608e+01,
        2.835996e+01, 5.666395e+00, 2.982378e+01, 1.196912e+01, 3.204141e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1271,(no reg loss)standard loss: 0.0045926, inverse loss: 0.0009586
Valid-->> Epoch [1271/3000], Standardized Loss: 0.0133335, Inverse Loss: 0.0027832
Training -->> Epoch: 1272,(no reg loss)standard loss: 0.0047938, inverse loss: 0.0010006
Valid-->> Epoch [1272/3000], Standardized Loss: 0.0132881, Inverse Loss: 0.0027737
Training -->> Epoch: 1273,(no reg loss)standard loss: 0.0046006, inverse loss: 0.0009603
Valid-->> Epoch [1273/3000], Standardized Loss: 0.0133160, Inverse Loss: 0.0027796
Training -->> Epoch: 1274,(no reg loss)standard loss: 0.0046255, inverse loss: 0.0009655
Valid-->> Epoch [1274/3000], Standardized Loss: 0.0134275, Inverse Loss: 0.0028028
Training -->> Epoch: 1275,(no reg loss)standard loss: 0.0048175, inverse loss: 0.0010056
Valid-->> Epoch [1275/3000], Standardized Loss: 0.0132141, Inverse Loss: 0.0027583
Training -->> Epoch: 1276,(no reg loss)standard loss: 0.0046381, inverse loss: 0.0009682
Valid-->> Epoch [1276/3000], Standardized Loss: 0.0133584, Inverse Loss: 0.0027884
Training -->> Epoch: 1277,(no reg loss)standard loss: 0.0046992, inverse loss: 0.0009809
Valid-->> Epoch [1277/3000], Standardized Loss: 0.0132423, Inverse Loss: 0.0027642
Training -->> Epoch: 1278,(no reg loss)standard loss: 0.0046494, inverse loss: 0.0009705
Valid-->> Epoch [1278/3000], Standardized Loss: 0.0133432, Inverse Loss: 0.0027852
Training -->> Epoch: 1279,(no reg loss)standard loss: 0.0045008, inverse loss: 0.0009395
Valid-->> Epoch [1279/3000], Standardized Loss: 0.0132153, Inverse Loss: 0.0027585
Training -->> Epoch: 1280,(no reg loss)standard loss: 0.0047061, inverse loss: 0.0009823
Valid-->> Epoch [1280/3000], Standardized Loss: 0.0134249, Inverse Loss: 0.0028023
Training -->> Epoch: 1281,(no reg loss)standard loss: 0.0047651, inverse loss: 0.0009947
Valid-->> Epoch [1281/3000], Standardized Loss: 0.0132204, Inverse Loss: 0.0027596
Training -->> Epoch: 1282,(no reg loss)standard loss: 0.0045126, inverse loss: 0.0009419
Valid-->> Epoch [1282/3000], Standardized Loss: 0.0132465, Inverse Loss: 0.0027651
Training -->> Epoch: 1283,(no reg loss)standard loss: 0.0046293, inverse loss: 0.0009663
Valid-->> Epoch [1283/3000], Standardized Loss: 0.0132833, Inverse Loss: 0.0027727
Training -->> Epoch: 1284,(no reg loss)standard loss: 0.0047805, inverse loss: 0.0009979
Valid-->> Epoch [1284/3000], Standardized Loss: 0.0132202, Inverse Loss: 0.0027596
Training -->> Epoch: 1285,(no reg loss)standard loss: 0.0045787, inverse loss: 0.0009558
Valid-->> Epoch [1285/3000], Standardized Loss: 0.0132501, Inverse Loss: 0.0027658
Training -->> Epoch: 1286,(no reg loss)standard loss: 0.0046041, inverse loss: 0.0009610
Valid-->> Epoch [1286/3000], Standardized Loss: 0.0133123, Inverse Loss: 0.0027788
Training -->> Epoch: 1287,(no reg loss)standard loss: 0.0048032, inverse loss: 0.0010026
Valid-->> Epoch [1287/3000], Standardized Loss: 0.0133909, Inverse Loss: 0.0027952
Training -->> Epoch: 1288,(no reg loss)standard loss: 0.0046496, inverse loss: 0.0009706
Valid-->> Epoch [1288/3000], Standardized Loss: 0.0132032, Inverse Loss: 0.0027560
Training -->> Epoch: 1289,(no reg loss)standard loss: 0.0045495, inverse loss: 0.0009497
Valid-->> Epoch [1289/3000], Standardized Loss: 0.0133666, Inverse Loss: 0.0027901
Training -->> Epoch: 1290,(no reg loss)standard loss: 0.0048540, inverse loss: 0.0010132
Valid-->> Epoch [1290/3000], Standardized Loss: 0.0133266, Inverse Loss: 0.0027818
Training -->> Epoch: 1291,(no reg loss)standard loss: 0.0045451, inverse loss: 0.0009487
Valid-->> Epoch [1291/3000], Standardized Loss: 0.0132655, Inverse Loss: 0.0027690
Training -->> Epoch: 1292,(no reg loss)standard loss: 0.0047942, inverse loss: 0.0010007
Valid-->> Epoch [1292/3000], Standardized Loss: 0.0133469, Inverse Loss: 0.0027860
Training -->> Epoch: 1293,(no reg loss)standard loss: 0.0045765, inverse loss: 0.0009553
Valid-->> Epoch [1293/3000], Standardized Loss: 0.0132754, Inverse Loss: 0.0027711
Training -->> Epoch: 1294,(no reg loss)standard loss: 0.0048275, inverse loss: 0.0010077
Valid-->> Epoch [1294/3000], Standardized Loss: 0.0132284, Inverse Loss: 0.0027613
Training -->> Epoch: 1295,(no reg loss)standard loss: 0.0046614, inverse loss: 0.0009730
Valid-->> Epoch [1295/3000], Standardized Loss: 0.0132351, Inverse Loss: 0.0027627
Training -->> Epoch: 1296,(no reg loss)standard loss: 0.0046942, inverse loss: 0.0009798
Valid-->> Epoch [1296/3000], Standardized Loss: 0.0132242, Inverse Loss: 0.0027604
Training -->> Epoch: 1297,(no reg loss)standard loss: 0.0045245, inverse loss: 0.0009444
Valid-->> Epoch [1297/3000], Standardized Loss: 0.0132638, Inverse Loss: 0.0027687
Training -->> Epoch: 1298,(no reg loss)standard loss: 0.0047771, inverse loss: 0.0009972
Valid-->> Epoch [1298/3000], Standardized Loss: 0.0132176, Inverse Loss: 0.0027590
Training -->> Epoch: 1299,(no reg loss)standard loss: 0.0043985, inverse loss: 0.0009181
Valid-->> Epoch [1299/3000], Standardized Loss: 0.0131690, Inverse Loss: 0.0027489
Valid-->> Lowest loss found at epoch 1299, loss: 0.0027489
Epoch 1299, Masked params (inverse standardized): tensor([3.229197e+01, 4.430161e+01, 6.295967e-02, 2.636674e+01, 2.770016e+01,
        1.780877e+01, 2.143193e+01, 1.633301e+00, 1.066115e+02, 2.756682e+01,
        2.616526e+01, 4.460973e+01, 2.443745e+01, 1.503451e+01, 8.461554e+01,
        2.836033e+01, 5.661613e+00, 2.982283e+01, 1.196430e+01, 3.203979e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1300,(no reg loss)standard loss: 0.0046637, inverse loss: 0.0009735
Valid-->> Epoch [1300/3000], Standardized Loss: 0.0133781, Inverse Loss: 0.0027925
Training -->> Epoch: 1301,(no reg loss)standard loss: 0.0047180, inverse loss: 0.0009848
Valid-->> Epoch [1301/3000], Standardized Loss: 0.0132486, Inverse Loss: 0.0027655
Training -->> Epoch: 1302,(no reg loss)standard loss: 0.0047153, inverse loss: 0.0009843
Valid-->> Epoch [1302/3000], Standardized Loss: 0.0133048, Inverse Loss: 0.0027772
Training -->> Epoch: 1303,(no reg loss)standard loss: 0.0046997, inverse loss: 0.0009810
Valid-->> Epoch [1303/3000], Standardized Loss: 0.0132283, Inverse Loss: 0.0027613
Training -->> Epoch: 1304,(no reg loss)standard loss: 0.0046155, inverse loss: 0.0009634
Valid-->> Epoch [1304/3000], Standardized Loss: 0.0133596, Inverse Loss: 0.0027887
Training -->> Epoch: 1305,(no reg loss)standard loss: 0.0047630, inverse loss: 0.0009942
Valid-->> Epoch [1305/3000], Standardized Loss: 0.0131571, Inverse Loss: 0.0027464
Valid-->> Lowest loss found at epoch 1305, loss: 0.0027464
Epoch 1305, Masked params (inverse standardized): tensor([3.229530e+01, 4.430173e+01, 7.172585e-02, 2.636584e+01, 2.769937e+01,
        1.781515e+01, 2.143523e+01, 1.642780e+00, 1.066121e+02, 2.756590e+01,
        2.616450e+01, 4.461958e+01, 2.443841e+01, 1.504217e+01, 8.461612e+01,
        2.836038e+01, 5.670444e+00, 2.982428e+01, 1.197355e+01, 3.204284e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1306,(no reg loss)standard loss: 0.0045225, inverse loss: 0.0009440
Valid-->> Epoch [1306/3000], Standardized Loss: 0.0132290, Inverse Loss: 0.0027614
Training -->> Epoch: 1307,(no reg loss)standard loss: 0.0046166, inverse loss: 0.0009637
Valid-->> Epoch [1307/3000], Standardized Loss: 0.0132460, Inverse Loss: 0.0027649
Training -->> Epoch: 1308,(no reg loss)standard loss: 0.0047620, inverse loss: 0.0009940
Valid-->> Epoch [1308/3000], Standardized Loss: 0.0131848, Inverse Loss: 0.0027522
Training -->> Epoch: 1309,(no reg loss)standard loss: 0.0045715, inverse loss: 0.0009542
Valid-->> Epoch [1309/3000], Standardized Loss: 0.0132681, Inverse Loss: 0.0027696
Training -->> Epoch: 1310,(no reg loss)standard loss: 0.0046521, inverse loss: 0.0009711
Valid-->> Epoch [1310/3000], Standardized Loss: 0.0132736, Inverse Loss: 0.0027707
Training -->> Epoch: 1311,(no reg loss)standard loss: 0.0046489, inverse loss: 0.0009704
Valid-->> Epoch [1311/3000], Standardized Loss: 0.0132581, Inverse Loss: 0.0027675
Training -->> Epoch: 1312,(no reg loss)standard loss: 0.0047349, inverse loss: 0.0009884
Valid-->> Epoch [1312/3000], Standardized Loss: 0.0132207, Inverse Loss: 0.0027597
Training -->> Epoch: 1313,(no reg loss)standard loss: 0.0046097, inverse loss: 0.0009622
Valid-->> Epoch [1313/3000], Standardized Loss: 0.0131774, Inverse Loss: 0.0027506
Training -->> Epoch: 1314,(no reg loss)standard loss: 0.0046986, inverse loss: 0.0009808
Valid-->> Epoch [1314/3000], Standardized Loss: 0.0132656, Inverse Loss: 0.0027691
Training -->> Epoch: 1315,(no reg loss)standard loss: 0.0046370, inverse loss: 0.0009679
Valid-->> Epoch [1315/3000], Standardized Loss: 0.0132584, Inverse Loss: 0.0027675
Training -->> Epoch: 1316,(no reg loss)standard loss: 0.0047028, inverse loss: 0.0009817
Valid-->> Epoch [1316/3000], Standardized Loss: 0.0131831, Inverse Loss: 0.0027518
Training -->> Epoch: 1317,(no reg loss)standard loss: 0.0044818, inverse loss: 0.0009355
Valid-->> Epoch [1317/3000], Standardized Loss: 0.0131989, Inverse Loss: 0.0027551
Training -->> Epoch: 1318,(no reg loss)standard loss: 0.0046854, inverse loss: 0.0009780
Valid-->> Epoch [1318/3000], Standardized Loss: 0.0132985, Inverse Loss: 0.0027759
Training -->> Epoch: 1319,(no reg loss)standard loss: 0.0046680, inverse loss: 0.0009744
Valid-->> Epoch [1319/3000], Standardized Loss: 0.0132668, Inverse Loss: 0.0027693
Training -->> Epoch: 1320,(no reg loss)standard loss: 0.0047161, inverse loss: 0.0009844
Valid-->> Epoch [1320/3000], Standardized Loss: 0.0132866, Inverse Loss: 0.0027734
Training -->> Epoch: 1321,(no reg loss)standard loss: 0.0047437, inverse loss: 0.0009902
Valid-->> Epoch [1321/3000], Standardized Loss: 0.0132317, Inverse Loss: 0.0027620
Training -->> Epoch: 1322,(no reg loss)standard loss: 0.0045852, inverse loss: 0.0009571
Valid-->> Epoch [1322/3000], Standardized Loss: 0.0131568, Inverse Loss: 0.0027463
Valid-->> Lowest loss found at epoch 1322, loss: 0.0027463
Epoch 1322, Masked params (inverse standardized): tensor([3.229377e+01, 4.430154e+01, 6.833649e-02, 2.636644e+01, 2.769985e+01,
        1.781205e+01, 2.143370e+01, 1.638895e+00, 1.066123e+02, 2.756650e+01,
        2.616496e+01, 4.461533e+01, 2.443827e+01, 1.503844e+01, 8.461604e+01,
        2.836049e+01, 5.666920e+00, 2.982389e+01, 1.196939e+01, 3.204150e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1323,(no reg loss)standard loss: 0.0045645, inverse loss: 0.0009528
Valid-->> Epoch [1323/3000], Standardized Loss: 0.0133054, Inverse Loss: 0.0027774
Training -->> Epoch: 1324,(no reg loss)standard loss: 0.0049050, inverse loss: 0.0010239
Valid-->> Epoch [1324/3000], Standardized Loss: 0.0132104, Inverse Loss: 0.0027575
Training -->> Epoch: 1325,(no reg loss)standard loss: 0.0046164, inverse loss: 0.0009636
Valid-->> Epoch [1325/3000], Standardized Loss: 0.0131984, Inverse Loss: 0.0027550
Training -->> Epoch: 1326,(no reg loss)standard loss: 0.0047052, inverse loss: 0.0009822
Valid-->> Epoch [1326/3000], Standardized Loss: 0.0132694, Inverse Loss: 0.0027698
Training -->> Epoch: 1327,(no reg loss)standard loss: 0.0047337, inverse loss: 0.0009881
Valid-->> Epoch [1327/3000], Standardized Loss: 0.0132221, Inverse Loss: 0.0027600
Training -->> Epoch: 1328,(no reg loss)standard loss: 0.0047663, inverse loss: 0.0009949
Valid-->> Epoch [1328/3000], Standardized Loss: 0.0131667, Inverse Loss: 0.0027484
Training -->> Epoch: 1329,(no reg loss)standard loss: 0.0046103, inverse loss: 0.0009623
Valid-->> Epoch [1329/3000], Standardized Loss: 0.0131599, Inverse Loss: 0.0027470
Training -->> Epoch: 1330,(no reg loss)standard loss: 0.0047772, inverse loss: 0.0009972
Valid-->> Epoch [1330/3000], Standardized Loss: 0.0133177, Inverse Loss: 0.0027799
Training -->> Epoch: 1331,(no reg loss)standard loss: 0.0047645, inverse loss: 0.0009945
Valid-->> Epoch [1331/3000], Standardized Loss: 0.0132213, Inverse Loss: 0.0027598
Training -->> Epoch: 1332,(no reg loss)standard loss: 0.0047052, inverse loss: 0.0009821
Valid-->> Epoch [1332/3000], Standardized Loss: 0.0132866, Inverse Loss: 0.0027734
Training -->> Epoch: 1333,(no reg loss)standard loss: 0.0047160, inverse loss: 0.0009844
Valid-->> Epoch [1333/3000], Standardized Loss: 0.0131263, Inverse Loss: 0.0027400
Valid-->> Lowest loss found at epoch 1333, loss: 0.0027400
Epoch 1333, Masked params (inverse standardized): tensor([3.229505e+01, 4.430113e+01, 7.046509e-02, 2.636676e+01, 2.770026e+01,
        1.781430e+01, 2.143504e+01, 1.641195e+00, 1.066120e+02, 2.756682e+01,
        2.616540e+01, 4.461801e+01, 2.443898e+01, 1.504112e+01, 8.461571e+01,
        2.836111e+01, 5.669064e+00, 2.982473e+01, 1.197208e+01, 3.204272e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1334,(no reg loss)standard loss: 0.0046597, inverse loss: 0.0009726
Valid-->> Epoch [1334/3000], Standardized Loss: 0.0132826, Inverse Loss: 0.0027726
Training -->> Epoch: 1335,(no reg loss)standard loss: 0.0044970, inverse loss: 0.0009387
Valid-->> Epoch [1335/3000], Standardized Loss: 0.0131494, Inverse Loss: 0.0027448
Training -->> Epoch: 1336,(no reg loss)standard loss: 0.0047798, inverse loss: 0.0009977
Valid-->> Epoch [1336/3000], Standardized Loss: 0.0133280, Inverse Loss: 0.0027821
Training -->> Epoch: 1337,(no reg loss)standard loss: 0.0046730, inverse loss: 0.0009754
Valid-->> Epoch [1337/3000], Standardized Loss: 0.0132453, Inverse Loss: 0.0027648
Training -->> Epoch: 1338,(no reg loss)standard loss: 0.0049759, inverse loss: 0.0010387
Valid-->> Epoch [1338/3000], Standardized Loss: 0.0132949, Inverse Loss: 0.0027752
Training -->> Epoch: 1339,(no reg loss)standard loss: 0.0045988, inverse loss: 0.0009599
Valid-->> Epoch [1339/3000], Standardized Loss: 0.0130961, Inverse Loss: 0.0027337
Valid-->> Lowest loss found at epoch 1339, loss: 0.0027337
Epoch 1339, Masked params (inverse standardized): tensor([3.229559e+01, 4.429949e+01, 6.617737e-02, 2.636759e+01, 2.770107e+01,
        1.781325e+01, 2.143556e+01, 1.636702e+00, 1.066122e+02, 2.756773e+01,
        2.616624e+01, 4.461411e+01, 2.443999e+01, 1.503915e+01, 8.461411e+01,
        2.836204e+01, 5.664970e+00, 2.982573e+01, 1.196881e+01, 3.204325e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1340,(no reg loss)standard loss: 0.0045826, inverse loss: 0.0009566
Valid-->> Epoch [1340/3000], Standardized Loss: 0.0134402, Inverse Loss: 0.0028055
Training -->> Epoch: 1341,(no reg loss)standard loss: 0.0049958, inverse loss: 0.0010428
Valid-->> Epoch [1341/3000], Standardized Loss: 0.0130996, Inverse Loss: 0.0027344
Training -->> Epoch: 1342,(no reg loss)standard loss: 0.0044966, inverse loss: 0.0009386
Valid-->> Epoch [1342/3000], Standardized Loss: 0.0133384, Inverse Loss: 0.0027842
Training -->> Epoch: 1343,(no reg loss)standard loss: 0.0050275, inverse loss: 0.0010494
Valid-->> Epoch [1343/3000], Standardized Loss: 0.0132242, Inverse Loss: 0.0027604
Training -->> Epoch: 1344,(no reg loss)standard loss: 0.0047041, inverse loss: 0.0009819
Valid-->> Epoch [1344/3000], Standardized Loss: 0.0131633, Inverse Loss: 0.0027477
Training -->> Epoch: 1345,(no reg loss)standard loss: 0.0046479, inverse loss: 0.0009702
Valid-->> Epoch [1345/3000], Standardized Loss: 0.0131885, Inverse Loss: 0.0027529
Training -->> Epoch: 1346,(no reg loss)standard loss: 0.0046185, inverse loss: 0.0009641
Valid-->> Epoch [1346/3000], Standardized Loss: 0.0131452, Inverse Loss: 0.0027439
Training -->> Epoch: 1347,(no reg loss)standard loss: 0.0046673, inverse loss: 0.0009742
Valid-->> Epoch [1347/3000], Standardized Loss: 0.0132801, Inverse Loss: 0.0027721
Training -->> Epoch: 1348,(no reg loss)standard loss: 0.0048526, inverse loss: 0.0010129
Valid-->> Epoch [1348/3000], Standardized Loss: 0.0131987, Inverse Loss: 0.0027551
Training -->> Epoch: 1349,(no reg loss)standard loss: 0.0046519, inverse loss: 0.0009710
Valid-->> Epoch [1349/3000], Standardized Loss: 0.0131542, Inverse Loss: 0.0027458
Training -->> Epoch: 1350,(no reg loss)standard loss: 0.0046840, inverse loss: 0.0009777
Valid-->> Epoch [1350/3000], Standardized Loss: 0.0132876, Inverse Loss: 0.0027736
Training -->> Epoch: 1351,(no reg loss)standard loss: 0.0047028, inverse loss: 0.0009817
Valid-->> Epoch [1351/3000], Standardized Loss: 0.0131022, Inverse Loss: 0.0027349
Training -->> Epoch: 1352,(no reg loss)standard loss: 0.0046305, inverse loss: 0.0009666
Valid-->> Epoch [1352/3000], Standardized Loss: 0.0132622, Inverse Loss: 0.0027683
Training -->> Epoch: 1353,(no reg loss)standard loss: 0.0048524, inverse loss: 0.0010129
Valid-->> Epoch [1353/3000], Standardized Loss: 0.0131815, Inverse Loss: 0.0027515
Training -->> Epoch: 1354,(no reg loss)standard loss: 0.0046578, inverse loss: 0.0009723
Valid-->> Epoch [1354/3000], Standardized Loss: 0.0130698, Inverse Loss: 0.0027282
Valid-->> Lowest loss found at epoch 1354, loss: 0.0027282
Epoch 1354, Masked params (inverse standardized): tensor([3.229595e+01, 4.430006e+01, 6.958961e-02, 2.636833e+01, 2.770165e+01,
        1.781406e+01, 2.143589e+01, 1.639999e+00, 1.066116e+02, 2.756842e+01,
        2.616678e+01, 4.461652e+01, 2.443989e+01, 1.504029e+01, 8.461475e+01,
        2.836208e+01, 5.668280e+00, 2.982566e+01, 1.197070e+01, 3.204364e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1355,(no reg loss)standard loss: 0.0046499, inverse loss: 0.0009706
Valid-->> Epoch [1355/3000], Standardized Loss: 0.0131880, Inverse Loss: 0.0027528
Training -->> Epoch: 1356,(no reg loss)standard loss: 0.0046868, inverse loss: 0.0009783
Valid-->> Epoch [1356/3000], Standardized Loss: 0.0132073, Inverse Loss: 0.0027569
Training -->> Epoch: 1357,(no reg loss)standard loss: 0.0047630, inverse loss: 0.0009942
Valid-->> Epoch [1357/3000], Standardized Loss: 0.0131653, Inverse Loss: 0.0027481
Training -->> Epoch: 1358,(no reg loss)standard loss: 0.0045795, inverse loss: 0.0009559
Valid-->> Epoch [1358/3000], Standardized Loss: 0.0132028, Inverse Loss: 0.0027559
Training -->> Epoch: 1359,(no reg loss)standard loss: 0.0048153, inverse loss: 0.0010051
Valid-->> Epoch [1359/3000], Standardized Loss: 0.0131451, Inverse Loss: 0.0027439
Training -->> Epoch: 1360,(no reg loss)standard loss: 0.0046405, inverse loss: 0.0009686
Valid-->> Epoch [1360/3000], Standardized Loss: 0.0132220, Inverse Loss: 0.0027599
Training -->> Epoch: 1361,(no reg loss)standard loss: 0.0048360, inverse loss: 0.0010095
Valid-->> Epoch [1361/3000], Standardized Loss: 0.0131454, Inverse Loss: 0.0027439
Training -->> Epoch: 1362,(no reg loss)standard loss: 0.0045784, inverse loss: 0.0009557
Valid-->> Epoch [1362/3000], Standardized Loss: 0.0132113, Inverse Loss: 0.0027577
Training -->> Epoch: 1363,(no reg loss)standard loss: 0.0048464, inverse loss: 0.0010116
Valid-->> Epoch [1363/3000], Standardized Loss: 0.0131568, Inverse Loss: 0.0027463
Training -->> Epoch: 1364,(no reg loss)standard loss: 0.0045513, inverse loss: 0.0009500
Valid-->> Epoch [1364/3000], Standardized Loss: 0.0131618, Inverse Loss: 0.0027474
Training -->> Epoch: 1365,(no reg loss)standard loss: 0.0046633, inverse loss: 0.0009734
Valid-->> Epoch [1365/3000], Standardized Loss: 0.0132018, Inverse Loss: 0.0027557
Training -->> Epoch: 1366,(no reg loss)standard loss: 0.0047851, inverse loss: 0.0009988
Valid-->> Epoch [1366/3000], Standardized Loss: 0.0131572, Inverse Loss: 0.0027464
Training -->> Epoch: 1367,(no reg loss)standard loss: 0.0045923, inverse loss: 0.0009586
Valid-->> Epoch [1367/3000], Standardized Loss: 0.0132026, Inverse Loss: 0.0027559
Training -->> Epoch: 1368,(no reg loss)standard loss: 0.0047769, inverse loss: 0.0009971
Valid-->> Epoch [1368/3000], Standardized Loss: 0.0131496, Inverse Loss: 0.0027448
Training -->> Epoch: 1369,(no reg loss)standard loss: 0.0047377, inverse loss: 0.0009889
Valid-->> Epoch [1369/3000], Standardized Loss: 0.0131530, Inverse Loss: 0.0027455
Training -->> Epoch: 1370,(no reg loss)standard loss: 0.0046722, inverse loss: 0.0009753
Valid-->> Epoch [1370/3000], Standardized Loss: 0.0133085, Inverse Loss: 0.0027780
Training -->> Epoch: 1371,(no reg loss)standard loss: 0.0048507, inverse loss: 0.0010125
Valid-->> Epoch [1371/3000], Standardized Loss: 0.0133087, Inverse Loss: 0.0027780
Training -->> Epoch: 1372,(no reg loss)standard loss: 0.0047050, inverse loss: 0.0009821
Valid-->> Epoch [1372/3000], Standardized Loss: 0.0131223, Inverse Loss: 0.0027391
Training -->> Epoch: 1373,(no reg loss)standard loss: 0.0047241, inverse loss: 0.0009861
Valid-->> Epoch [1373/3000], Standardized Loss: 0.0132842, Inverse Loss: 0.0027729
Training -->> Epoch: 1374,(no reg loss)standard loss: 0.0047369, inverse loss: 0.0009888
Valid-->> Epoch [1374/3000], Standardized Loss: 0.0131622, Inverse Loss: 0.0027475
Training -->> Epoch: 1375,(no reg loss)standard loss: 0.0045650, inverse loss: 0.0009529
Valid-->> Epoch [1375/3000], Standardized Loss: 0.0132131, Inverse Loss: 0.0027581
Training -->> Epoch: 1376,(no reg loss)standard loss: 0.0047963, inverse loss: 0.0010012
Valid-->> Epoch [1376/3000], Standardized Loss: 0.0132195, Inverse Loss: 0.0027594
Training -->> Epoch: 1377,(no reg loss)standard loss: 0.0047208, inverse loss: 0.0009854
Valid-->> Epoch [1377/3000], Standardized Loss: 0.0131321, Inverse Loss: 0.0027412
Training -->> Epoch: 1378,(no reg loss)standard loss: 0.0047159, inverse loss: 0.0009844
Valid-->> Epoch [1378/3000], Standardized Loss: 0.0131011, Inverse Loss: 0.0027347
Training -->> Epoch: 1379,(no reg loss)standard loss: 0.0045943, inverse loss: 0.0009590
Valid-->> Epoch [1379/3000], Standardized Loss: 0.0131498, Inverse Loss: 0.0027449
Training -->> Epoch: 1380,(no reg loss)standard loss: 0.0046940, inverse loss: 0.0009798
Valid-->> Epoch [1380/3000], Standardized Loss: 0.0132335, Inverse Loss: 0.0027624
Training -->> Epoch: 1381,(no reg loss)standard loss: 0.0046961, inverse loss: 0.0009803
Valid-->> Epoch [1381/3000], Standardized Loss: 0.0131757, Inverse Loss: 0.0027503
Training -->> Epoch: 1382,(no reg loss)standard loss: 0.0047914, inverse loss: 0.0010001
Valid-->> Epoch [1382/3000], Standardized Loss: 0.0131580, Inverse Loss: 0.0027466
Training -->> Epoch: 1383,(no reg loss)standard loss: 0.0047500, inverse loss: 0.0009915
Valid-->> Epoch [1383/3000], Standardized Loss: 0.0132095, Inverse Loss: 0.0027573
Training -->> Epoch: 1384,(no reg loss)standard loss: 0.0046932, inverse loss: 0.0009797
Valid-->> Epoch [1384/3000], Standardized Loss: 0.0131965, Inverse Loss: 0.0027546
Training -->> Epoch: 1385,(no reg loss)standard loss: 0.0048850, inverse loss: 0.0010197
Valid-->> Epoch [1385/3000], Standardized Loss: 0.0130778, Inverse Loss: 0.0027298
Training -->> Epoch: 1386,(no reg loss)standard loss: 0.0045273, inverse loss: 0.0009450
Valid-->> Epoch [1386/3000], Standardized Loss: 0.0131406, Inverse Loss: 0.0027430
Training -->> Epoch: 1387,(no reg loss)standard loss: 0.0046829, inverse loss: 0.0009775
Valid-->> Epoch [1387/3000], Standardized Loss: 0.0131040, Inverse Loss: 0.0027353
Training -->> Epoch: 1388,(no reg loss)standard loss: 0.0047806, inverse loss: 0.0009979
Valid-->> Epoch [1388/3000], Standardized Loss: 0.0131874, Inverse Loss: 0.0027527
Training -->> Epoch: 1389,(no reg loss)standard loss: 0.0047277, inverse loss: 0.0009868
Valid-->> Epoch [1389/3000], Standardized Loss: 0.0131366, Inverse Loss: 0.0027421
Training -->> Epoch: 1390,(no reg loss)standard loss: 0.0046390, inverse loss: 0.0009683
Valid-->> Epoch [1390/3000], Standardized Loss: 0.0132590, Inverse Loss: 0.0027677
Training -->> Epoch: 1391,(no reg loss)standard loss: 0.0049098, inverse loss: 0.0010249
Valid-->> Epoch [1391/3000], Standardized Loss: 0.0130609, Inverse Loss: 0.0027263
Valid-->> Lowest loss found at epoch 1391, loss: 0.0027263
Epoch 1391, Masked params (inverse standardized): tensor([3.229618e+01, 4.430093e+01, 7.322121e-02, 2.636861e+01, 2.770164e+01,
        1.781645e+01, 2.143617e+01, 1.643913e+00, 1.066127e+02, 2.756871e+01,
        2.616676e+01, 4.462093e+01, 2.443936e+01, 1.504346e+01, 8.461551e+01,
        2.836164e+01, 5.671862e+00, 2.982515e+01, 1.197489e+01, 3.204371e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1392,(no reg loss)standard loss: 0.0045740, inverse loss: 0.0009548
Valid-->> Epoch [1392/3000], Standardized Loss: 0.0132633, Inverse Loss: 0.0027686
Training -->> Epoch: 1393,(no reg loss)standard loss: 0.0048598, inverse loss: 0.0010144
Valid-->> Epoch [1393/3000], Standardized Loss: 0.0131044, Inverse Loss: 0.0027354
Training -->> Epoch: 1394,(no reg loss)standard loss: 0.0046821, inverse loss: 0.0009773
Valid-->> Epoch [1394/3000], Standardized Loss: 0.0131185, Inverse Loss: 0.0027383
Training -->> Epoch: 1395,(no reg loss)standard loss: 0.0046185, inverse loss: 0.0009640
Valid-->> Epoch [1395/3000], Standardized Loss: 0.0131616, Inverse Loss: 0.0027473
Training -->> Epoch: 1396,(no reg loss)standard loss: 0.0046978, inverse loss: 0.0009806
Valid-->> Epoch [1396/3000], Standardized Loss: 0.0132491, Inverse Loss: 0.0027656
Training -->> Epoch: 1397,(no reg loss)standard loss: 0.0046535, inverse loss: 0.0009714
Valid-->> Epoch [1397/3000], Standardized Loss: 0.0130931, Inverse Loss: 0.0027330
Training -->> Epoch: 1398,(no reg loss)standard loss: 0.0047327, inverse loss: 0.0009879
Valid-->> Epoch [1398/3000], Standardized Loss: 0.0131666, Inverse Loss: 0.0027484
Training -->> Epoch: 1399,(no reg loss)standard loss: 0.0047256, inverse loss: 0.0009864
Valid-->> Epoch [1399/3000], Standardized Loss: 0.0130855, Inverse Loss: 0.0027314
Training -->> Epoch: 1400,(no reg loss)standard loss: 0.0047471, inverse loss: 0.0009909
Valid-->> Epoch [1400/3000], Standardized Loss: 0.0131683, Inverse Loss: 0.0027487
Training -->> Epoch: 1401,(no reg loss)standard loss: 0.0047652, inverse loss: 0.0009947
Valid-->> Epoch [1401/3000], Standardized Loss: 0.0131018, Inverse Loss: 0.0027349
Training -->> Epoch: 1402,(no reg loss)standard loss: 0.0046350, inverse loss: 0.0009675
Valid-->> Epoch [1402/3000], Standardized Loss: 0.0130612, Inverse Loss: 0.0027264
Training -->> Epoch: 1403,(no reg loss)standard loss: 0.0046339, inverse loss: 0.0009673
Valid-->> Epoch [1403/3000], Standardized Loss: 0.0131942, Inverse Loss: 0.0027541
Training -->> Epoch: 1404,(no reg loss)standard loss: 0.0048742, inverse loss: 0.0010174
Valid-->> Epoch [1404/3000], Standardized Loss: 0.0130689, Inverse Loss: 0.0027280
Training -->> Epoch: 1405,(no reg loss)standard loss: 0.0047389, inverse loss: 0.0009892
Valid-->> Epoch [1405/3000], Standardized Loss: 0.0131153, Inverse Loss: 0.0027377
Training -->> Epoch: 1406,(no reg loss)standard loss: 0.0048235, inverse loss: 0.0010068
Valid-->> Epoch [1406/3000], Standardized Loss: 0.0130373, Inverse Loss: 0.0027214
Valid-->> Lowest loss found at epoch 1406, loss: 0.0027214
Epoch 1406, Masked params (inverse standardized): tensor([3.229578e+01, 4.430140e+01, 7.235909e-02, 2.636950e+01, 2.770237e+01,
        1.781511e+01, 2.143575e+01, 1.642670e+00, 1.066134e+02, 2.756951e+01,
        2.616749e+01, 4.461947e+01, 2.443976e+01, 1.504208e+01, 8.461618e+01,
        2.836208e+01, 5.670536e+00, 2.982551e+01, 1.197356e+01, 3.204342e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1407,(no reg loss)standard loss: 0.0046967, inverse loss: 0.0009804
Valid-->> Epoch [1407/3000], Standardized Loss: 0.0131448, Inverse Loss: 0.0027438
Training -->> Epoch: 1408,(no reg loss)standard loss: 0.0047916, inverse loss: 0.0010002
Valid-->> Epoch [1408/3000], Standardized Loss: 0.0132052, Inverse Loss: 0.0027564
Training -->> Epoch: 1409,(no reg loss)standard loss: 0.0046158, inverse loss: 0.0009635
Valid-->> Epoch [1409/3000], Standardized Loss: 0.0131538, Inverse Loss: 0.0027457
Training -->> Epoch: 1410,(no reg loss)standard loss: 0.0048231, inverse loss: 0.0010068
Valid-->> Epoch [1410/3000], Standardized Loss: 0.0131168, Inverse Loss: 0.0027380
Training -->> Epoch: 1411,(no reg loss)standard loss: 0.0047634, inverse loss: 0.0009943
Valid-->> Epoch [1411/3000], Standardized Loss: 0.0131017, Inverse Loss: 0.0027348
Training -->> Epoch: 1412,(no reg loss)standard loss: 0.0046773, inverse loss: 0.0009763
Valid-->> Epoch [1412/3000], Standardized Loss: 0.0131468, Inverse Loss: 0.0027442
Training -->> Epoch: 1413,(no reg loss)standard loss: 0.0046832, inverse loss: 0.0009776
Valid-->> Epoch [1413/3000], Standardized Loss: 0.0131312, Inverse Loss: 0.0027410
Training -->> Epoch: 1414,(no reg loss)standard loss: 0.0047914, inverse loss: 0.0010001
Valid-->> Epoch [1414/3000], Standardized Loss: 0.0132830, Inverse Loss: 0.0027727
Training -->> Epoch: 1415,(no reg loss)standard loss: 0.0048283, inverse loss: 0.0010078
Valid-->> Epoch [1415/3000], Standardized Loss: 0.0130544, Inverse Loss: 0.0027250
Training -->> Epoch: 1416,(no reg loss)standard loss: 0.0046323, inverse loss: 0.0009669
Valid-->> Epoch [1416/3000], Standardized Loss: 0.0131529, Inverse Loss: 0.0027455
Training -->> Epoch: 1417,(no reg loss)standard loss: 0.0047129, inverse loss: 0.0009838
Valid-->> Epoch [1417/3000], Standardized Loss: 0.0131596, Inverse Loss: 0.0027469
Training -->> Epoch: 1418,(no reg loss)standard loss: 0.0047417, inverse loss: 0.0009898
Valid-->> Epoch [1418/3000], Standardized Loss: 0.0130918, Inverse Loss: 0.0027328
Training -->> Epoch: 1419,(no reg loss)standard loss: 0.0046161, inverse loss: 0.0009635
Valid-->> Epoch [1419/3000], Standardized Loss: 0.0131714, Inverse Loss: 0.0027494
Training -->> Epoch: 1420,(no reg loss)standard loss: 0.0048455, inverse loss: 0.0010114
Valid-->> Epoch [1420/3000], Standardized Loss: 0.0130762, Inverse Loss: 0.0027295
Training -->> Epoch: 1421,(no reg loss)standard loss: 0.0047329, inverse loss: 0.0009879
Valid-->> Epoch [1421/3000], Standardized Loss: 0.0131286, Inverse Loss: 0.0027404
Training -->> Epoch: 1422,(no reg loss)standard loss: 0.0047426, inverse loss: 0.0009900
Valid-->> Epoch [1422/3000], Standardized Loss: 0.0131620, Inverse Loss: 0.0027474
Training -->> Epoch: 1423,(no reg loss)standard loss: 0.0048138, inverse loss: 0.0010048
Valid-->> Epoch [1423/3000], Standardized Loss: 0.0130860, Inverse Loss: 0.0027315
Training -->> Epoch: 1424,(no reg loss)standard loss: 0.0046208, inverse loss: 0.0009645
Valid-->> Epoch [1424/3000], Standardized Loss: 0.0130686, Inverse Loss: 0.0027279
Training -->> Epoch: 1425,(no reg loss)standard loss: 0.0047374, inverse loss: 0.0009889
Valid-->> Epoch [1425/3000], Standardized Loss: 0.0131340, Inverse Loss: 0.0027416
Training -->> Epoch: 1426,(no reg loss)standard loss: 0.0047379, inverse loss: 0.0009890
Valid-->> Epoch [1426/3000], Standardized Loss: 0.0132186, Inverse Loss: 0.0027592
Training -->> Epoch: 1427,(no reg loss)standard loss: 0.0049108, inverse loss: 0.0010251
Valid-->> Epoch [1427/3000], Standardized Loss: 0.0131508, Inverse Loss: 0.0027451
Training -->> Epoch: 1428,(no reg loss)standard loss: 0.0046342, inverse loss: 0.0009673
Valid-->> Epoch [1428/3000], Standardized Loss: 0.0130983, Inverse Loss: 0.0027341
Training -->> Epoch: 1429,(no reg loss)standard loss: 0.0046612, inverse loss: 0.0009730
Valid-->> Epoch [1429/3000], Standardized Loss: 0.0130913, Inverse Loss: 0.0027327
Training -->> Epoch: 1430,(no reg loss)standard loss: 0.0047796, inverse loss: 0.0009977
Valid-->> Epoch [1430/3000], Standardized Loss: 0.0132153, Inverse Loss: 0.0027585
Training -->> Epoch: 1431,(no reg loss)standard loss: 0.0046544, inverse loss: 0.0009716
Valid-->> Epoch [1431/3000], Standardized Loss: 0.0130978, Inverse Loss: 0.0027340
Training -->> Epoch: 1432,(no reg loss)standard loss: 0.0046379, inverse loss: 0.0009681
Valid-->> Epoch [1432/3000], Standardized Loss: 0.0132245, Inverse Loss: 0.0027605
Training -->> Epoch: 1433,(no reg loss)standard loss: 0.0048358, inverse loss: 0.0010094
Valid-->> Epoch [1433/3000], Standardized Loss: 0.0131278, Inverse Loss: 0.0027403
Training -->> Epoch: 1434,(no reg loss)standard loss: 0.0047719, inverse loss: 0.0009961
Valid-->> Epoch [1434/3000], Standardized Loss: 0.0131989, Inverse Loss: 0.0027551
Training -->> Epoch: 1435,(no reg loss)standard loss: 0.0047175, inverse loss: 0.0009847
Valid-->> Epoch [1435/3000], Standardized Loss: 0.0131239, Inverse Loss: 0.0027395
Training -->> Epoch: 1436,(no reg loss)standard loss: 0.0045490, inverse loss: 0.0009495
Valid-->> Epoch [1436/3000], Standardized Loss: 0.0130803, Inverse Loss: 0.0027304
Training -->> Epoch: 1437,(no reg loss)standard loss: 0.0047728, inverse loss: 0.0009963
Valid-->> Epoch [1437/3000], Standardized Loss: 0.0131629, Inverse Loss: 0.0027476
Training -->> Epoch: 1438,(no reg loss)standard loss: 0.0047929, inverse loss: 0.0010005
Valid-->> Epoch [1438/3000], Standardized Loss: 0.0130879, Inverse Loss: 0.0027320
Training -->> Epoch: 1439,(no reg loss)standard loss: 0.0048277, inverse loss: 0.0010077
Valid-->> Epoch [1439/3000], Standardized Loss: 0.0132808, Inverse Loss: 0.0027722
Training -->> Epoch: 1440,(no reg loss)standard loss: 0.0048027, inverse loss: 0.0010025
Valid-->> Epoch [1440/3000], Standardized Loss: 0.0129525, Inverse Loss: 0.0027037
Valid-->> Lowest loss found at epoch 1440, loss: 0.0027037
Epoch 1440, Masked params (inverse standardized): tensor([3.229757e+01, 4.430009e+01, 7.044983e-02, 2.637177e+01, 2.770442e+01,
        1.781651e+01, 2.143750e+01, 1.641279e+00, 1.066119e+02, 2.757182e+01,
        2.616959e+01, 4.461896e+01, 2.444148e+01, 1.504305e+01, 8.461513e+01,
        2.836383e+01, 5.668926e+00, 2.982722e+01, 1.197349e+01, 3.204519e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1441,(no reg loss)standard loss: 0.0046670, inverse loss: 0.0009742
Valid-->> Epoch [1441/3000], Standardized Loss: 0.0131051, Inverse Loss: 0.0027355
Training -->> Epoch: 1442,(no reg loss)standard loss: 0.0049030, inverse loss: 0.0010234
Valid-->> Epoch [1442/3000], Standardized Loss: 0.0130965, Inverse Loss: 0.0027337
Training -->> Epoch: 1443,(no reg loss)standard loss: 0.0047110, inverse loss: 0.0009834
Valid-->> Epoch [1443/3000], Standardized Loss: 0.0130535, Inverse Loss: 0.0027248
Training -->> Epoch: 1444,(no reg loss)standard loss: 0.0045028, inverse loss: 0.0009399
Valid-->> Epoch [1444/3000], Standardized Loss: 0.0131042, Inverse Loss: 0.0027354
Training -->> Epoch: 1445,(no reg loss)standard loss: 0.0049332, inverse loss: 0.0010297
Valid-->> Epoch [1445/3000], Standardized Loss: 0.0131684, Inverse Loss: 0.0027488
Training -->> Epoch: 1446,(no reg loss)standard loss: 0.0047374, inverse loss: 0.0009889
Valid-->> Epoch [1446/3000], Standardized Loss: 0.0130571, Inverse Loss: 0.0027255
Training -->> Epoch: 1447,(no reg loss)standard loss: 0.0048269, inverse loss: 0.0010076
Valid-->> Epoch [1447/3000], Standardized Loss: 0.0131576, Inverse Loss: 0.0027465
Training -->> Epoch: 1448,(no reg loss)standard loss: 0.0047167, inverse loss: 0.0009846
Valid-->> Epoch [1448/3000], Standardized Loss: 0.0131553, Inverse Loss: 0.0027460
Training -->> Epoch: 1449,(no reg loss)standard loss: 0.0048235, inverse loss: 0.0010068
Valid-->> Epoch [1449/3000], Standardized Loss: 0.0130542, Inverse Loss: 0.0027249
Training -->> Epoch: 1450,(no reg loss)standard loss: 0.0045609, inverse loss: 0.0009520
Valid-->> Epoch [1450/3000], Standardized Loss: 0.0130198, Inverse Loss: 0.0027177
Training -->> Epoch: 1451,(no reg loss)standard loss: 0.0048245, inverse loss: 0.0010071
Valid-->> Epoch [1451/3000], Standardized Loss: 0.0130701, Inverse Loss: 0.0027282
Training -->> Epoch: 1452,(no reg loss)standard loss: 0.0046576, inverse loss: 0.0009722
Valid-->> Epoch [1452/3000], Standardized Loss: 0.0131237, Inverse Loss: 0.0027394
Training -->> Epoch: 1453,(no reg loss)standard loss: 0.0047062, inverse loss: 0.0009824
Valid-->> Epoch [1453/3000], Standardized Loss: 0.0130451, Inverse Loss: 0.0027230
Training -->> Epoch: 1454,(no reg loss)standard loss: 0.0047128, inverse loss: 0.0009838
Valid-->> Epoch [1454/3000], Standardized Loss: 0.0131434, Inverse Loss: 0.0027435
Training -->> Epoch: 1455,(no reg loss)standard loss: 0.0047471, inverse loss: 0.0009909
Valid-->> Epoch [1455/3000], Standardized Loss: 0.0131474, Inverse Loss: 0.0027444
Training -->> Epoch: 1456,(no reg loss)standard loss: 0.0048013, inverse loss: 0.0010022
Valid-->> Epoch [1456/3000], Standardized Loss: 0.0132053, Inverse Loss: 0.0027564
Training -->> Epoch: 1457,(no reg loss)standard loss: 0.0048309, inverse loss: 0.0010084
Valid-->> Epoch [1457/3000], Standardized Loss: 0.0130580, Inverse Loss: 0.0027257
Training -->> Epoch: 1458,(no reg loss)standard loss: 0.0046428, inverse loss: 0.0009691
Valid-->> Epoch [1458/3000], Standardized Loss: 0.0130756, Inverse Loss: 0.0027294
Training -->> Epoch: 1459,(no reg loss)standard loss: 0.0048786, inverse loss: 0.0010184
Valid-->> Epoch [1459/3000], Standardized Loss: 0.0130466, Inverse Loss: 0.0027233
Training -->> Epoch: 1460,(no reg loss)standard loss: 0.0046882, inverse loss: 0.0009786
Valid-->> Epoch [1460/3000], Standardized Loss: 0.0130877, Inverse Loss: 0.0027319
Training -->> Epoch: 1461,(no reg loss)standard loss: 0.0049252, inverse loss: 0.0010281
Valid-->> Epoch [1461/3000], Standardized Loss: 0.0129602, Inverse Loss: 0.0027053
Training -->> Epoch: 1462,(no reg loss)standard loss: 0.0047851, inverse loss: 0.0009988
Valid-->> Epoch [1462/3000], Standardized Loss: 0.0130306, Inverse Loss: 0.0027200
Training -->> Epoch: 1463,(no reg loss)standard loss: 0.0045906, inverse loss: 0.0009582
Valid-->> Epoch [1463/3000], Standardized Loss: 0.0130603, Inverse Loss: 0.0027262
Training -->> Epoch: 1464,(no reg loss)standard loss: 0.0047350, inverse loss: 0.0009884
Valid-->> Epoch [1464/3000], Standardized Loss: 0.0130603, Inverse Loss: 0.0027262
Training -->> Epoch: 1465,(no reg loss)standard loss: 0.0046394, inverse loss: 0.0009684
Valid-->> Epoch [1465/3000], Standardized Loss: 0.0130666, Inverse Loss: 0.0027275
Training -->> Epoch: 1466,(no reg loss)standard loss: 0.0048910, inverse loss: 0.0010209
Valid-->> Epoch [1466/3000], Standardized Loss: 0.0129861, Inverse Loss: 0.0027107
Training -->> Epoch: 1467,(no reg loss)standard loss: 0.0047505, inverse loss: 0.0009916
Valid-->> Epoch [1467/3000], Standardized Loss: 0.0131217, Inverse Loss: 0.0027390
Training -->> Epoch: 1468,(no reg loss)standard loss: 0.0049223, inverse loss: 0.0010275
Valid-->> Epoch [1468/3000], Standardized Loss: 0.0129496, Inverse Loss: 0.0027031
Valid-->> Lowest loss found at epoch 1468, loss: 0.0027031
Epoch 1468, Masked params (inverse standardized): tensor([3.229728e+01, 4.430191e+01, 7.361221e-02, 2.637197e+01, 2.770441e+01,
        1.781666e+01, 2.143722e+01, 1.644400e+00, 1.066127e+02, 2.757205e+01,
        2.616961e+01, 4.462130e+01, 2.444092e+01, 1.504363e+01, 8.461642e+01,
        2.836341e+01, 5.671961e+00, 2.982671e+01, 1.197519e+01, 3.204482e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1469,(no reg loss)standard loss: 0.0045238, inverse loss: 0.0009443
Valid-->> Epoch [1469/3000], Standardized Loss: 0.0130381, Inverse Loss: 0.0027215
Training -->> Epoch: 1470,(no reg loss)standard loss: 0.0047692, inverse loss: 0.0009955
Valid-->> Epoch [1470/3000], Standardized Loss: 0.0131632, Inverse Loss: 0.0027477
Training -->> Epoch: 1471,(no reg loss)standard loss: 0.0047096, inverse loss: 0.0009831
Valid-->> Epoch [1471/3000], Standardized Loss: 0.0130316, Inverse Loss: 0.0027202
Training -->> Epoch: 1472,(no reg loss)standard loss: 0.0048936, inverse loss: 0.0010215
Valid-->> Epoch [1472/3000], Standardized Loss: 0.0130632, Inverse Loss: 0.0027268
Training -->> Epoch: 1473,(no reg loss)standard loss: 0.0045805, inverse loss: 0.0009561
Valid-->> Epoch [1473/3000], Standardized Loss: 0.0130622, Inverse Loss: 0.0027266
Training -->> Epoch: 1474,(no reg loss)standard loss: 0.0048713, inverse loss: 0.0010168
Valid-->> Epoch [1474/3000], Standardized Loss: 0.0131445, Inverse Loss: 0.0027438
Training -->> Epoch: 1475,(no reg loss)standard loss: 0.0047811, inverse loss: 0.0009980
Valid-->> Epoch [1475/3000], Standardized Loss: 0.0129512, Inverse Loss: 0.0027034
Training -->> Epoch: 1476,(no reg loss)standard loss: 0.0047254, inverse loss: 0.0009864
Valid-->> Epoch [1476/3000], Standardized Loss: 0.0131025, Inverse Loss: 0.0027350
Training -->> Epoch: 1477,(no reg loss)standard loss: 0.0050523, inverse loss: 0.0010546
Valid-->> Epoch [1477/3000], Standardized Loss: 0.0130079, Inverse Loss: 0.0027153
Training -->> Epoch: 1478,(no reg loss)standard loss: 0.0045940, inverse loss: 0.0009589
Valid-->> Epoch [1478/3000], Standardized Loss: 0.0131119, Inverse Loss: 0.0027370
Training -->> Epoch: 1479,(no reg loss)standard loss: 0.0048510, inverse loss: 0.0010126
Valid-->> Epoch [1479/3000], Standardized Loss: 0.0130573, Inverse Loss: 0.0027256
Training -->> Epoch: 1480,(no reg loss)standard loss: 0.0047088, inverse loss: 0.0009829
Valid-->> Epoch [1480/3000], Standardized Loss: 0.0130200, Inverse Loss: 0.0027178
Training -->> Epoch: 1481,(no reg loss)standard loss: 0.0047416, inverse loss: 0.0009898
Valid-->> Epoch [1481/3000], Standardized Loss: 0.0130503, Inverse Loss: 0.0027241
Training -->> Epoch: 1482,(no reg loss)standard loss: 0.0046213, inverse loss: 0.0009646
Valid-->> Epoch [1482/3000], Standardized Loss: 0.0130716, Inverse Loss: 0.0027285
Training -->> Epoch: 1483,(no reg loss)standard loss: 0.0049220, inverse loss: 0.0010274
Valid-->> Epoch [1483/3000], Standardized Loss: 0.0132130, Inverse Loss: 0.0027581
Training -->> Epoch: 1484,(no reg loss)standard loss: 0.0047460, inverse loss: 0.0009907
Valid-->> Epoch [1484/3000], Standardized Loss: 0.0130734, Inverse Loss: 0.0027289
Training -->> Epoch: 1485,(no reg loss)standard loss: 0.0047432, inverse loss: 0.0009901
Valid-->> Epoch [1485/3000], Standardized Loss: 0.0131010, Inverse Loss: 0.0027347
Training -->> Epoch: 1486,(no reg loss)standard loss: 0.0048659, inverse loss: 0.0010157
Valid-->> Epoch [1486/3000], Standardized Loss: 0.0129799, Inverse Loss: 0.0027094
Training -->> Epoch: 1487,(no reg loss)standard loss: 0.0046428, inverse loss: 0.0009691
Valid-->> Epoch [1487/3000], Standardized Loss: 0.0130630, Inverse Loss: 0.0027268
Training -->> Epoch: 1488,(no reg loss)standard loss: 0.0045477, inverse loss: 0.0009493
Valid-->> Epoch [1488/3000], Standardized Loss: 0.0130014, Inverse Loss: 0.0027139
Training -->> Epoch: 1489,(no reg loss)standard loss: 0.0048536, inverse loss: 0.0010131
Valid-->> Epoch [1489/3000], Standardized Loss: 0.0131496, Inverse Loss: 0.0027448
Training -->> Epoch: 1490,(no reg loss)standard loss: 0.0048429, inverse loss: 0.0010109
Valid-->> Epoch [1490/3000], Standardized Loss: 0.0129856, Inverse Loss: 0.0027106
Training -->> Epoch: 1491,(no reg loss)standard loss: 0.0046933, inverse loss: 0.0009797
Valid-->> Epoch [1491/3000], Standardized Loss: 0.0131220, Inverse Loss: 0.0027391
Training -->> Epoch: 1492,(no reg loss)standard loss: 0.0050623, inverse loss: 0.0010567
Valid-->> Epoch [1492/3000], Standardized Loss: 0.0129726, Inverse Loss: 0.0027079
Training -->> Epoch: 1493,(no reg loss)standard loss: 0.0045813, inverse loss: 0.0009563
Valid-->> Epoch [1493/3000], Standardized Loss: 0.0130274, Inverse Loss: 0.0027193
Training -->> Epoch: 1494,(no reg loss)standard loss: 0.0049457, inverse loss: 0.0010324
Valid-->> Epoch [1494/3000], Standardized Loss: 0.0130457, Inverse Loss: 0.0027231
Training -->> Epoch: 1495,(no reg loss)standard loss: 0.0047082, inverse loss: 0.0009828
Valid-->> Epoch [1495/3000], Standardized Loss: 0.0130018, Inverse Loss: 0.0027140
Training -->> Epoch: 1496,(no reg loss)standard loss: 0.0045296, inverse loss: 0.0009455
Valid-->> Epoch [1496/3000], Standardized Loss: 0.0129322, Inverse Loss: 0.0026994
Valid-->> Lowest loss found at epoch 1496, loss: 0.0026994
Epoch 1496, Masked params (inverse standardized): tensor([3.229333e+01, 4.430132e+01, 6.660271e-02, 2.637389e+01, 2.770600e+01,
        1.781047e+01, 2.143323e+01, 1.637436e+00, 1.066132e+02, 2.757400e+01,
        2.617120e+01, 4.461436e+01, 2.443987e+01, 1.503706e+01, 8.461574e+01,
        2.836381e+01, 5.665222e+00, 2.982489e+01, 1.196826e+01, 3.204114e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1497,(no reg loss)standard loss: 0.0047051, inverse loss: 0.0009821
Valid-->> Epoch [1497/3000], Standardized Loss: 0.0130752, Inverse Loss: 0.0027293
Training -->> Epoch: 1498,(no reg loss)standard loss: 0.0047777, inverse loss: 0.0009973
Valid-->> Epoch [1498/3000], Standardized Loss: 0.0130305, Inverse Loss: 0.0027200
Training -->> Epoch: 1499,(no reg loss)standard loss: 0.0046257, inverse loss: 0.0009656
Valid-->> Epoch [1499/3000], Standardized Loss: 0.0130794, Inverse Loss: 0.0027302
Training -->> Epoch: 1500,(no reg loss)standard loss: 0.0049052, inverse loss: 0.0010239
Valid-->> Epoch [1500/3000], Standardized Loss: 0.0131264, Inverse Loss: 0.0027400
Training -->> Epoch: 1501,(no reg loss)standard loss: 0.0047165, inverse loss: 0.0009845
Valid-->> Epoch [1501/3000], Standardized Loss: 0.0129580, Inverse Loss: 0.0027048
Training -->> Epoch: 1502,(no reg loss)standard loss: 0.0047596, inverse loss: 0.0009935
Valid-->> Epoch [1502/3000], Standardized Loss: 0.0130544, Inverse Loss: 0.0027249
Training -->> Epoch: 1503,(no reg loss)standard loss: 0.0049033, inverse loss: 0.0010235
Valid-->> Epoch [1503/3000], Standardized Loss: 0.0129885, Inverse Loss: 0.0027112
Training -->> Epoch: 1504,(no reg loss)standard loss: 0.0046921, inverse loss: 0.0009794
Valid-->> Epoch [1504/3000], Standardized Loss: 0.0130373, Inverse Loss: 0.0027214
Training -->> Epoch: 1505,(no reg loss)standard loss: 0.0046244, inverse loss: 0.0009653
Valid-->> Epoch [1505/3000], Standardized Loss: 0.0129739, Inverse Loss: 0.0027082
Training -->> Epoch: 1506,(no reg loss)standard loss: 0.0047844, inverse loss: 0.0009987
Valid-->> Epoch [1506/3000], Standardized Loss: 0.0132195, Inverse Loss: 0.0027594
Training -->> Epoch: 1507,(no reg loss)standard loss: 0.0048660, inverse loss: 0.0010157
Valid-->> Epoch [1507/3000], Standardized Loss: 0.0130315, Inverse Loss: 0.0027202
Training -->> Epoch: 1508,(no reg loss)standard loss: 0.0046778, inverse loss: 0.0009764
Valid-->> Epoch [1508/3000], Standardized Loss: 0.0129649, Inverse Loss: 0.0027063
Training -->> Epoch: 1509,(no reg loss)standard loss: 0.0048825, inverse loss: 0.0010192
Valid-->> Epoch [1509/3000], Standardized Loss: 0.0129924, Inverse Loss: 0.0027120
Training -->> Epoch: 1510,(no reg loss)standard loss: 0.0048346, inverse loss: 0.0010092
Valid-->> Epoch [1510/3000], Standardized Loss: 0.0129839, Inverse Loss: 0.0027102
Training -->> Epoch: 1511,(no reg loss)standard loss: 0.0045904, inverse loss: 0.0009582
Valid-->> Epoch [1511/3000], Standardized Loss: 0.0129563, Inverse Loss: 0.0027045
Training -->> Epoch: 1512,(no reg loss)standard loss: 0.0047256, inverse loss: 0.0009864
Valid-->> Epoch [1512/3000], Standardized Loss: 0.0130672, Inverse Loss: 0.0027276
Training -->> Epoch: 1513,(no reg loss)standard loss: 0.0049471, inverse loss: 0.0010326
Valid-->> Epoch [1513/3000], Standardized Loss: 0.0129394, Inverse Loss: 0.0027010
Training -->> Epoch: 1514,(no reg loss)standard loss: 0.0046822, inverse loss: 0.0009774
Valid-->> Epoch [1514/3000], Standardized Loss: 0.0130939, Inverse Loss: 0.0027332
Training -->> Epoch: 1515,(no reg loss)standard loss: 0.0047148, inverse loss: 0.0009842
Valid-->> Epoch [1515/3000], Standardized Loss: 0.0129338, Inverse Loss: 0.0026998
Training -->> Epoch: 1516,(no reg loss)standard loss: 0.0046741, inverse loss: 0.0009757
Valid-->> Epoch [1516/3000], Standardized Loss: 0.0130789, Inverse Loss: 0.0027301
Training -->> Epoch: 1517,(no reg loss)standard loss: 0.0049914, inverse loss: 0.0010419
Valid-->> Epoch [1517/3000], Standardized Loss: 0.0129653, Inverse Loss: 0.0027064
Training -->> Epoch: 1518,(no reg loss)standard loss: 0.0048463, inverse loss: 0.0010116
Valid-->> Epoch [1518/3000], Standardized Loss: 0.0131885, Inverse Loss: 0.0027529
Training -->> Epoch: 1519,(no reg loss)standard loss: 0.0048359, inverse loss: 0.0010094
Valid-->> Epoch [1519/3000], Standardized Loss: 0.0129718, Inverse Loss: 0.0027077
Training -->> Epoch: 1520,(no reg loss)standard loss: 0.0046842, inverse loss: 0.0009778
Valid-->> Epoch [1520/3000], Standardized Loss: 0.0129379, Inverse Loss: 0.0027006
Training -->> Epoch: 1521,(no reg loss)standard loss: 0.0047395, inverse loss: 0.0009893
Valid-->> Epoch [1521/3000], Standardized Loss: 0.0130321, Inverse Loss: 0.0027203
Training -->> Epoch: 1522,(no reg loss)standard loss: 0.0049052, inverse loss: 0.0010239
Valid-->> Epoch [1522/3000], Standardized Loss: 0.0130855, Inverse Loss: 0.0027315
Training -->> Epoch: 1523,(no reg loss)standard loss: 0.0047398, inverse loss: 0.0009894
Valid-->> Epoch [1523/3000], Standardized Loss: 0.0129444, Inverse Loss: 0.0027020
Training -->> Epoch: 1524,(no reg loss)standard loss: 0.0047442, inverse loss: 0.0009903
Valid-->> Epoch [1524/3000], Standardized Loss: 0.0130754, Inverse Loss: 0.0027293
Training -->> Epoch: 1525,(no reg loss)standard loss: 0.0047800, inverse loss: 0.0009978
Valid-->> Epoch [1525/3000], Standardized Loss: 0.0131800, Inverse Loss: 0.0027512
Training -->> Epoch: 1526,(no reg loss)standard loss: 0.0049494, inverse loss: 0.0010331
Valid-->> Epoch [1526/3000], Standardized Loss: 0.0129878, Inverse Loss: 0.0027111
Training -->> Epoch: 1527,(no reg loss)standard loss: 0.0047662, inverse loss: 0.0009949
Valid-->> Epoch [1527/3000], Standardized Loss: 0.0129658, Inverse Loss: 0.0027065
Training -->> Epoch: 1528,(no reg loss)standard loss: 0.0047962, inverse loss: 0.0010012
Valid-->> Epoch [1528/3000], Standardized Loss: 0.0129740, Inverse Loss: 0.0027082
Training -->> Epoch: 1529,(no reg loss)standard loss: 0.0046667, inverse loss: 0.0009741
Valid-->> Epoch [1529/3000], Standardized Loss: 0.0130208, Inverse Loss: 0.0027179
Training -->> Epoch: 1530,(no reg loss)standard loss: 0.0049528, inverse loss: 0.0010338
Valid-->> Epoch [1530/3000], Standardized Loss: 0.0129961, Inverse Loss: 0.0027128
Training -->> Epoch: 1531,(no reg loss)standard loss: 0.0045769, inverse loss: 0.0009554
Valid-->> Epoch [1531/3000], Standardized Loss: 0.0129746, Inverse Loss: 0.0027083
Training -->> Epoch: 1532,(no reg loss)standard loss: 0.0049523, inverse loss: 0.0010337
Valid-->> Epoch [1532/3000], Standardized Loss: 0.0130259, Inverse Loss: 0.0027190
Training -->> Epoch: 1533,(no reg loss)standard loss: 0.0046706, inverse loss: 0.0009749
Valid-->> Epoch [1533/3000], Standardized Loss: 0.0130064, Inverse Loss: 0.0027149
Training -->> Epoch: 1534,(no reg loss)standard loss: 0.0048329, inverse loss: 0.0010088
Valid-->> Epoch [1534/3000], Standardized Loss: 0.0130351, Inverse Loss: 0.0027209
Training -->> Epoch: 1535,(no reg loss)standard loss: 0.0048234, inverse loss: 0.0010068
Valid-->> Epoch [1535/3000], Standardized Loss: 0.0129612, Inverse Loss: 0.0027055
Training -->> Epoch: 1536,(no reg loss)standard loss: 0.0047571, inverse loss: 0.0009930
Valid-->> Epoch [1536/3000], Standardized Loss: 0.0129441, Inverse Loss: 0.0027019
Training -->> Epoch: 1537,(no reg loss)standard loss: 0.0047397, inverse loss: 0.0009894
Valid-->> Epoch [1537/3000], Standardized Loss: 0.0129285, Inverse Loss: 0.0026987
Valid-->> Lowest loss found at epoch 1537, loss: 0.0026987
Epoch 1537, Masked params (inverse standardized): tensor([3.229441e+01, 4.430139e+01, 6.992531e-02, 2.637415e+01, 2.770592e+01,
        1.781277e+01, 2.143429e+01, 1.640385e+00, 1.066110e+02, 2.757410e+01,
        2.617128e+01, 4.461678e+01, 2.443992e+01, 1.503982e+01, 8.461555e+01,
        2.836347e+01, 5.668468e+00, 2.982523e+01, 1.197078e+01, 3.204232e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1538,(no reg loss)standard loss: 0.0048701, inverse loss: 0.0010166
Valid-->> Epoch [1538/3000], Standardized Loss: 0.0130065, Inverse Loss: 0.0027150
Training -->> Epoch: 1539,(no reg loss)standard loss: 0.0047259, inverse loss: 0.0009865
Valid-->> Epoch [1539/3000], Standardized Loss: 0.0129917, Inverse Loss: 0.0027119
Training -->> Epoch: 1540,(no reg loss)standard loss: 0.0048506, inverse loss: 0.0010125
Valid-->> Epoch [1540/3000], Standardized Loss: 0.0130070, Inverse Loss: 0.0027151
Training -->> Epoch: 1541,(no reg loss)standard loss: 0.0046460, inverse loss: 0.0009698
Valid-->> Epoch [1541/3000], Standardized Loss: 0.0130200, Inverse Loss: 0.0027178
Training -->> Epoch: 1542,(no reg loss)standard loss: 0.0049706, inverse loss: 0.0010375
Valid-->> Epoch [1542/3000], Standardized Loss: 0.0129971, Inverse Loss: 0.0027130
Training -->> Epoch: 1543,(no reg loss)standard loss: 0.0048054, inverse loss: 0.0010031
Valid-->> Epoch [1543/3000], Standardized Loss: 0.0130643, Inverse Loss: 0.0027270
Training -->> Epoch: 1544,(no reg loss)standard loss: 0.0047151, inverse loss: 0.0009842
Valid-->> Epoch [1544/3000], Standardized Loss: 0.0130721, Inverse Loss: 0.0027287
Training -->> Epoch: 1545,(no reg loss)standard loss: 0.0049742, inverse loss: 0.0010383
Valid-->> Epoch [1545/3000], Standardized Loss: 0.0130828, Inverse Loss: 0.0027309
Training -->> Epoch: 1546,(no reg loss)standard loss: 0.0048375, inverse loss: 0.0010098
Valid-->> Epoch [1546/3000], Standardized Loss: 0.0129467, Inverse Loss: 0.0027025
Training -->> Epoch: 1547,(no reg loss)standard loss: 0.0048451, inverse loss: 0.0010114
Valid-->> Epoch [1547/3000], Standardized Loss: 0.0129562, Inverse Loss: 0.0027045
Training -->> Epoch: 1548,(no reg loss)standard loss: 0.0048160, inverse loss: 0.0010053
Valid-->> Epoch [1548/3000], Standardized Loss: 0.0130170, Inverse Loss: 0.0027172
Training -->> Epoch: 1549,(no reg loss)standard loss: 0.0047392, inverse loss: 0.0009893
Valid-->> Epoch [1549/3000], Standardized Loss: 0.0129742, Inverse Loss: 0.0027082
Training -->> Epoch: 1550,(no reg loss)standard loss: 0.0046351, inverse loss: 0.0009675
Valid-->> Epoch [1550/3000], Standardized Loss: 0.0129912, Inverse Loss: 0.0027118
Training -->> Epoch: 1551,(no reg loss)standard loss: 0.0049609, inverse loss: 0.0010355
Valid-->> Epoch [1551/3000], Standardized Loss: 0.0129426, Inverse Loss: 0.0027016
Training -->> Epoch: 1552,(no reg loss)standard loss: 0.0046943, inverse loss: 0.0009799
Valid-->> Epoch [1552/3000], Standardized Loss: 0.0130925, Inverse Loss: 0.0027329
Training -->> Epoch: 1553,(no reg loss)standard loss: 0.0049844, inverse loss: 0.0010404
Valid-->> Epoch [1553/3000], Standardized Loss: 0.0129415, Inverse Loss: 0.0027014
Training -->> Epoch: 1554,(no reg loss)standard loss: 0.0047693, inverse loss: 0.0009955
Valid-->> Epoch [1554/3000], Standardized Loss: 0.0129354, Inverse Loss: 0.0027001
Training -->> Epoch: 1555,(no reg loss)standard loss: 0.0047583, inverse loss: 0.0009932
Valid-->> Epoch [1555/3000], Standardized Loss: 0.0129246, Inverse Loss: 0.0026979
Valid-->> Lowest loss found at epoch 1555, loss: 0.0026979
Epoch 1555, Masked params (inverse standardized): tensor([3.229419e+01, 4.430255e+01, 6.999397e-02, 2.637457e+01, 2.770621e+01,
        1.781233e+01, 2.143407e+01, 1.640221e+00, 1.066127e+02, 2.757450e+01,
        2.617151e+01, 4.461702e+01, 2.443995e+01, 1.503928e+01, 8.461714e+01,
        2.836357e+01, 5.668051e+00, 2.982516e+01, 1.197085e+01, 3.204184e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1556,(no reg loss)standard loss: 0.0046105, inverse loss: 0.0009624
Valid-->> Epoch [1556/3000], Standardized Loss: 0.0129872, Inverse Loss: 0.0027109
Training -->> Epoch: 1557,(no reg loss)standard loss: 0.0048043, inverse loss: 0.0010028
Valid-->> Epoch [1557/3000], Standardized Loss: 0.0130379, Inverse Loss: 0.0027215
Training -->> Epoch: 1558,(no reg loss)standard loss: 0.0049200, inverse loss: 0.0010270
Valid-->> Epoch [1558/3000], Standardized Loss: 0.0130186, Inverse Loss: 0.0027175
Training -->> Epoch: 1559,(no reg loss)standard loss: 0.0047893, inverse loss: 0.0009997
Valid-->> Epoch [1559/3000], Standardized Loss: 0.0129950, Inverse Loss: 0.0027125
Training -->> Epoch: 1560,(no reg loss)standard loss: 0.0048291, inverse loss: 0.0010080
Valid-->> Epoch [1560/3000], Standardized Loss: 0.0130730, Inverse Loss: 0.0027288
Training -->> Epoch: 1561,(no reg loss)standard loss: 0.0048599, inverse loss: 0.0010144
Valid-->> Epoch [1561/3000], Standardized Loss: 0.0129562, Inverse Loss: 0.0027045
Training -->> Epoch: 1562,(no reg loss)standard loss: 0.0047417, inverse loss: 0.0009898
Valid-->> Epoch [1562/3000], Standardized Loss: 0.0129852, Inverse Loss: 0.0027105
Training -->> Epoch: 1563,(no reg loss)standard loss: 0.0048129, inverse loss: 0.0010046
Valid-->> Epoch [1563/3000], Standardized Loss: 0.0130468, Inverse Loss: 0.0027234
Training -->> Epoch: 1564,(no reg loss)standard loss: 0.0047855, inverse loss: 0.0009989
Valid-->> Epoch [1564/3000], Standardized Loss: 0.0129942, Inverse Loss: 0.0027124
Training -->> Epoch: 1565,(no reg loss)standard loss: 0.0048762, inverse loss: 0.0010178
Valid-->> Epoch [1565/3000], Standardized Loss: 0.0130025, Inverse Loss: 0.0027141
Training -->> Epoch: 1566,(no reg loss)standard loss: 0.0049037, inverse loss: 0.0010236
Valid-->> Epoch [1566/3000], Standardized Loss: 0.0128553, Inverse Loss: 0.0026834
Valid-->> Lowest loss found at epoch 1566, loss: 0.0026834
Epoch 1566, Masked params (inverse standardized): tensor([3.229713e+01, 4.430131e+01, 7.209969e-02, 2.637556e+01, 2.770730e+01,
        1.781566e+01, 2.143706e+01, 1.642551e+00, 1.066124e+02, 2.757561e+01,
        2.617262e+01, 4.461917e+01, 2.444174e+01, 1.504238e+01, 8.461584e+01,
        2.836499e+01, 5.671019e+00, 2.982727e+01, 1.197324e+01, 3.204476e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1567,(no reg loss)standard loss: 0.0046705, inverse loss: 0.0009749
Valid-->> Epoch [1567/3000], Standardized Loss: 0.0131229, Inverse Loss: 0.0027393
Training -->> Epoch: 1568,(no reg loss)standard loss: 0.0048239, inverse loss: 0.0010069
Valid-->> Epoch [1568/3000], Standardized Loss: 0.0129316, Inverse Loss: 0.0026993
Training -->> Epoch: 1569,(no reg loss)standard loss: 0.0048199, inverse loss: 0.0010061
Valid-->> Epoch [1569/3000], Standardized Loss: 0.0129908, Inverse Loss: 0.0027117
Training -->> Epoch: 1570,(no reg loss)standard loss: 0.0048166, inverse loss: 0.0010054
Valid-->> Epoch [1570/3000], Standardized Loss: 0.0130454, Inverse Loss: 0.0027231
Training -->> Epoch: 1571,(no reg loss)standard loss: 0.0049657, inverse loss: 0.0010365
Valid-->> Epoch [1571/3000], Standardized Loss: 0.0129573, Inverse Loss: 0.0027047
Training -->> Epoch: 1572,(no reg loss)standard loss: 0.0048672, inverse loss: 0.0010160
Valid-->> Epoch [1572/3000], Standardized Loss: 0.0129639, Inverse Loss: 0.0027061
Training -->> Epoch: 1573,(no reg loss)standard loss: 0.0047730, inverse loss: 0.0009963
Valid-->> Epoch [1573/3000], Standardized Loss: 0.0129380, Inverse Loss: 0.0027007
Training -->> Epoch: 1574,(no reg loss)standard loss: 0.0048711, inverse loss: 0.0010168
Valid-->> Epoch [1574/3000], Standardized Loss: 0.0129430, Inverse Loss: 0.0027017
Training -->> Epoch: 1575,(no reg loss)standard loss: 0.0049333, inverse loss: 0.0010298
Valid-->> Epoch [1575/3000], Standardized Loss: 0.0128400, Inverse Loss: 0.0026802
Valid-->> Lowest loss found at epoch 1575, loss: 0.0026802
Epoch 1575, Masked params (inverse standardized): tensor([3.229707e+01, 4.430053e+01, 7.220078e-02, 2.637609e+01, 2.770779e+01,
        1.781564e+01, 2.143699e+01, 1.642725e+00, 1.066127e+02, 2.757611e+01,
        2.617313e+01, 4.461927e+01, 2.444183e+01, 1.504232e+01, 8.461495e+01,
        2.836532e+01, 5.671049e+00, 2.982725e+01, 1.197328e+01, 3.204468e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1576,(no reg loss)standard loss: 0.0047077, inverse loss: 0.0009827
Valid-->> Epoch [1576/3000], Standardized Loss: 0.0131194, Inverse Loss: 0.0027385
Training -->> Epoch: 1577,(no reg loss)standard loss: 0.0051131, inverse loss: 0.0010673
Valid-->> Epoch [1577/3000], Standardized Loss: 0.0128756, Inverse Loss: 0.0026876
Training -->> Epoch: 1578,(no reg loss)standard loss: 0.0047057, inverse loss: 0.0009823
Valid-->> Epoch [1578/3000], Standardized Loss: 0.0130326, Inverse Loss: 0.0027204
Training -->> Epoch: 1579,(no reg loss)standard loss: 0.0048350, inverse loss: 0.0010093
Valid-->> Epoch [1579/3000], Standardized Loss: 0.0130177, Inverse Loss: 0.0027173
Training -->> Epoch: 1580,(no reg loss)standard loss: 0.0049911, inverse loss: 0.0010418
Valid-->> Epoch [1580/3000], Standardized Loss: 0.0130477, Inverse Loss: 0.0027236
Training -->> Epoch: 1581,(no reg loss)standard loss: 0.0048263, inverse loss: 0.0010074
Valid-->> Epoch [1581/3000], Standardized Loss: 0.0128357, Inverse Loss: 0.0026793
Valid-->> Lowest loss found at epoch 1581, loss: 0.0026793
Epoch 1581, Masked params (inverse standardized): tensor([3.229746e+01, 4.430204e+01, 7.005119e-02, 2.637622e+01, 2.770792e+01,
        1.781514e+01, 2.143734e+01, 1.640547e+00, 1.066125e+02, 2.757623e+01,
        2.617327e+01, 4.461799e+01, 2.444228e+01, 1.504152e+01, 8.461638e+01,
        2.836558e+01, 5.668793e+00, 2.982780e+01, 1.197223e+01, 3.204510e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1582,(no reg loss)standard loss: 0.0046138, inverse loss: 0.0009631
Valid-->> Epoch [1582/3000], Standardized Loss: 0.0129409, Inverse Loss: 0.0027013
Training -->> Epoch: 1583,(no reg loss)standard loss: 0.0048990, inverse loss: 0.0010226
Valid-->> Epoch [1583/3000], Standardized Loss: 0.0129125, Inverse Loss: 0.0026953
Training -->> Epoch: 1584,(no reg loss)standard loss: 0.0045695, inverse loss: 0.0009538
Valid-->> Epoch [1584/3000], Standardized Loss: 0.0129382, Inverse Loss: 0.0027007
Training -->> Epoch: 1585,(no reg loss)standard loss: 0.0050782, inverse loss: 0.0010600
Valid-->> Epoch [1585/3000], Standardized Loss: 0.0130864, Inverse Loss: 0.0027316
Training -->> Epoch: 1586,(no reg loss)standard loss: 0.0049567, inverse loss: 0.0010346
Valid-->> Epoch [1586/3000], Standardized Loss: 0.0129266, Inverse Loss: 0.0026983
Training -->> Epoch: 1587,(no reg loss)standard loss: 0.0047480, inverse loss: 0.0009911
Valid-->> Epoch [1587/3000], Standardized Loss: 0.0129298, Inverse Loss: 0.0026989
Training -->> Epoch: 1588,(no reg loss)standard loss: 0.0046991, inverse loss: 0.0009809
Valid-->> Epoch [1588/3000], Standardized Loss: 0.0129610, Inverse Loss: 0.0027055
Training -->> Epoch: 1589,(no reg loss)standard loss: 0.0050458, inverse loss: 0.0010532
Valid-->> Epoch [1589/3000], Standardized Loss: 0.0129385, Inverse Loss: 0.0027008
Training -->> Epoch: 1590,(no reg loss)standard loss: 0.0047302, inverse loss: 0.0009874
Valid-->> Epoch [1590/3000], Standardized Loss: 0.0128913, Inverse Loss: 0.0026909
Training -->> Epoch: 1591,(no reg loss)standard loss: 0.0048280, inverse loss: 0.0010078
Valid-->> Epoch [1591/3000], Standardized Loss: 0.0129755, Inverse Loss: 0.0027085
Training -->> Epoch: 1592,(no reg loss)standard loss: 0.0049091, inverse loss: 0.0010247
Valid-->> Epoch [1592/3000], Standardized Loss: 0.0129259, Inverse Loss: 0.0026981
Training -->> Epoch: 1593,(no reg loss)standard loss: 0.0047291, inverse loss: 0.0009872
Valid-->> Epoch [1593/3000], Standardized Loss: 0.0129628, Inverse Loss: 0.0027058
Training -->> Epoch: 1594,(no reg loss)standard loss: 0.0050027, inverse loss: 0.0010443
Valid-->> Epoch [1594/3000], Standardized Loss: 0.0131501, Inverse Loss: 0.0027449
Training -->> Epoch: 1595,(no reg loss)standard loss: 0.0048216, inverse loss: 0.0010064
Valid-->> Epoch [1595/3000], Standardized Loss: 0.0128814, Inverse Loss: 0.0026888
Training -->> Epoch: 1596,(no reg loss)standard loss: 0.0048370, inverse loss: 0.0010097
Valid-->> Epoch [1596/3000], Standardized Loss: 0.0129817, Inverse Loss: 0.0027098
Training -->> Epoch: 1597,(no reg loss)standard loss: 0.0047870, inverse loss: 0.0009992
Valid-->> Epoch [1597/3000], Standardized Loss: 0.0128935, Inverse Loss: 0.0026914
Training -->> Epoch: 1598,(no reg loss)standard loss: 0.0049996, inverse loss: 0.0010436
Valid-->> Epoch [1598/3000], Standardized Loss: 0.0129793, Inverse Loss: 0.0027093
Training -->> Epoch: 1599,(no reg loss)standard loss: 0.0048297, inverse loss: 0.0010082
Valid-->> Epoch [1599/3000], Standardized Loss: 0.0128793, Inverse Loss: 0.0026884
Training -->> Epoch: 1600,(no reg loss)standard loss: 0.0048675, inverse loss: 0.0010160
Valid-->> Epoch [1600/3000], Standardized Loss: 0.0129712, Inverse Loss: 0.0027076
Training -->> Epoch: 1601,(no reg loss)standard loss: 0.0047663, inverse loss: 0.0009949
Valid-->> Epoch [1601/3000], Standardized Loss: 0.0130269, Inverse Loss: 0.0027192
Training -->> Epoch: 1602,(no reg loss)standard loss: 0.0051369, inverse loss: 0.0010723
Valid-->> Epoch [1602/3000], Standardized Loss: 0.0129232, Inverse Loss: 0.0026976
Training -->> Epoch: 1603,(no reg loss)standard loss: 0.0046572, inverse loss: 0.0009721
Valid-->> Epoch [1603/3000], Standardized Loss: 0.0129270, Inverse Loss: 0.0026984
Training -->> Epoch: 1604,(no reg loss)standard loss: 0.0049768, inverse loss: 0.0010389
Valid-->> Epoch [1604/3000], Standardized Loss: 0.0129538, Inverse Loss: 0.0027040
Training -->> Epoch: 1605,(no reg loss)standard loss: 0.0049185, inverse loss: 0.0010267
Valid-->> Epoch [1605/3000], Standardized Loss: 0.0128693, Inverse Loss: 0.0026863
Training -->> Epoch: 1606,(no reg loss)standard loss: 0.0046885, inverse loss: 0.0009787
Valid-->> Epoch [1606/3000], Standardized Loss: 0.0129485, Inverse Loss: 0.0027029
Training -->> Epoch: 1607,(no reg loss)standard loss: 0.0050318, inverse loss: 0.0010503
Valid-->> Epoch [1607/3000], Standardized Loss: 0.0129504, Inverse Loss: 0.0027033
Training -->> Epoch: 1608,(no reg loss)standard loss: 0.0048973, inverse loss: 0.0010222
Valid-->> Epoch [1608/3000], Standardized Loss: 0.0129183, Inverse Loss: 0.0026966
Training -->> Epoch: 1609,(no reg loss)standard loss: 0.0047272, inverse loss: 0.0009867
Valid-->> Epoch [1609/3000], Standardized Loss: 0.0129548, Inverse Loss: 0.0027042
Training -->> Epoch: 1610,(no reg loss)standard loss: 0.0049081, inverse loss: 0.0010245
Valid-->> Epoch [1610/3000], Standardized Loss: 0.0129997, Inverse Loss: 0.0027135
Training -->> Epoch: 1611,(no reg loss)standard loss: 0.0048698, inverse loss: 0.0010165
Valid-->> Epoch [1611/3000], Standardized Loss: 0.0129170, Inverse Loss: 0.0026963
Training -->> Epoch: 1612,(no reg loss)standard loss: 0.0048913, inverse loss: 0.0010210
Valid-->> Epoch [1612/3000], Standardized Loss: 0.0128961, Inverse Loss: 0.0026919
Training -->> Epoch: 1613,(no reg loss)standard loss: 0.0046410, inverse loss: 0.0009687
Valid-->> Epoch [1613/3000], Standardized Loss: 0.0129638, Inverse Loss: 0.0027060
Training -->> Epoch: 1614,(no reg loss)standard loss: 0.0050542, inverse loss: 0.0010550
Valid-->> Epoch [1614/3000], Standardized Loss: 0.0128824, Inverse Loss: 0.0026891
Training -->> Epoch: 1615,(no reg loss)standard loss: 0.0047933, inverse loss: 0.0010006
Valid-->> Epoch [1615/3000], Standardized Loss: 0.0130204, Inverse Loss: 0.0027179
Training -->> Epoch: 1616,(no reg loss)standard loss: 0.0049242, inverse loss: 0.0010279
Valid-->> Epoch [1616/3000], Standardized Loss: 0.0129141, Inverse Loss: 0.0026957
Training -->> Epoch: 1617,(no reg loss)standard loss: 0.0048693, inverse loss: 0.0010164
Valid-->> Epoch [1617/3000], Standardized Loss: 0.0128874, Inverse Loss: 0.0026901
Training -->> Epoch: 1618,(no reg loss)standard loss: 0.0048380, inverse loss: 0.0010099
Valid-->> Epoch [1618/3000], Standardized Loss: 0.0129854, Inverse Loss: 0.0027105
Training -->> Epoch: 1619,(no reg loss)standard loss: 0.0049617, inverse loss: 0.0010357
Valid-->> Epoch [1619/3000], Standardized Loss: 0.0129189, Inverse Loss: 0.0026967
Training -->> Epoch: 1620,(no reg loss)standard loss: 0.0048287, inverse loss: 0.0010079
Valid-->> Epoch [1620/3000], Standardized Loss: 0.0129831, Inverse Loss: 0.0027101
Training -->> Epoch: 1621,(no reg loss)standard loss: 0.0049005, inverse loss: 0.0010229
Valid-->> Epoch [1621/3000], Standardized Loss: 0.0129111, Inverse Loss: 0.0026950
Training -->> Epoch: 1622,(no reg loss)standard loss: 0.0049207, inverse loss: 0.0010271
Valid-->> Epoch [1622/3000], Standardized Loss: 0.0129350, Inverse Loss: 0.0027000
Training -->> Epoch: 1623,(no reg loss)standard loss: 0.0049086, inverse loss: 0.0010246
Valid-->> Epoch [1623/3000], Standardized Loss: 0.0128608, Inverse Loss: 0.0026845
Training -->> Epoch: 1624,(no reg loss)standard loss: 0.0047765, inverse loss: 0.0009970
Valid-->> Epoch [1624/3000], Standardized Loss: 0.0130235, Inverse Loss: 0.0027185
Training -->> Epoch: 1625,(no reg loss)standard loss: 0.0048985, inverse loss: 0.0010225
Valid-->> Epoch [1625/3000], Standardized Loss: 0.0129195, Inverse Loss: 0.0026968
Training -->> Epoch: 1626,(no reg loss)standard loss: 0.0049397, inverse loss: 0.0010311
Valid-->> Epoch [1626/3000], Standardized Loss: 0.0130222, Inverse Loss: 0.0027182
Training -->> Epoch: 1627,(no reg loss)standard loss: 0.0048915, inverse loss: 0.0010210
Valid-->> Epoch [1627/3000], Standardized Loss: 0.0128886, Inverse Loss: 0.0026903
Training -->> Epoch: 1628,(no reg loss)standard loss: 0.0048316, inverse loss: 0.0010085
Valid-->> Epoch [1628/3000], Standardized Loss: 0.0129708, Inverse Loss: 0.0027075
Training -->> Epoch: 1629,(no reg loss)standard loss: 0.0049470, inverse loss: 0.0010326
Valid-->> Epoch [1629/3000], Standardized Loss: 0.0128529, Inverse Loss: 0.0026829
Training -->> Epoch: 1630,(no reg loss)standard loss: 0.0047945, inverse loss: 0.0010008
Valid-->> Epoch [1630/3000], Standardized Loss: 0.0130312, Inverse Loss: 0.0027201
Training -->> Epoch: 1631,(no reg loss)standard loss: 0.0050778, inverse loss: 0.0010599
Valid-->> Epoch [1631/3000], Standardized Loss: 0.0129179, Inverse Loss: 0.0026965
Training -->> Epoch: 1632,(no reg loss)standard loss: 0.0049552, inverse loss: 0.0010343
Valid-->> Epoch [1632/3000], Standardized Loss: 0.0129082, Inverse Loss: 0.0026944
Training -->> Epoch: 1633,(no reg loss)standard loss: 0.0048019, inverse loss: 0.0010023
Valid-->> Epoch [1633/3000], Standardized Loss: 0.0128993, Inverse Loss: 0.0026926
Training -->> Epoch: 1634,(no reg loss)standard loss: 0.0049087, inverse loss: 0.0010246
Valid-->> Epoch [1634/3000], Standardized Loss: 0.0129833, Inverse Loss: 0.0027101
Training -->> Epoch: 1635,(no reg loss)standard loss: 0.0049465, inverse loss: 0.0010325
Valid-->> Epoch [1635/3000], Standardized Loss: 0.0129015, Inverse Loss: 0.0026930
Training -->> Epoch: 1636,(no reg loss)standard loss: 0.0048193, inverse loss: 0.0010060
Valid-->> Epoch [1636/3000], Standardized Loss: 0.0130124, Inverse Loss: 0.0027162
Training -->> Epoch: 1637,(no reg loss)standard loss: 0.0048791, inverse loss: 0.0010185
Valid-->> Epoch [1637/3000], Standardized Loss: 0.0129614, Inverse Loss: 0.0027055
Training -->> Epoch: 1638,(no reg loss)standard loss: 0.0048574, inverse loss: 0.0010139
Valid-->> Epoch [1638/3000], Standardized Loss: 0.0129821, Inverse Loss: 0.0027099
Training -->> Epoch: 1639,(no reg loss)standard loss: 0.0050054, inverse loss: 0.0010448
Valid-->> Epoch [1639/3000], Standardized Loss: 0.0128946, Inverse Loss: 0.0026916
Training -->> Epoch: 1640,(no reg loss)standard loss: 0.0049718, inverse loss: 0.0010378
Valid-->> Epoch [1640/3000], Standardized Loss: 0.0128529, Inverse Loss: 0.0026829
Training -->> Epoch: 1641,(no reg loss)standard loss: 0.0047633, inverse loss: 0.0009943
Valid-->> Epoch [1641/3000], Standardized Loss: 0.0128946, Inverse Loss: 0.0026916
Training -->> Epoch: 1642,(no reg loss)standard loss: 0.0048380, inverse loss: 0.0010099
Valid-->> Epoch [1642/3000], Standardized Loss: 0.0130281, Inverse Loss: 0.0027195
Training -->> Epoch: 1643,(no reg loss)standard loss: 0.0051327, inverse loss: 0.0010714
Valid-->> Epoch [1643/3000], Standardized Loss: 0.0128183, Inverse Loss: 0.0026757
Valid-->> Lowest loss found at epoch 1643, loss: 0.0026757
Epoch 1643, Masked params (inverse standardized): tensor([3.229790e+01, 4.430103e+01, 7.424927e-02, 2.637743e+01, 2.770856e+01,
        1.781725e+01, 2.143776e+01, 1.644070e+00, 1.066113e+02, 2.757733e+01,
        2.617402e+01, 4.462118e+01, 2.444165e+01, 1.504422e+01, 8.461555e+01,
        2.836519e+01, 5.672541e+00, 2.982729e+01, 1.197527e+01, 3.204542e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1644,(no reg loss)standard loss: 0.0047670, inverse loss: 0.0009951
Valid-->> Epoch [1644/3000], Standardized Loss: 0.0128802, Inverse Loss: 0.0026886
Training -->> Epoch: 1645,(no reg loss)standard loss: 0.0047361, inverse loss: 0.0009886
Valid-->> Epoch [1645/3000], Standardized Loss: 0.0128842, Inverse Loss: 0.0026894
Training -->> Epoch: 1646,(no reg loss)standard loss: 0.0050223, inverse loss: 0.0010483
Valid-->> Epoch [1646/3000], Standardized Loss: 0.0128317, Inverse Loss: 0.0026785
Training -->> Epoch: 1647,(no reg loss)standard loss: 0.0050470, inverse loss: 0.0010535
Valid-->> Epoch [1647/3000], Standardized Loss: 0.0129360, Inverse Loss: 0.0027002
Training -->> Epoch: 1648,(no reg loss)standard loss: 0.0047917, inverse loss: 0.0010002
Valid-->> Epoch [1648/3000], Standardized Loss: 0.0128581, Inverse Loss: 0.0026840
Training -->> Epoch: 1649,(no reg loss)standard loss: 0.0048286, inverse loss: 0.0010079
Valid-->> Epoch [1649/3000], Standardized Loss: 0.0128276, Inverse Loss: 0.0026776
Training -->> Epoch: 1650,(no reg loss)standard loss: 0.0047566, inverse loss: 0.0009929
Valid-->> Epoch [1650/3000], Standardized Loss: 0.0130515, Inverse Loss: 0.0027243
Training -->> Epoch: 1651,(no reg loss)standard loss: 0.0050304, inverse loss: 0.0010500
Valid-->> Epoch [1651/3000], Standardized Loss: 0.0128373, Inverse Loss: 0.0026796
Training -->> Epoch: 1652,(no reg loss)standard loss: 0.0047768, inverse loss: 0.0009971
Valid-->> Epoch [1652/3000], Standardized Loss: 0.0129477, Inverse Loss: 0.0027027
Training -->> Epoch: 1653,(no reg loss)standard loss: 0.0048874, inverse loss: 0.0010202
Valid-->> Epoch [1653/3000], Standardized Loss: 0.0129601, Inverse Loss: 0.0027053
Training -->> Epoch: 1654,(no reg loss)standard loss: 0.0050143, inverse loss: 0.0010467
Valid-->> Epoch [1654/3000], Standardized Loss: 0.0128499, Inverse Loss: 0.0026823
Training -->> Epoch: 1655,(no reg loss)standard loss: 0.0048312, inverse loss: 0.0010085
Valid-->> Epoch [1655/3000], Standardized Loss: 0.0129276, Inverse Loss: 0.0026985
Training -->> Epoch: 1656,(no reg loss)standard loss: 0.0047868, inverse loss: 0.0009992
Valid-->> Epoch [1656/3000], Standardized Loss: 0.0129881, Inverse Loss: 0.0027111
Training -->> Epoch: 1657,(no reg loss)standard loss: 0.0048835, inverse loss: 0.0010194
Valid-->> Epoch [1657/3000], Standardized Loss: 0.0129321, Inverse Loss: 0.0026994
Training -->> Epoch: 1658,(no reg loss)standard loss: 0.0050665, inverse loss: 0.0010576
Valid-->> Epoch [1658/3000], Standardized Loss: 0.0128976, Inverse Loss: 0.0026922
Training -->> Epoch: 1659,(no reg loss)standard loss: 0.0048735, inverse loss: 0.0010173
Valid-->> Epoch [1659/3000], Standardized Loss: 0.0129664, Inverse Loss: 0.0027066
Training -->> Epoch: 1660,(no reg loss)standard loss: 0.0048888, inverse loss: 0.0010205
Valid-->> Epoch [1660/3000], Standardized Loss: 0.0129260, Inverse Loss: 0.0026981
Training -->> Epoch: 1661,(no reg loss)standard loss: 0.0050484, inverse loss: 0.0010538
Valid-->> Epoch [1661/3000], Standardized Loss: 0.0129029, Inverse Loss: 0.0026933
Training -->> Epoch: 1662,(no reg loss)standard loss: 0.0048202, inverse loss: 0.0010062
Valid-->> Epoch [1662/3000], Standardized Loss: 0.0130583, Inverse Loss: 0.0027258
Training -->> Epoch: 1663,(no reg loss)standard loss: 0.0052709, inverse loss: 0.0011002
Valid-->> Epoch [1663/3000], Standardized Loss: 0.0128536, Inverse Loss: 0.0026830
Training -->> Epoch: 1664,(no reg loss)standard loss: 0.0047823, inverse loss: 0.0009983
Valid-->> Epoch [1664/3000], Standardized Loss: 0.0128410, Inverse Loss: 0.0026804
Training -->> Epoch: 1665,(no reg loss)standard loss: 0.0049786, inverse loss: 0.0010392
Valid-->> Epoch [1665/3000], Standardized Loss: 0.0128573, Inverse Loss: 0.0026838
Training -->> Epoch: 1666,(no reg loss)standard loss: 0.0048639, inverse loss: 0.0010153
Valid-->> Epoch [1666/3000], Standardized Loss: 0.0130339, Inverse Loss: 0.0027207
Training -->> Epoch: 1667,(no reg loss)standard loss: 0.0048888, inverse loss: 0.0010205
Valid-->> Epoch [1667/3000], Standardized Loss: 0.0128745, Inverse Loss: 0.0026874
Training -->> Epoch: 1668,(no reg loss)standard loss: 0.0050339, inverse loss: 0.0010508
Valid-->> Epoch [1668/3000], Standardized Loss: 0.0129067, Inverse Loss: 0.0026941
Training -->> Epoch: 1669,(no reg loss)standard loss: 0.0048580, inverse loss: 0.0010141
Valid-->> Epoch [1669/3000], Standardized Loss: 0.0128573, Inverse Loss: 0.0026838
Training -->> Epoch: 1670,(no reg loss)standard loss: 0.0049468, inverse loss: 0.0010326
Valid-->> Epoch [1670/3000], Standardized Loss: 0.0129127, Inverse Loss: 0.0026954
Training -->> Epoch: 1671,(no reg loss)standard loss: 0.0049704, inverse loss: 0.0010375
Valid-->> Epoch [1671/3000], Standardized Loss: 0.0129151, Inverse Loss: 0.0026959
Training -->> Epoch: 1672,(no reg loss)standard loss: 0.0050406, inverse loss: 0.0010522
Valid-->> Epoch [1672/3000], Standardized Loss: 0.0129610, Inverse Loss: 0.0027055
Training -->> Epoch: 1673,(no reg loss)standard loss: 0.0047994, inverse loss: 0.0010018
Valid-->> Epoch [1673/3000], Standardized Loss: 0.0128785, Inverse Loss: 0.0026882
Training -->> Epoch: 1674,(no reg loss)standard loss: 0.0050383, inverse loss: 0.0010517
Valid-->> Epoch [1674/3000], Standardized Loss: 0.0128857, Inverse Loss: 0.0026897
Training -->> Epoch: 1675,(no reg loss)standard loss: 0.0049258, inverse loss: 0.0010282
Valid-->> Epoch [1675/3000], Standardized Loss: 0.0128651, Inverse Loss: 0.0026854
Training -->> Epoch: 1676,(no reg loss)standard loss: 0.0049198, inverse loss: 0.0010269
Valid-->> Epoch [1676/3000], Standardized Loss: 0.0128665, Inverse Loss: 0.0026857
Training -->> Epoch: 1677,(no reg loss)standard loss: 0.0050716, inverse loss: 0.0010586
Valid-->> Epoch [1677/3000], Standardized Loss: 0.0128804, Inverse Loss: 0.0026886
Training -->> Epoch: 1678,(no reg loss)standard loss: 0.0046868, inverse loss: 0.0009783
Valid-->> Epoch [1678/3000], Standardized Loss: 0.0128246, Inverse Loss: 0.0026770
Training -->> Epoch: 1679,(no reg loss)standard loss: 0.0048674, inverse loss: 0.0010160
Valid-->> Epoch [1679/3000], Standardized Loss: 0.0129569, Inverse Loss: 0.0027046
Training -->> Epoch: 1680,(no reg loss)standard loss: 0.0048918, inverse loss: 0.0010211
Valid-->> Epoch [1680/3000], Standardized Loss: 0.0129248, Inverse Loss: 0.0026979
Training -->> Epoch: 1681,(no reg loss)standard loss: 0.0051302, inverse loss: 0.0010709
Valid-->> Epoch [1681/3000], Standardized Loss: 0.0127885, Inverse Loss: 0.0026695
Valid-->> Lowest loss found at epoch 1681, loss: 0.0026695
Epoch 1681, Masked params (inverse standardized): tensor([3.229746e+01, 4.430056e+01, 7.376289e-02, 2.637901e+01, 2.770967e+01,
        1.781587e+01, 2.143732e+01, 1.643995e+00, 1.066111e+02, 2.757882e+01,
        2.617523e+01, 4.462071e+01, 2.444180e+01, 1.504297e+01, 8.461512e+01,
        2.836551e+01, 5.672449e+00, 2.982740e+01, 1.197444e+01, 3.204504e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1682,(no reg loss)standard loss: 0.0048576, inverse loss: 0.0010140
Valid-->> Epoch [1682/3000], Standardized Loss: 0.0129294, Inverse Loss: 0.0026989
Training -->> Epoch: 1683,(no reg loss)standard loss: 0.0048252, inverse loss: 0.0010072
Valid-->> Epoch [1683/3000], Standardized Loss: 0.0129031, Inverse Loss: 0.0026934
Training -->> Epoch: 1684,(no reg loss)standard loss: 0.0051716, inverse loss: 0.0010795
Valid-->> Epoch [1684/3000], Standardized Loss: 0.0129797, Inverse Loss: 0.0027094
Training -->> Epoch: 1685,(no reg loss)standard loss: 0.0047930, inverse loss: 0.0010005
Valid-->> Epoch [1685/3000], Standardized Loss: 0.0128357, Inverse Loss: 0.0026793
Training -->> Epoch: 1686,(no reg loss)standard loss: 0.0050332, inverse loss: 0.0010506
Valid-->> Epoch [1686/3000], Standardized Loss: 0.0128025, Inverse Loss: 0.0026724
Training -->> Epoch: 1687,(no reg loss)standard loss: 0.0049732, inverse loss: 0.0010381
Valid-->> Epoch [1687/3000], Standardized Loss: 0.0128417, Inverse Loss: 0.0026806
Training -->> Epoch: 1688,(no reg loss)standard loss: 0.0047519, inverse loss: 0.0009919
Valid-->> Epoch [1688/3000], Standardized Loss: 0.0128941, Inverse Loss: 0.0026915
Training -->> Epoch: 1689,(no reg loss)standard loss: 0.0050199, inverse loss: 0.0010479
Valid-->> Epoch [1689/3000], Standardized Loss: 0.0128669, Inverse Loss: 0.0026858
Training -->> Epoch: 1690,(no reg loss)standard loss: 0.0048202, inverse loss: 0.0010062
Valid-->> Epoch [1690/3000], Standardized Loss: 0.0129424, Inverse Loss: 0.0027016
Training -->> Epoch: 1691,(no reg loss)standard loss: 0.0050305, inverse loss: 0.0010501
Valid-->> Epoch [1691/3000], Standardized Loss: 0.0129456, Inverse Loss: 0.0027022
Training -->> Epoch: 1692,(no reg loss)standard loss: 0.0047325, inverse loss: 0.0009879
Valid-->> Epoch [1692/3000], Standardized Loss: 0.0127942, Inverse Loss: 0.0026706
Training -->> Epoch: 1693,(no reg loss)standard loss: 0.0049734, inverse loss: 0.0010381
Valid-->> Epoch [1693/3000], Standardized Loss: 0.0129475, Inverse Loss: 0.0027026
Training -->> Epoch: 1694,(no reg loss)standard loss: 0.0050493, inverse loss: 0.0010540
Valid-->> Epoch [1694/3000], Standardized Loss: 0.0128080, Inverse Loss: 0.0026735
Training -->> Epoch: 1695,(no reg loss)standard loss: 0.0048342, inverse loss: 0.0010091
Valid-->> Epoch [1695/3000], Standardized Loss: 0.0129286, Inverse Loss: 0.0026987
Training -->> Epoch: 1696,(no reg loss)standard loss: 0.0049712, inverse loss: 0.0010377
Valid-->> Epoch [1696/3000], Standardized Loss: 0.0128949, Inverse Loss: 0.0026917
Training -->> Epoch: 1697,(no reg loss)standard loss: 0.0050021, inverse loss: 0.0010441
Valid-->> Epoch [1697/3000], Standardized Loss: 0.0128156, Inverse Loss: 0.0026751
Training -->> Epoch: 1698,(no reg loss)standard loss: 0.0050711, inverse loss: 0.0010585
Valid-->> Epoch [1698/3000], Standardized Loss: 0.0128967, Inverse Loss: 0.0026920
Training -->> Epoch: 1699,(no reg loss)standard loss: 0.0047983, inverse loss: 0.0010016
Valid-->> Epoch [1699/3000], Standardized Loss: 0.0128339, Inverse Loss: 0.0026789
Training -->> Epoch: 1700,(no reg loss)standard loss: 0.0051392, inverse loss: 0.0010727
Valid-->> Epoch [1700/3000], Standardized Loss: 0.0128844, Inverse Loss: 0.0026895
Training -->> Epoch: 1701,(no reg loss)standard loss: 0.0048728, inverse loss: 0.0010171
Valid-->> Epoch [1701/3000], Standardized Loss: 0.0129238, Inverse Loss: 0.0026977
Training -->> Epoch: 1702,(no reg loss)standard loss: 0.0050977, inverse loss: 0.0010641
Valid-->> Epoch [1702/3000], Standardized Loss: 0.0128479, Inverse Loss: 0.0026818
Training -->> Epoch: 1703,(no reg loss)standard loss: 0.0048149, inverse loss: 0.0010051
Valid-->> Epoch [1703/3000], Standardized Loss: 0.0127945, Inverse Loss: 0.0026707
Training -->> Epoch: 1704,(no reg loss)standard loss: 0.0050112, inverse loss: 0.0010460
Valid-->> Epoch [1704/3000], Standardized Loss: 0.0128353, Inverse Loss: 0.0026792
Training -->> Epoch: 1705,(no reg loss)standard loss: 0.0050035, inverse loss: 0.0010444
Valid-->> Epoch [1705/3000], Standardized Loss: 0.0128429, Inverse Loss: 0.0026808
Training -->> Epoch: 1706,(no reg loss)standard loss: 0.0047066, inverse loss: 0.0009825
Valid-->> Epoch [1706/3000], Standardized Loss: 0.0127544, Inverse Loss: 0.0026623
Valid-->> Lowest loss found at epoch 1706, loss: 0.0026623
Epoch 1706, Masked params (inverse standardized): tensor([3.229428e+01, 4.430386e+01, 6.471062e-02, 2.638119e+01, 2.771165e+01,
        1.781029e+01, 2.143410e+01, 1.635201e+00, 1.066120e+02, 2.758107e+01,
        2.617723e+01, 4.461265e+01, 2.444185e+01, 1.503652e+01, 8.461736e+01,
        2.836674e+01, 5.663839e+00, 2.982669e+01, 1.196671e+01, 3.204204e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1707,(no reg loss)standard loss: 0.0049980, inverse loss: 0.0010433
Valid-->> Epoch [1707/3000], Standardized Loss: 0.0128604, Inverse Loss: 0.0026845
Training -->> Epoch: 1708,(no reg loss)standard loss: 0.0047877, inverse loss: 0.0009994
Valid-->> Epoch [1708/3000], Standardized Loss: 0.0128161, Inverse Loss: 0.0026752
Training -->> Epoch: 1709,(no reg loss)standard loss: 0.0049981, inverse loss: 0.0010433
Valid-->> Epoch [1709/3000], Standardized Loss: 0.0129882, Inverse Loss: 0.0027111
Training -->> Epoch: 1710,(no reg loss)standard loss: 0.0049586, inverse loss: 0.0010350
Valid-->> Epoch [1710/3000], Standardized Loss: 0.0129096, Inverse Loss: 0.0026947
Training -->> Epoch: 1711,(no reg loss)standard loss: 0.0050141, inverse loss: 0.0010466
Valid-->> Epoch [1711/3000], Standardized Loss: 0.0129583, Inverse Loss: 0.0027049
Training -->> Epoch: 1712,(no reg loss)standard loss: 0.0049789, inverse loss: 0.0010393
Valid-->> Epoch [1712/3000], Standardized Loss: 0.0129034, Inverse Loss: 0.0026934
Training -->> Epoch: 1713,(no reg loss)standard loss: 0.0049935, inverse loss: 0.0010423
Valid-->> Epoch [1713/3000], Standardized Loss: 0.0128964, Inverse Loss: 0.0026920
Training -->> Epoch: 1714,(no reg loss)standard loss: 0.0049956, inverse loss: 0.0010428
Valid-->> Epoch [1714/3000], Standardized Loss: 0.0128972, Inverse Loss: 0.0026921
Training -->> Epoch: 1715,(no reg loss)standard loss: 0.0050591, inverse loss: 0.0010560
Valid-->> Epoch [1715/3000], Standardized Loss: 0.0127887, Inverse Loss: 0.0026695
Training -->> Epoch: 1716,(no reg loss)standard loss: 0.0049977, inverse loss: 0.0010432
Valid-->> Epoch [1716/3000], Standardized Loss: 0.0128402, Inverse Loss: 0.0026803
Training -->> Epoch: 1717,(no reg loss)standard loss: 0.0049518, inverse loss: 0.0010336
Valid-->> Epoch [1717/3000], Standardized Loss: 0.0128596, Inverse Loss: 0.0026843
Training -->> Epoch: 1718,(no reg loss)standard loss: 0.0049705, inverse loss: 0.0010375
Valid-->> Epoch [1718/3000], Standardized Loss: 0.0128474, Inverse Loss: 0.0026817
Training -->> Epoch: 1719,(no reg loss)standard loss: 0.0049687, inverse loss: 0.0010372
Valid-->> Epoch [1719/3000], Standardized Loss: 0.0128864, Inverse Loss: 0.0026899
Training -->> Epoch: 1720,(no reg loss)standard loss: 0.0051250, inverse loss: 0.0010698
Valid-->> Epoch [1720/3000], Standardized Loss: 0.0127782, Inverse Loss: 0.0026673
Training -->> Epoch: 1721,(no reg loss)standard loss: 0.0049201, inverse loss: 0.0010270
Valid-->> Epoch [1721/3000], Standardized Loss: 0.0129198, Inverse Loss: 0.0026969
Training -->> Epoch: 1722,(no reg loss)standard loss: 0.0050447, inverse loss: 0.0010530
Valid-->> Epoch [1722/3000], Standardized Loss: 0.0129230, Inverse Loss: 0.0026975
Training -->> Epoch: 1723,(no reg loss)standard loss: 0.0050000, inverse loss: 0.0010437
Valid-->> Epoch [1723/3000], Standardized Loss: 0.0128022, Inverse Loss: 0.0026723
Training -->> Epoch: 1724,(no reg loss)standard loss: 0.0048429, inverse loss: 0.0010109
Valid-->> Epoch [1724/3000], Standardized Loss: 0.0128421, Inverse Loss: 0.0026806
Training -->> Epoch: 1725,(no reg loss)standard loss: 0.0049710, inverse loss: 0.0010376
Valid-->> Epoch [1725/3000], Standardized Loss: 0.0128109, Inverse Loss: 0.0026741
Training -->> Epoch: 1726,(no reg loss)standard loss: 0.0050159, inverse loss: 0.0010470
Valid-->> Epoch [1726/3000], Standardized Loss: 0.0127917, Inverse Loss: 0.0026701
Training -->> Epoch: 1727,(no reg loss)standard loss: 0.0049175, inverse loss: 0.0010265
Valid-->> Epoch [1727/3000], Standardized Loss: 0.0128505, Inverse Loss: 0.0026824
Training -->> Epoch: 1728,(no reg loss)standard loss: 0.0051019, inverse loss: 0.0010650
Valid-->> Epoch [1728/3000], Standardized Loss: 0.0128101, Inverse Loss: 0.0026740
Training -->> Epoch: 1729,(no reg loss)standard loss: 0.0048879, inverse loss: 0.0010203
Valid-->> Epoch [1729/3000], Standardized Loss: 0.0127913, Inverse Loss: 0.0026700
Training -->> Epoch: 1730,(no reg loss)standard loss: 0.0049529, inverse loss: 0.0010339
Valid-->> Epoch [1730/3000], Standardized Loss: 0.0128201, Inverse Loss: 0.0026760
Training -->> Epoch: 1731,(no reg loss)standard loss: 0.0050080, inverse loss: 0.0010454
Valid-->> Epoch [1731/3000], Standardized Loss: 0.0128689, Inverse Loss: 0.0026862
Training -->> Epoch: 1732,(no reg loss)standard loss: 0.0050655, inverse loss: 0.0010574
Valid-->> Epoch [1732/3000], Standardized Loss: 0.0127938, Inverse Loss: 0.0026706
Training -->> Epoch: 1733,(no reg loss)standard loss: 0.0049237, inverse loss: 0.0010278
Valid-->> Epoch [1733/3000], Standardized Loss: 0.0128258, Inverse Loss: 0.0026772
Training -->> Epoch: 1734,(no reg loss)standard loss: 0.0049375, inverse loss: 0.0010306
Valid-->> Epoch [1734/3000], Standardized Loss: 0.0128376, Inverse Loss: 0.0026797
Training -->> Epoch: 1735,(no reg loss)standard loss: 0.0050536, inverse loss: 0.0010549
Valid-->> Epoch [1735/3000], Standardized Loss: 0.0129331, Inverse Loss: 0.0026996
Training -->> Epoch: 1736,(no reg loss)standard loss: 0.0049697, inverse loss: 0.0010374
Valid-->> Epoch [1736/3000], Standardized Loss: 0.0127493, Inverse Loss: 0.0026613
Valid-->> Lowest loss found at epoch 1736, loss: 0.0026613
Epoch 1736, Masked params (inverse standardized): tensor([3.229671e+01, 4.430177e+01, 6.908607e-02, 2.638113e+01, 2.771143e+01,
        1.781432e+01, 2.143653e+01, 1.639126e+00, 1.066132e+02, 2.758087e+01,
        2.617717e+01, 4.461691e+01, 2.444244e+01, 1.504076e+01, 8.461601e+01,
        2.836669e+01, 5.667622e+00, 2.982773e+01, 1.197131e+01, 3.204433e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1737,(no reg loss)standard loss: 0.0049759, inverse loss: 0.0010387
Valid-->> Epoch [1737/3000], Standardized Loss: 0.0129002, Inverse Loss: 0.0026928
Training -->> Epoch: 1738,(no reg loss)standard loss: 0.0049681, inverse loss: 0.0010370
Valid-->> Epoch [1738/3000], Standardized Loss: 0.0127933, Inverse Loss: 0.0026705
Training -->> Epoch: 1739,(no reg loss)standard loss: 0.0051405, inverse loss: 0.0010730
Valid-->> Epoch [1739/3000], Standardized Loss: 0.0129221, Inverse Loss: 0.0026973
Training -->> Epoch: 1740,(no reg loss)standard loss: 0.0048424, inverse loss: 0.0010108
Valid-->> Epoch [1740/3000], Standardized Loss: 0.0128190, Inverse Loss: 0.0026758
Training -->> Epoch: 1741,(no reg loss)standard loss: 0.0050907, inverse loss: 0.0010626
Valid-->> Epoch [1741/3000], Standardized Loss: 0.0127596, Inverse Loss: 0.0026634
Training -->> Epoch: 1742,(no reg loss)standard loss: 0.0050037, inverse loss: 0.0010445
Valid-->> Epoch [1742/3000], Standardized Loss: 0.0129089, Inverse Loss: 0.0026946
Training -->> Epoch: 1743,(no reg loss)standard loss: 0.0050806, inverse loss: 0.0010605
Valid-->> Epoch [1743/3000], Standardized Loss: 0.0128564, Inverse Loss: 0.0026836
Training -->> Epoch: 1744,(no reg loss)standard loss: 0.0050330, inverse loss: 0.0010506
Valid-->> Epoch [1744/3000], Standardized Loss: 0.0127819, Inverse Loss: 0.0026681
Training -->> Epoch: 1745,(no reg loss)standard loss: 0.0049813, inverse loss: 0.0010398
Valid-->> Epoch [1745/3000], Standardized Loss: 0.0129745, Inverse Loss: 0.0027083
Training -->> Epoch: 1746,(no reg loss)standard loss: 0.0052479, inverse loss: 0.0010954
Valid-->> Epoch [1746/3000], Standardized Loss: 0.0128585, Inverse Loss: 0.0026841
Training -->> Epoch: 1747,(no reg loss)standard loss: 0.0049237, inverse loss: 0.0010278
Valid-->> Epoch [1747/3000], Standardized Loss: 0.0128365, Inverse Loss: 0.0026795
Training -->> Epoch: 1748,(no reg loss)standard loss: 0.0051545, inverse loss: 0.0010759
Valid-->> Epoch [1748/3000], Standardized Loss: 0.0128161, Inverse Loss: 0.0026752
Training -->> Epoch: 1749,(no reg loss)standard loss: 0.0049522, inverse loss: 0.0010337
Valid-->> Epoch [1749/3000], Standardized Loss: 0.0128009, Inverse Loss: 0.0026720
Training -->> Epoch: 1750,(no reg loss)standard loss: 0.0051298, inverse loss: 0.0010708
Valid-->> Epoch [1750/3000], Standardized Loss: 0.0127808, Inverse Loss: 0.0026678
Training -->> Epoch: 1751,(no reg loss)standard loss: 0.0047137, inverse loss: 0.0009839
Valid-->> Epoch [1751/3000], Standardized Loss: 0.0126731, Inverse Loss: 0.0026454
Valid-->> Lowest loss found at epoch 1751, loss: 0.0026454
Epoch 1751, Masked params (inverse standardized): tensor([3.229560e+01, 4.430249e+01, 6.467247e-02, 2.638389e+01, 2.771394e+01,
        1.781141e+01, 2.143534e+01, 1.634933e+00, 1.066111e+02, 2.758349e+01,
        2.617972e+01, 4.461275e+01, 2.444343e+01, 1.503788e+01, 8.461600e+01,
        2.836839e+01, 5.663580e+00, 2.982825e+01, 1.196730e+01, 3.204345e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1752,(no reg loss)standard loss: 0.0049655, inverse loss: 0.0010365
Valid-->> Epoch [1752/3000], Standardized Loss: 0.0128939, Inverse Loss: 0.0026915
Training -->> Epoch: 1753,(no reg loss)standard loss: 0.0050911, inverse loss: 0.0010627
Valid-->> Epoch [1753/3000], Standardized Loss: 0.0128712, Inverse Loss: 0.0026867
Training -->> Epoch: 1754,(no reg loss)standard loss: 0.0051467, inverse loss: 0.0010743
Valid-->> Epoch [1754/3000], Standardized Loss: 0.0128088, Inverse Loss: 0.0026737
Training -->> Epoch: 1755,(no reg loss)standard loss: 0.0047829, inverse loss: 0.0009984
Valid-->> Epoch [1755/3000], Standardized Loss: 0.0127770, Inverse Loss: 0.0026671
Training -->> Epoch: 1756,(no reg loss)standard loss: 0.0051223, inverse loss: 0.0010692
Valid-->> Epoch [1756/3000], Standardized Loss: 0.0128263, Inverse Loss: 0.0026774
Training -->> Epoch: 1757,(no reg loss)standard loss: 0.0048771, inverse loss: 0.0010180
Valid-->> Epoch [1757/3000], Standardized Loss: 0.0127615, Inverse Loss: 0.0026638
Training -->> Epoch: 1758,(no reg loss)standard loss: 0.0051274, inverse loss: 0.0010703
Valid-->> Epoch [1758/3000], Standardized Loss: 0.0128768, Inverse Loss: 0.0026879
Training -->> Epoch: 1759,(no reg loss)standard loss: 0.0050401, inverse loss: 0.0010521
Valid-->> Epoch [1759/3000], Standardized Loss: 0.0127746, Inverse Loss: 0.0026666
Training -->> Epoch: 1760,(no reg loss)standard loss: 0.0050239, inverse loss: 0.0010487
Valid-->> Epoch [1760/3000], Standardized Loss: 0.0127995, Inverse Loss: 0.0026717
Training -->> Epoch: 1761,(no reg loss)standard loss: 0.0049961, inverse loss: 0.0010429
Valid-->> Epoch [1761/3000], Standardized Loss: 0.0127702, Inverse Loss: 0.0026656
Training -->> Epoch: 1762,(no reg loss)standard loss: 0.0050750, inverse loss: 0.0010593
Valid-->> Epoch [1762/3000], Standardized Loss: 0.0129242, Inverse Loss: 0.0026978
Training -->> Epoch: 1763,(no reg loss)standard loss: 0.0050587, inverse loss: 0.0010559
Valid-->> Epoch [1763/3000], Standardized Loss: 0.0127546, Inverse Loss: 0.0026624
Training -->> Epoch: 1764,(no reg loss)standard loss: 0.0052278, inverse loss: 0.0010912
Valid-->> Epoch [1764/3000], Standardized Loss: 0.0126691, Inverse Loss: 0.0026445
Valid-->> Lowest loss found at epoch 1764, loss: 0.0026445
Epoch 1764, Masked params (inverse standardized): tensor([3.229852e+01, 4.430183e+01, 7.495308e-02, 2.638305e+01, 2.771317e+01,
        1.781652e+01, 2.143836e+01, 1.644594e+00, 1.066131e+02, 2.758274e+01,
        2.617895e+01, 4.462095e+01, 2.444415e+01, 1.504351e+01, 8.461620e+01,
        2.836831e+01, 5.673635e+00, 2.982950e+01, 1.197477e+01, 3.204615e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1765,(no reg loss)standard loss: 0.0048938, inverse loss: 0.0010215
Valid-->> Epoch [1765/3000], Standardized Loss: 0.0128552, Inverse Loss: 0.0026834
Training -->> Epoch: 1766,(no reg loss)standard loss: 0.0052451, inverse loss: 0.0010948
Valid-->> Epoch [1766/3000], Standardized Loss: 0.0127850, Inverse Loss: 0.0026687
Training -->> Epoch: 1767,(no reg loss)standard loss: 0.0048493, inverse loss: 0.0010122
Valid-->> Epoch [1767/3000], Standardized Loss: 0.0128345, Inverse Loss: 0.0026791
Training -->> Epoch: 1768,(no reg loss)standard loss: 0.0051723, inverse loss: 0.0010797
Valid-->> Epoch [1768/3000], Standardized Loss: 0.0128309, Inverse Loss: 0.0026783
Training -->> Epoch: 1769,(no reg loss)standard loss: 0.0050978, inverse loss: 0.0010641
Valid-->> Epoch [1769/3000], Standardized Loss: 0.0128424, Inverse Loss: 0.0026807
Training -->> Epoch: 1770,(no reg loss)standard loss: 0.0050104, inverse loss: 0.0010459
Valid-->> Epoch [1770/3000], Standardized Loss: 0.0127697, Inverse Loss: 0.0026655
Training -->> Epoch: 1771,(no reg loss)standard loss: 0.0048568, inverse loss: 0.0010138
Valid-->> Epoch [1771/3000], Standardized Loss: 0.0127838, Inverse Loss: 0.0026685
Training -->> Epoch: 1772,(no reg loss)standard loss: 0.0051644, inverse loss: 0.0010780
Valid-->> Epoch [1772/3000], Standardized Loss: 0.0127853, Inverse Loss: 0.0026688
Training -->> Epoch: 1773,(no reg loss)standard loss: 0.0049322, inverse loss: 0.0010295
Valid-->> Epoch [1773/3000], Standardized Loss: 0.0127901, Inverse Loss: 0.0026698
Training -->> Epoch: 1774,(no reg loss)standard loss: 0.0050363, inverse loss: 0.0010513
Valid-->> Epoch [1774/3000], Standardized Loss: 0.0128059, Inverse Loss: 0.0026731
Training -->> Epoch: 1775,(no reg loss)standard loss: 0.0050958, inverse loss: 0.0010637
Valid-->> Epoch [1775/3000], Standardized Loss: 0.0128822, Inverse Loss: 0.0026890
Training -->> Epoch: 1776,(no reg loss)standard loss: 0.0051257, inverse loss: 0.0010699
Valid-->> Epoch [1776/3000], Standardized Loss: 0.0128339, Inverse Loss: 0.0026789
Training -->> Epoch: 1777,(no reg loss)standard loss: 0.0049130, inverse loss: 0.0010255
Valid-->> Epoch [1777/3000], Standardized Loss: 0.0127659, Inverse Loss: 0.0026647
Training -->> Epoch: 1778,(no reg loss)standard loss: 0.0050187, inverse loss: 0.0010476
Valid-->> Epoch [1778/3000], Standardized Loss: 0.0127606, Inverse Loss: 0.0026636
Training -->> Epoch: 1779,(no reg loss)standard loss: 0.0049936, inverse loss: 0.0010424
Valid-->> Epoch [1779/3000], Standardized Loss: 0.0128179, Inverse Loss: 0.0026756
Training -->> Epoch: 1780,(no reg loss)standard loss: 0.0049832, inverse loss: 0.0010402
Valid-->> Epoch [1780/3000], Standardized Loss: 0.0128761, Inverse Loss: 0.0026877
Training -->> Epoch: 1781,(no reg loss)standard loss: 0.0052095, inverse loss: 0.0010874
Valid-->> Epoch [1781/3000], Standardized Loss: 0.0129006, Inverse Loss: 0.0026928
Training -->> Epoch: 1782,(no reg loss)standard loss: 0.0049381, inverse loss: 0.0010308
Valid-->> Epoch [1782/3000], Standardized Loss: 0.0127467, Inverse Loss: 0.0026607
Training -->> Epoch: 1783,(no reg loss)standard loss: 0.0051099, inverse loss: 0.0010666
Valid-->> Epoch [1783/3000], Standardized Loss: 0.0128493, Inverse Loss: 0.0026822
Training -->> Epoch: 1784,(no reg loss)standard loss: 0.0048282, inverse loss: 0.0010078
Valid-->> Epoch [1784/3000], Standardized Loss: 0.0126713, Inverse Loss: 0.0026450
Training -->> Epoch: 1785,(no reg loss)standard loss: 0.0050214, inverse loss: 0.0010482
Valid-->> Epoch [1785/3000], Standardized Loss: 0.0129035, Inverse Loss: 0.0026935
Training -->> Epoch: 1786,(no reg loss)standard loss: 0.0051424, inverse loss: 0.0010734
Valid-->> Epoch [1786/3000], Standardized Loss: 0.0127856, Inverse Loss: 0.0026689
Training -->> Epoch: 1787,(no reg loss)standard loss: 0.0050475, inverse loss: 0.0010536
Valid-->> Epoch [1787/3000], Standardized Loss: 0.0127888, Inverse Loss: 0.0026695
Training -->> Epoch: 1788,(no reg loss)standard loss: 0.0049659, inverse loss: 0.0010366
Valid-->> Epoch [1788/3000], Standardized Loss: 0.0128000, Inverse Loss: 0.0026719
Training -->> Epoch: 1789,(no reg loss)standard loss: 0.0050544, inverse loss: 0.0010551
Valid-->> Epoch [1789/3000], Standardized Loss: 0.0128346, Inverse Loss: 0.0026791
Training -->> Epoch: 1790,(no reg loss)standard loss: 0.0051457, inverse loss: 0.0010741
Valid-->> Epoch [1790/3000], Standardized Loss: 0.0127927, Inverse Loss: 0.0026703
Training -->> Epoch: 1791,(no reg loss)standard loss: 0.0049905, inverse loss: 0.0010417
Valid-->> Epoch [1791/3000], Standardized Loss: 0.0128091, Inverse Loss: 0.0026738
Training -->> Epoch: 1792,(no reg loss)standard loss: 0.0049562, inverse loss: 0.0010345
Valid-->> Epoch [1792/3000], Standardized Loss: 0.0127177, Inverse Loss: 0.0026547
Training -->> Epoch: 1793,(no reg loss)standard loss: 0.0051338, inverse loss: 0.0010716
Valid-->> Epoch [1793/3000], Standardized Loss: 0.0128585, Inverse Loss: 0.0026841
Training -->> Epoch: 1794,(no reg loss)standard loss: 0.0050915, inverse loss: 0.0010628
Valid-->> Epoch [1794/3000], Standardized Loss: 0.0127673, Inverse Loss: 0.0026650
Training -->> Epoch: 1795,(no reg loss)standard loss: 0.0050778, inverse loss: 0.0010599
Valid-->> Epoch [1795/3000], Standardized Loss: 0.0128418, Inverse Loss: 0.0026806
Training -->> Epoch: 1796,(no reg loss)standard loss: 0.0050930, inverse loss: 0.0010631
Valid-->> Epoch [1796/3000], Standardized Loss: 0.0126971, Inverse Loss: 0.0026504
Training -->> Epoch: 1797,(no reg loss)standard loss: 0.0050986, inverse loss: 0.0010643
Valid-->> Epoch [1797/3000], Standardized Loss: 0.0128638, Inverse Loss: 0.0026852
Training -->> Epoch: 1798,(no reg loss)standard loss: 0.0050256, inverse loss: 0.0010490
Valid-->> Epoch [1798/3000], Standardized Loss: 0.0127368, Inverse Loss: 0.0026587
Training -->> Epoch: 1799,(no reg loss)standard loss: 0.0049252, inverse loss: 0.0010281
Valid-->> Epoch [1799/3000], Standardized Loss: 0.0128054, Inverse Loss: 0.0026730
Training -->> Epoch: 1800,(no reg loss)standard loss: 0.0052434, inverse loss: 0.0010945
Valid-->> Epoch [1800/3000], Standardized Loss: 0.0127730, Inverse Loss: 0.0026662
Training -->> Epoch: 1801,(no reg loss)standard loss: 0.0050152, inverse loss: 0.0010469
Valid-->> Epoch [1801/3000], Standardized Loss: 0.0127479, Inverse Loss: 0.0026610
Training -->> Epoch: 1802,(no reg loss)standard loss: 0.0050570, inverse loss: 0.0010556
Valid-->> Epoch [1802/3000], Standardized Loss: 0.0127359, Inverse Loss: 0.0026585
Training -->> Epoch: 1803,(no reg loss)standard loss: 0.0051685, inverse loss: 0.0010789
Valid-->> Epoch [1803/3000], Standardized Loss: 0.0128978, Inverse Loss: 0.0026923
Training -->> Epoch: 1804,(no reg loss)standard loss: 0.0051333, inverse loss: 0.0010715
Valid-->> Epoch [1804/3000], Standardized Loss: 0.0127148, Inverse Loss: 0.0026541
Training -->> Epoch: 1805,(no reg loss)standard loss: 0.0050698, inverse loss: 0.0010583
Valid-->> Epoch [1805/3000], Standardized Loss: 0.0127993, Inverse Loss: 0.0026717
Training -->> Epoch: 1806,(no reg loss)standard loss: 0.0051135, inverse loss: 0.0010674
Valid-->> Epoch [1806/3000], Standardized Loss: 0.0127765, Inverse Loss: 0.0026670
Training -->> Epoch: 1807,(no reg loss)standard loss: 0.0052048, inverse loss: 0.0010864
Valid-->> Epoch [1807/3000], Standardized Loss: 0.0127477, Inverse Loss: 0.0026609
Training -->> Epoch: 1808,(no reg loss)standard loss: 0.0049963, inverse loss: 0.0010429
Valid-->> Epoch [1808/3000], Standardized Loss: 0.0128141, Inverse Loss: 0.0026748
Training -->> Epoch: 1809,(no reg loss)standard loss: 0.0051161, inverse loss: 0.0010679
Valid-->> Epoch [1809/3000], Standardized Loss: 0.0126980, Inverse Loss: 0.0026506
Training -->> Epoch: 1810,(no reg loss)standard loss: 0.0050648, inverse loss: 0.0010572
Valid-->> Epoch [1810/3000], Standardized Loss: 0.0127522, Inverse Loss: 0.0026619
Training -->> Epoch: 1811,(no reg loss)standard loss: 0.0050648, inverse loss: 0.0010572
Valid-->> Epoch [1811/3000], Standardized Loss: 0.0128634, Inverse Loss: 0.0026851
Training -->> Epoch: 1812,(no reg loss)standard loss: 0.0053115, inverse loss: 0.0011087
Valid-->> Epoch [1812/3000], Standardized Loss: 0.0127748, Inverse Loss: 0.0026666
Training -->> Epoch: 1813,(no reg loss)standard loss: 0.0050727, inverse loss: 0.0010589
Valid-->> Epoch [1813/3000], Standardized Loss: 0.0127512, Inverse Loss: 0.0026617
Training -->> Epoch: 1814,(no reg loss)standard loss: 0.0052222, inverse loss: 0.0010901
Valid-->> Epoch [1814/3000], Standardized Loss: 0.0128173, Inverse Loss: 0.0026755
Training -->> Epoch: 1815,(no reg loss)standard loss: 0.0049531, inverse loss: 0.0010339
Valid-->> Epoch [1815/3000], Standardized Loss: 0.0127145, Inverse Loss: 0.0026540
Training -->> Epoch: 1816,(no reg loss)standard loss: 0.0051571, inverse loss: 0.0010765
Valid-->> Epoch [1816/3000], Standardized Loss: 0.0128788, Inverse Loss: 0.0026883
Training -->> Epoch: 1817,(no reg loss)standard loss: 0.0051936, inverse loss: 0.0010841
Valid-->> Epoch [1817/3000], Standardized Loss: 0.0128000, Inverse Loss: 0.0026718
Training -->> Epoch: 1818,(no reg loss)standard loss: 0.0051786, inverse loss: 0.0010810
Valid-->> Epoch [1818/3000], Standardized Loss: 0.0127922, Inverse Loss: 0.0026702
Training -->> Epoch: 1819,(no reg loss)standard loss: 0.0050692, inverse loss: 0.0010581
Valid-->> Epoch [1819/3000], Standardized Loss: 0.0127842, Inverse Loss: 0.0026685
Training -->> Epoch: 1820,(no reg loss)standard loss: 0.0051501, inverse loss: 0.0010750
Valid-->> Epoch [1820/3000], Standardized Loss: 0.0127470, Inverse Loss: 0.0026608
Training -->> Epoch: 1821,(no reg loss)standard loss: 0.0051816, inverse loss: 0.0010816
Valid-->> Epoch [1821/3000], Standardized Loss: 0.0127550, Inverse Loss: 0.0026625
Training -->> Epoch: 1822,(no reg loss)standard loss: 0.0051166, inverse loss: 0.0010680
Valid-->> Epoch [1822/3000], Standardized Loss: 0.0127389, Inverse Loss: 0.0026591
Training -->> Epoch: 1823,(no reg loss)standard loss: 0.0051204, inverse loss: 0.0010688
Valid-->> Epoch [1823/3000], Standardized Loss: 0.0127603, Inverse Loss: 0.0026636
Training -->> Epoch: 1824,(no reg loss)standard loss: 0.0052275, inverse loss: 0.0010912
Valid-->> Epoch [1824/3000], Standardized Loss: 0.0127616, Inverse Loss: 0.0026638
Training -->> Epoch: 1825,(no reg loss)standard loss: 0.0051032, inverse loss: 0.0010652
Valid-->> Epoch [1825/3000], Standardized Loss: 0.0127345, Inverse Loss: 0.0026582
Training -->> Epoch: 1826,(no reg loss)standard loss: 0.0050393, inverse loss: 0.0010519
Valid-->> Epoch [1826/3000], Standardized Loss: 0.0128100, Inverse Loss: 0.0026739
Training -->> Epoch: 1827,(no reg loss)standard loss: 0.0051326, inverse loss: 0.0010714
Valid-->> Epoch [1827/3000], Standardized Loss: 0.0127214, Inverse Loss: 0.0026554
Training -->> Epoch: 1828,(no reg loss)standard loss: 0.0051769, inverse loss: 0.0010806
Valid-->> Epoch [1828/3000], Standardized Loss: 0.0127478, Inverse Loss: 0.0026610
Training -->> Epoch: 1829,(no reg loss)standard loss: 0.0049042, inverse loss: 0.0010237
Valid-->> Epoch [1829/3000], Standardized Loss: 0.0126796, Inverse Loss: 0.0026467
Training -->> Epoch: 1830,(no reg loss)standard loss: 0.0051675, inverse loss: 0.0010787
Valid-->> Epoch [1830/3000], Standardized Loss: 0.0128604, Inverse Loss: 0.0026845
Training -->> Epoch: 1831,(no reg loss)standard loss: 0.0051632, inverse loss: 0.0010778
Valid-->> Epoch [1831/3000], Standardized Loss: 0.0127294, Inverse Loss: 0.0026571
Training -->> Epoch: 1832,(no reg loss)standard loss: 0.0051156, inverse loss: 0.0010678
Valid-->> Epoch [1832/3000], Standardized Loss: 0.0127427, Inverse Loss: 0.0026599
Training -->> Epoch: 1833,(no reg loss)standard loss: 0.0051019, inverse loss: 0.0010650
Valid-->> Epoch [1833/3000], Standardized Loss: 0.0126561, Inverse Loss: 0.0026418
Valid-->> Lowest loss found at epoch 1833, loss: 0.0026418
Epoch 1833, Masked params (inverse standardized): tensor([3.229667e+01, 4.430129e+01, 7.031631e-02, 2.638526e+01, 2.771459e+01,
        1.781388e+01, 2.143637e+01, 1.641016e+00, 1.066122e+02, 2.758475e+01,
        2.618070e+01, 4.461778e+01, 2.444360e+01, 1.504113e+01, 8.461580e+01,
        2.836843e+01, 5.668869e+00, 2.982876e+01, 1.197195e+01, 3.204422e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1834,(no reg loss)standard loss: 0.0051477, inverse loss: 0.0010745
Valid-->> Epoch [1834/3000], Standardized Loss: 0.0128756, Inverse Loss: 0.0026876
Training -->> Epoch: 1835,(no reg loss)standard loss: 0.0051636, inverse loss: 0.0010778
Valid-->> Epoch [1835/3000], Standardized Loss: 0.0127641, Inverse Loss: 0.0026644
Training -->> Epoch: 1836,(no reg loss)standard loss: 0.0051300, inverse loss: 0.0010708
Valid-->> Epoch [1836/3000], Standardized Loss: 0.0128303, Inverse Loss: 0.0026782
Training -->> Epoch: 1837,(no reg loss)standard loss: 0.0052866, inverse loss: 0.0011035
Valid-->> Epoch [1837/3000], Standardized Loss: 0.0128501, Inverse Loss: 0.0026823
Training -->> Epoch: 1838,(no reg loss)standard loss: 0.0050681, inverse loss: 0.0010579
Valid-->> Epoch [1838/3000], Standardized Loss: 0.0127019, Inverse Loss: 0.0026514
Training -->> Epoch: 1839,(no reg loss)standard loss: 0.0052491, inverse loss: 0.0010957
Valid-->> Epoch [1839/3000], Standardized Loss: 0.0127585, Inverse Loss: 0.0026632
Training -->> Epoch: 1840,(no reg loss)standard loss: 0.0051313, inverse loss: 0.0010711
Valid-->> Epoch [1840/3000], Standardized Loss: 0.0127526, Inverse Loss: 0.0026620
Training -->> Epoch: 1841,(no reg loss)standard loss: 0.0050176, inverse loss: 0.0010474
Valid-->> Epoch [1841/3000], Standardized Loss: 0.0127811, Inverse Loss: 0.0026679
Training -->> Epoch: 1842,(no reg loss)standard loss: 0.0052689, inverse loss: 0.0010998
Valid-->> Epoch [1842/3000], Standardized Loss: 0.0127499, Inverse Loss: 0.0026614
Training -->> Epoch: 1843,(no reg loss)standard loss: 0.0052575, inverse loss: 0.0010974
Valid-->> Epoch [1843/3000], Standardized Loss: 0.0128405, Inverse Loss: 0.0026803
Training -->> Epoch: 1844,(no reg loss)standard loss: 0.0052259, inverse loss: 0.0010909
Valid-->> Epoch [1844/3000], Standardized Loss: 0.0126816, Inverse Loss: 0.0026471
Training -->> Epoch: 1845,(no reg loss)standard loss: 0.0052257, inverse loss: 0.0010908
Valid-->> Epoch [1845/3000], Standardized Loss: 0.0127893, Inverse Loss: 0.0026696
Training -->> Epoch: 1846,(no reg loss)standard loss: 0.0052146, inverse loss: 0.0010885
Valid-->> Epoch [1846/3000], Standardized Loss: 0.0127324, Inverse Loss: 0.0026577
Training -->> Epoch: 1847,(no reg loss)standard loss: 0.0049653, inverse loss: 0.0010364
Valid-->> Epoch [1847/3000], Standardized Loss: 0.0127301, Inverse Loss: 0.0026573
Training -->> Epoch: 1848,(no reg loss)standard loss: 0.0052334, inverse loss: 0.0010924
Valid-->> Epoch [1848/3000], Standardized Loss: 0.0129385, Inverse Loss: 0.0027008
Training -->> Epoch: 1849,(no reg loss)standard loss: 0.0054006, inverse loss: 0.0011273
Valid-->> Epoch [1849/3000], Standardized Loss: 0.0127893, Inverse Loss: 0.0026696
Training -->> Epoch: 1850,(no reg loss)standard loss: 0.0051411, inverse loss: 0.0010731
Valid-->> Epoch [1850/3000], Standardized Loss: 0.0126586, Inverse Loss: 0.0026423
Training -->> Epoch: 1851,(no reg loss)standard loss: 0.0048724, inverse loss: 0.0010171
Valid-->> Epoch [1851/3000], Standardized Loss: 0.0126520, Inverse Loss: 0.0026410
Valid-->> Lowest loss found at epoch 1851, loss: 0.0026410
Epoch 1851, Masked params (inverse standardized): tensor([3.229378e+01, 4.430329e+01, 6.476212e-02, 2.638646e+01, 2.771572e+01,
        1.780932e+01, 2.143343e+01, 1.634808e+00, 1.066127e+02, 2.758603e+01,
        2.618175e+01, 4.461245e+01, 2.444320e+01, 1.503626e+01, 8.461624e+01,
        2.836894e+01, 5.663761e+00, 2.982776e+01, 1.196663e+01, 3.204158e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1852,(no reg loss)standard loss: 0.0052762, inverse loss: 0.0011014
Valid-->> Epoch [1852/3000], Standardized Loss: 0.0127911, Inverse Loss: 0.0026700
Training -->> Epoch: 1853,(no reg loss)standard loss: 0.0052304, inverse loss: 0.0010918
Valid-->> Epoch [1853/3000], Standardized Loss: 0.0127352, Inverse Loss: 0.0026583
Training -->> Epoch: 1854,(no reg loss)standard loss: 0.0052724, inverse loss: 0.0011006
Valid-->> Epoch [1854/3000], Standardized Loss: 0.0127675, Inverse Loss: 0.0026651
Training -->> Epoch: 1855,(no reg loss)standard loss: 0.0051859, inverse loss: 0.0010825
Valid-->> Epoch [1855/3000], Standardized Loss: 0.0128230, Inverse Loss: 0.0026767
Training -->> Epoch: 1856,(no reg loss)standard loss: 0.0052186, inverse loss: 0.0010893
Valid-->> Epoch [1856/3000], Standardized Loss: 0.0126906, Inverse Loss: 0.0026490
Training -->> Epoch: 1857,(no reg loss)standard loss: 0.0051130, inverse loss: 0.0010673
Valid-->> Epoch [1857/3000], Standardized Loss: 0.0127475, Inverse Loss: 0.0026609
Training -->> Epoch: 1858,(no reg loss)standard loss: 0.0053815, inverse loss: 0.0011233
Valid-->> Epoch [1858/3000], Standardized Loss: 0.0126493, Inverse Loss: 0.0026404
Valid-->> Lowest loss found at epoch 1858, loss: 0.0026404
Epoch 1858, Masked params (inverse standardized): tensor([3.229779e+01, 4.430095e+01, 7.386208e-02, 2.638546e+01, 2.771480e+01,
        1.781605e+01, 2.143756e+01, 1.643509e+00, 1.066123e+02, 2.758500e+01,
        2.618088e+01, 4.462098e+01, 2.444379e+01, 1.504355e+01, 8.461543e+01,
        2.836860e+01, 5.672695e+00, 2.982903e+01, 1.197486e+01, 3.204532e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1859,(no reg loss)standard loss: 0.0051213, inverse loss: 0.0010690
Valid-->> Epoch [1859/3000], Standardized Loss: 0.0126863, Inverse Loss: 0.0026481
Training -->> Epoch: 1860,(no reg loss)standard loss: 0.0051541, inverse loss: 0.0010759
Valid-->> Epoch [1860/3000], Standardized Loss: 0.0128278, Inverse Loss: 0.0026776
Training -->> Epoch: 1861,(no reg loss)standard loss: 0.0051784, inverse loss: 0.0010809
Valid-->> Epoch [1861/3000], Standardized Loss: 0.0127585, Inverse Loss: 0.0026632
Training -->> Epoch: 1862,(no reg loss)standard loss: 0.0053575, inverse loss: 0.0011183
Valid-->> Epoch [1862/3000], Standardized Loss: 0.0128033, Inverse Loss: 0.0026725
Training -->> Epoch: 1863,(no reg loss)standard loss: 0.0051886, inverse loss: 0.0010831
Valid-->> Epoch [1863/3000], Standardized Loss: 0.0127037, Inverse Loss: 0.0026518
Training -->> Epoch: 1864,(no reg loss)standard loss: 0.0051517, inverse loss: 0.0010754
Valid-->> Epoch [1864/3000], Standardized Loss: 0.0127471, Inverse Loss: 0.0026608
Training -->> Epoch: 1865,(no reg loss)standard loss: 0.0053256, inverse loss: 0.0011117
Valid-->> Epoch [1865/3000], Standardized Loss: 0.0128648, Inverse Loss: 0.0026854
Training -->> Epoch: 1866,(no reg loss)standard loss: 0.0052547, inverse loss: 0.0010969
Valid-->> Epoch [1866/3000], Standardized Loss: 0.0127468, Inverse Loss: 0.0026607
Training -->> Epoch: 1867,(no reg loss)standard loss: 0.0050515, inverse loss: 0.0010544
Valid-->> Epoch [1867/3000], Standardized Loss: 0.0126991, Inverse Loss: 0.0026508
Training -->> Epoch: 1868,(no reg loss)standard loss: 0.0053081, inverse loss: 0.0011080
Valid-->> Epoch [1868/3000], Standardized Loss: 0.0128572, Inverse Loss: 0.0026838
Training -->> Epoch: 1869,(no reg loss)standard loss: 0.0051959, inverse loss: 0.0010846
Valid-->> Epoch [1869/3000], Standardized Loss: 0.0126891, Inverse Loss: 0.0026487
Training -->> Epoch: 1870,(no reg loss)standard loss: 0.0054434, inverse loss: 0.0011363
Valid-->> Epoch [1870/3000], Standardized Loss: 0.0127912, Inverse Loss: 0.0026700
Training -->> Epoch: 1871,(no reg loss)standard loss: 0.0051610, inverse loss: 0.0010773
Valid-->> Epoch [1871/3000], Standardized Loss: 0.0126997, Inverse Loss: 0.0026509
Training -->> Epoch: 1872,(no reg loss)standard loss: 0.0053096, inverse loss: 0.0011083
Valid-->> Epoch [1872/3000], Standardized Loss: 0.0127044, Inverse Loss: 0.0026519
Training -->> Epoch: 1873,(no reg loss)standard loss: 0.0052051, inverse loss: 0.0010865
Valid-->> Epoch [1873/3000], Standardized Loss: 0.0128158, Inverse Loss: 0.0026752
Training -->> Epoch: 1874,(no reg loss)standard loss: 0.0052875, inverse loss: 0.0011037
Valid-->> Epoch [1874/3000], Standardized Loss: 0.0127467, Inverse Loss: 0.0026607
Training -->> Epoch: 1875,(no reg loss)standard loss: 0.0052348, inverse loss: 0.0010927
Valid-->> Epoch [1875/3000], Standardized Loss: 0.0127900, Inverse Loss: 0.0026698
Training -->> Epoch: 1876,(no reg loss)standard loss: 0.0052187, inverse loss: 0.0010893
Valid-->> Epoch [1876/3000], Standardized Loss: 0.0127288, Inverse Loss: 0.0026570
Training -->> Epoch: 1877,(no reg loss)standard loss: 0.0052749, inverse loss: 0.0011011
Valid-->> Epoch [1877/3000], Standardized Loss: 0.0127679, Inverse Loss: 0.0026652
Training -->> Epoch: 1878,(no reg loss)standard loss: 0.0052813, inverse loss: 0.0011024
Valid-->> Epoch [1878/3000], Standardized Loss: 0.0127056, Inverse Loss: 0.0026521
Training -->> Epoch: 1879,(no reg loss)standard loss: 0.0050978, inverse loss: 0.0010641
Valid-->> Epoch [1879/3000], Standardized Loss: 0.0127217, Inverse Loss: 0.0026555
Training -->> Epoch: 1880,(no reg loss)standard loss: 0.0052714, inverse loss: 0.0011003
Valid-->> Epoch [1880/3000], Standardized Loss: 0.0129331, Inverse Loss: 0.0026996
Training -->> Epoch: 1881,(no reg loss)standard loss: 0.0054254, inverse loss: 0.0011325
Valid-->> Epoch [1881/3000], Standardized Loss: 0.0126593, Inverse Loss: 0.0026425
Training -->> Epoch: 1882,(no reg loss)standard loss: 0.0049470, inverse loss: 0.0010326
Valid-->> Epoch [1882/3000], Standardized Loss: 0.0126515, Inverse Loss: 0.0026409
Training -->> Epoch: 1883,(no reg loss)standard loss: 0.0053737, inverse loss: 0.0011217
Valid-->> Epoch [1883/3000], Standardized Loss: 0.0129121, Inverse Loss: 0.0026953
Training -->> Epoch: 1884,(no reg loss)standard loss: 0.0052819, inverse loss: 0.0011025
Valid-->> Epoch [1884/3000], Standardized Loss: 0.0127095, Inverse Loss: 0.0026530
Training -->> Epoch: 1885,(no reg loss)standard loss: 0.0053952, inverse loss: 0.0011262
Valid-->> Epoch [1885/3000], Standardized Loss: 0.0127623, Inverse Loss: 0.0026640
Training -->> Epoch: 1886,(no reg loss)standard loss: 0.0052452, inverse loss: 0.0010949
Valid-->> Epoch [1886/3000], Standardized Loss: 0.0126601, Inverse Loss: 0.0026426
Training -->> Epoch: 1887,(no reg loss)standard loss: 0.0051976, inverse loss: 0.0010849
Valid-->> Epoch [1887/3000], Standardized Loss: 0.0127332, Inverse Loss: 0.0026579
Training -->> Epoch: 1888,(no reg loss)standard loss: 0.0052529, inverse loss: 0.0010965
Valid-->> Epoch [1888/3000], Standardized Loss: 0.0127224, Inverse Loss: 0.0026556
Training -->> Epoch: 1889,(no reg loss)standard loss: 0.0052680, inverse loss: 0.0010996
Valid-->> Epoch [1889/3000], Standardized Loss: 0.0126745, Inverse Loss: 0.0026456
Training -->> Epoch: 1890,(no reg loss)standard loss: 0.0052368, inverse loss: 0.0010931
Valid-->> Epoch [1890/3000], Standardized Loss: 0.0128118, Inverse Loss: 0.0026743
Training -->> Epoch: 1891,(no reg loss)standard loss: 0.0052527, inverse loss: 0.0010964
Valid-->> Epoch [1891/3000], Standardized Loss: 0.0126750, Inverse Loss: 0.0026458
Training -->> Epoch: 1892,(no reg loss)standard loss: 0.0053609, inverse loss: 0.0011190
Valid-->> Epoch [1892/3000], Standardized Loss: 0.0127377, Inverse Loss: 0.0026589
Training -->> Epoch: 1893,(no reg loss)standard loss: 0.0051929, inverse loss: 0.0010840
Valid-->> Epoch [1893/3000], Standardized Loss: 0.0127947, Inverse Loss: 0.0026707
Training -->> Epoch: 1894,(no reg loss)standard loss: 0.0053863, inverse loss: 0.0011243
Valid-->> Epoch [1894/3000], Standardized Loss: 0.0127040, Inverse Loss: 0.0026518
Training -->> Epoch: 1895,(no reg loss)standard loss: 0.0052260, inverse loss: 0.0010909
Valid-->> Epoch [1895/3000], Standardized Loss: 0.0128012, Inverse Loss: 0.0026721
Training -->> Epoch: 1896,(no reg loss)standard loss: 0.0054444, inverse loss: 0.0011365
Valid-->> Epoch [1896/3000], Standardized Loss: 0.0127115, Inverse Loss: 0.0026534
Training -->> Epoch: 1897,(no reg loss)standard loss: 0.0054141, inverse loss: 0.0011301
Valid-->> Epoch [1897/3000], Standardized Loss: 0.0127158, Inverse Loss: 0.0026543
Training -->> Epoch: 1898,(no reg loss)standard loss: 0.0052106, inverse loss: 0.0010876
Valid-->> Epoch [1898/3000], Standardized Loss: 0.0127420, Inverse Loss: 0.0026598
Training -->> Epoch: 1899,(no reg loss)standard loss: 0.0053015, inverse loss: 0.0011066
Valid-->> Epoch [1899/3000], Standardized Loss: 0.0126607, Inverse Loss: 0.0026428
Training -->> Epoch: 1900,(no reg loss)standard loss: 0.0051988, inverse loss: 0.0010852
Valid-->> Epoch [1900/3000], Standardized Loss: 0.0126437, Inverse Loss: 0.0026392
Valid-->> Lowest loss found at epoch 1900, loss: 0.0026392
Epoch 1900, Masked params (inverse standardized): tensor([3.229603e+01, 4.430266e+01, 6.975555e-02, 2.638707e+01, 2.771604e+01,
        1.781285e+01, 2.143573e+01, 1.639071e+00, 1.066123e+02, 2.758650e+01,
        2.618232e+01, 4.461623e+01, 2.444353e+01, 1.503974e+01, 8.461701e+01,
        2.836907e+01, 5.668703e+00, 2.982845e+01, 1.197023e+01, 3.204360e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1901,(no reg loss)standard loss: 0.0051169, inverse loss: 0.0010681
Valid-->> Epoch [1901/3000], Standardized Loss: 0.0127155, Inverse Loss: 0.0026542
Training -->> Epoch: 1902,(no reg loss)standard loss: 0.0053495, inverse loss: 0.0011166
Valid-->> Epoch [1902/3000], Standardized Loss: 0.0128929, Inverse Loss: 0.0026912
Training -->> Epoch: 1903,(no reg loss)standard loss: 0.0054270, inverse loss: 0.0011328
Valid-->> Epoch [1903/3000], Standardized Loss: 0.0127937, Inverse Loss: 0.0026705
Training -->> Epoch: 1904,(no reg loss)standard loss: 0.0052726, inverse loss: 0.0011006
Valid-->> Epoch [1904/3000], Standardized Loss: 0.0126375, Inverse Loss: 0.0026379
Valid-->> Lowest loss found at epoch 1904, loss: 0.0026379
Epoch 1904, Masked params (inverse standardized): tensor([3.229745e+01, 4.430247e+01, 6.952858e-02, 2.638702e+01, 2.771591e+01,
        1.781357e+01, 2.143713e+01, 1.639305e+00, 1.066124e+02, 2.758639e+01,
        2.618221e+01, 4.461701e+01, 2.444435e+01, 1.504056e+01, 8.461505e+01,
        2.836913e+01, 5.668501e+00, 2.982967e+01, 1.197107e+01, 3.204506e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1905,(no reg loss)standard loss: 0.0052317, inverse loss: 0.0010921
Valid-->> Epoch [1905/3000], Standardized Loss: 0.0127739, Inverse Loss: 0.0026664
Training -->> Epoch: 1906,(no reg loss)standard loss: 0.0054571, inverse loss: 0.0011391
Valid-->> Epoch [1906/3000], Standardized Loss: 0.0126415, Inverse Loss: 0.0026388
Training -->> Epoch: 1907,(no reg loss)standard loss: 0.0051259, inverse loss: 0.0010700
Valid-->> Epoch [1907/3000], Standardized Loss: 0.0127829, Inverse Loss: 0.0026683
Training -->> Epoch: 1908,(no reg loss)standard loss: 0.0054483, inverse loss: 0.0011373
Valid-->> Epoch [1908/3000], Standardized Loss: 0.0127573, Inverse Loss: 0.0026629
Training -->> Epoch: 1909,(no reg loss)standard loss: 0.0050847, inverse loss: 0.0010614
Valid-->> Epoch [1909/3000], Standardized Loss: 0.0126580, Inverse Loss: 0.0026422
Training -->> Epoch: 1910,(no reg loss)standard loss: 0.0054191, inverse loss: 0.0011312
Valid-->> Epoch [1910/3000], Standardized Loss: 0.0128118, Inverse Loss: 0.0026743
Training -->> Epoch: 1911,(no reg loss)standard loss: 0.0052640, inverse loss: 0.0010988
Valid-->> Epoch [1911/3000], Standardized Loss: 0.0126774, Inverse Loss: 0.0026463
Training -->> Epoch: 1912,(no reg loss)standard loss: 0.0054690, inverse loss: 0.0011416
Valid-->> Epoch [1912/3000], Standardized Loss: 0.0126033, Inverse Loss: 0.0026308
Valid-->> Lowest loss found at epoch 1912, loss: 0.0026308
Epoch 1912, Masked params (inverse standardized): tensor([3.229819e+01, 4.430184e+01, 7.435799e-02, 2.638776e+01, 2.771649e+01,
        1.781526e+01, 2.143783e+01, 1.643745e+00, 1.066117e+02, 2.758703e+01,
        2.618289e+01, 4.462099e+01, 2.444462e+01, 1.504281e+01, 8.461542e+01,
        2.836950e+01, 5.673227e+00, 2.982998e+01, 1.197437e+01, 3.204570e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1913,(no reg loss)standard loss: 0.0052198, inverse loss: 0.0010896
Valid-->> Epoch [1913/3000], Standardized Loss: 0.0126775, Inverse Loss: 0.0026463
Training -->> Epoch: 1914,(no reg loss)standard loss: 0.0053996, inverse loss: 0.0011271
Valid-->> Epoch [1914/3000], Standardized Loss: 0.0126117, Inverse Loss: 0.0026325
Training -->> Epoch: 1915,(no reg loss)standard loss: 0.0053342, inverse loss: 0.0011135
Valid-->> Epoch [1915/3000], Standardized Loss: 0.0127069, Inverse Loss: 0.0026524
Training -->> Epoch: 1916,(no reg loss)standard loss: 0.0050630, inverse loss: 0.0010568
Valid-->> Epoch [1916/3000], Standardized Loss: 0.0126621, Inverse Loss: 0.0026431
Training -->> Epoch: 1917,(no reg loss)standard loss: 0.0053629, inverse loss: 0.0011194
Valid-->> Epoch [1917/3000], Standardized Loss: 0.0128779, Inverse Loss: 0.0026881
Training -->> Epoch: 1918,(no reg loss)standard loss: 0.0054280, inverse loss: 0.0011330
Valid-->> Epoch [1918/3000], Standardized Loss: 0.0126037, Inverse Loss: 0.0026309
Training -->> Epoch: 1919,(no reg loss)standard loss: 0.0054250, inverse loss: 0.0011324
Valid-->> Epoch [1919/3000], Standardized Loss: 0.0126482, Inverse Loss: 0.0026402
Training -->> Epoch: 1920,(no reg loss)standard loss: 0.0051919, inverse loss: 0.0010837
Valid-->> Epoch [1920/3000], Standardized Loss: 0.0126957, Inverse Loss: 0.0026501
Training -->> Epoch: 1921,(no reg loss)standard loss: 0.0052114, inverse loss: 0.0010878
Valid-->> Epoch [1921/3000], Standardized Loss: 0.0127016, Inverse Loss: 0.0026513
Training -->> Epoch: 1922,(no reg loss)standard loss: 0.0053587, inverse loss: 0.0011186
Valid-->> Epoch [1922/3000], Standardized Loss: 0.0127472, Inverse Loss: 0.0026608
Training -->> Epoch: 1923,(no reg loss)standard loss: 0.0054152, inverse loss: 0.0011304
Valid-->> Epoch [1923/3000], Standardized Loss: 0.0126615, Inverse Loss: 0.0026429
Training -->> Epoch: 1924,(no reg loss)standard loss: 0.0052060, inverse loss: 0.0010867
Valid-->> Epoch [1924/3000], Standardized Loss: 0.0127771, Inverse Loss: 0.0026671
Training -->> Epoch: 1925,(no reg loss)standard loss: 0.0054155, inverse loss: 0.0011304
Valid-->> Epoch [1925/3000], Standardized Loss: 0.0126754, Inverse Loss: 0.0026458
Training -->> Epoch: 1926,(no reg loss)standard loss: 0.0054095, inverse loss: 0.0011292
Valid-->> Epoch [1926/3000], Standardized Loss: 0.0127946, Inverse Loss: 0.0026707
Training -->> Epoch: 1927,(no reg loss)standard loss: 0.0053681, inverse loss: 0.0011205
Valid-->> Epoch [1927/3000], Standardized Loss: 0.0127215, Inverse Loss: 0.0026555
Training -->> Epoch: 1928,(no reg loss)standard loss: 0.0052788, inverse loss: 0.0011019
Valid-->> Epoch [1928/3000], Standardized Loss: 0.0127273, Inverse Loss: 0.0026567
Training -->> Epoch: 1929,(no reg loss)standard loss: 0.0052926, inverse loss: 0.0011048
Valid-->> Epoch [1929/3000], Standardized Loss: 0.0128873, Inverse Loss: 0.0026901
Training -->> Epoch: 1930,(no reg loss)standard loss: 0.0056855, inverse loss: 0.0011868
Valid-->> Epoch [1930/3000], Standardized Loss: 0.0127609, Inverse Loss: 0.0026637
Training -->> Epoch: 1931,(no reg loss)standard loss: 0.0055225, inverse loss: 0.0011528
Valid-->> Epoch [1931/3000], Standardized Loss: 0.0127046, Inverse Loss: 0.0026519
Training -->> Epoch: 1932,(no reg loss)standard loss: 0.0051457, inverse loss: 0.0010741
Valid-->> Epoch [1932/3000], Standardized Loss: 0.0126183, Inverse Loss: 0.0026339
Training -->> Epoch: 1933,(no reg loss)standard loss: 0.0052733, inverse loss: 0.0011007
Valid-->> Epoch [1933/3000], Standardized Loss: 0.0127472, Inverse Loss: 0.0026608
Training -->> Epoch: 1934,(no reg loss)standard loss: 0.0055641, inverse loss: 0.0011614
Valid-->> Epoch [1934/3000], Standardized Loss: 0.0126773, Inverse Loss: 0.0026462
Training -->> Epoch: 1935,(no reg loss)standard loss: 0.0052115, inverse loss: 0.0010878
Valid-->> Epoch [1935/3000], Standardized Loss: 0.0127158, Inverse Loss: 0.0026543
Training -->> Epoch: 1936,(no reg loss)standard loss: 0.0056258, inverse loss: 0.0011743
Valid-->> Epoch [1936/3000], Standardized Loss: 0.0127716, Inverse Loss: 0.0026659
Training -->> Epoch: 1937,(no reg loss)standard loss: 0.0051918, inverse loss: 0.0010837
Valid-->> Epoch [1937/3000], Standardized Loss: 0.0126685, Inverse Loss: 0.0026444
Training -->> Epoch: 1938,(no reg loss)standard loss: 0.0055411, inverse loss: 0.0011566
Valid-->> Epoch [1938/3000], Standardized Loss: 0.0126757, Inverse Loss: 0.0026459
Training -->> Epoch: 1939,(no reg loss)standard loss: 0.0051913, inverse loss: 0.0010836
Valid-->> Epoch [1939/3000], Standardized Loss: 0.0126847, Inverse Loss: 0.0026478
Training -->> Epoch: 1940,(no reg loss)standard loss: 0.0054703, inverse loss: 0.0011419
Valid-->> Epoch [1940/3000], Standardized Loss: 0.0126920, Inverse Loss: 0.0026493
Training -->> Epoch: 1941,(no reg loss)standard loss: 0.0053108, inverse loss: 0.0011086
Valid-->> Epoch [1941/3000], Standardized Loss: 0.0126749, Inverse Loss: 0.0026458
Training -->> Epoch: 1942,(no reg loss)standard loss: 0.0053660, inverse loss: 0.0011201
Valid-->> Epoch [1942/3000], Standardized Loss: 0.0127255, Inverse Loss: 0.0026563
Training -->> Epoch: 1943,(no reg loss)standard loss: 0.0053492, inverse loss: 0.0011166
Valid-->> Epoch [1943/3000], Standardized Loss: 0.0127155, Inverse Loss: 0.0026542
Training -->> Epoch: 1944,(no reg loss)standard loss: 0.0057323, inverse loss: 0.0011966
Valid-->> Epoch [1944/3000], Standardized Loss: 0.0126463, Inverse Loss: 0.0026398
Training -->> Epoch: 1945,(no reg loss)standard loss: 0.0051948, inverse loss: 0.0010843
Valid-->> Epoch [1945/3000], Standardized Loss: 0.0127077, Inverse Loss: 0.0026526
Training -->> Epoch: 1946,(no reg loss)standard loss: 0.0054357, inverse loss: 0.0011346
Valid-->> Epoch [1946/3000], Standardized Loss: 0.0126909, Inverse Loss: 0.0026491
Training -->> Epoch: 1947,(no reg loss)standard loss: 0.0054936, inverse loss: 0.0011467
Valid-->> Epoch [1947/3000], Standardized Loss: 0.0127498, Inverse Loss: 0.0026614
Training -->> Epoch: 1948,(no reg loss)standard loss: 0.0054913, inverse loss: 0.0011463
Valid-->> Epoch [1948/3000], Standardized Loss: 0.0126725, Inverse Loss: 0.0026452
Training -->> Epoch: 1949,(no reg loss)standard loss: 0.0053869, inverse loss: 0.0011244
Valid-->> Epoch [1949/3000], Standardized Loss: 0.0126670, Inverse Loss: 0.0026441
Training -->> Epoch: 1950,(no reg loss)standard loss: 0.0053228, inverse loss: 0.0011111
Valid-->> Epoch [1950/3000], Standardized Loss: 0.0126443, Inverse Loss: 0.0026394
Training -->> Epoch: 1951,(no reg loss)standard loss: 0.0054411, inverse loss: 0.0011358
Valid-->> Epoch [1951/3000], Standardized Loss: 0.0127073, Inverse Loss: 0.0026525
Training -->> Epoch: 1952,(no reg loss)standard loss: 0.0053545, inverse loss: 0.0011177
Valid-->> Epoch [1952/3000], Standardized Loss: 0.0126460, Inverse Loss: 0.0026397
Training -->> Epoch: 1953,(no reg loss)standard loss: 0.0053349, inverse loss: 0.0011136
Valid-->> Epoch [1953/3000], Standardized Loss: 0.0126702, Inverse Loss: 0.0026448
Training -->> Epoch: 1954,(no reg loss)standard loss: 0.0055404, inverse loss: 0.0011565
Valid-->> Epoch [1954/3000], Standardized Loss: 0.0126170, Inverse Loss: 0.0026336
Training -->> Epoch: 1955,(no reg loss)standard loss: 0.0054470, inverse loss: 0.0011370
Valid-->> Epoch [1955/3000], Standardized Loss: 0.0128165, Inverse Loss: 0.0026753
Training -->> Epoch: 1956,(no reg loss)standard loss: 0.0055348, inverse loss: 0.0011553
Valid-->> Epoch [1956/3000], Standardized Loss: 0.0125685, Inverse Loss: 0.0026235
Valid-->> Lowest loss found at epoch 1956, loss: 0.0026235
Epoch 1956, Masked params (inverse standardized): tensor([3.229902e+01, 4.430302e+01, 7.272911e-02, 2.638937e+01, 2.771783e+01,
        1.781596e+01, 2.143863e+01, 1.641891e+00, 1.066128e+02, 2.758858e+01,
        2.618433e+01, 4.462032e+01, 2.444557e+01, 1.504349e+01, 8.461662e+01,
        2.837054e+01, 5.671509e+00, 2.983095e+01, 1.197431e+01, 3.204655e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 1957,(no reg loss)standard loss: 0.0052290, inverse loss: 0.0010915
Valid-->> Epoch [1957/3000], Standardized Loss: 0.0126509, Inverse Loss: 0.0026407
Training -->> Epoch: 1958,(no reg loss)standard loss: 0.0054465, inverse loss: 0.0011369
Valid-->> Epoch [1958/3000], Standardized Loss: 0.0126759, Inverse Loss: 0.0026459
Training -->> Epoch: 1959,(no reg loss)standard loss: 0.0055062, inverse loss: 0.0011494
Valid-->> Epoch [1959/3000], Standardized Loss: 0.0126068, Inverse Loss: 0.0026315
Training -->> Epoch: 1960,(no reg loss)standard loss: 0.0053185, inverse loss: 0.0011102
Valid-->> Epoch [1960/3000], Standardized Loss: 0.0126657, Inverse Loss: 0.0026438
Training -->> Epoch: 1961,(no reg loss)standard loss: 0.0055527, inverse loss: 0.0011591
Valid-->> Epoch [1961/3000], Standardized Loss: 0.0126374, Inverse Loss: 0.0026379
Training -->> Epoch: 1962,(no reg loss)standard loss: 0.0053820, inverse loss: 0.0011234
Valid-->> Epoch [1962/3000], Standardized Loss: 0.0125874, Inverse Loss: 0.0026275
Training -->> Epoch: 1963,(no reg loss)standard loss: 0.0053300, inverse loss: 0.0011126
Valid-->> Epoch [1963/3000], Standardized Loss: 0.0126296, Inverse Loss: 0.0026363
Training -->> Epoch: 1964,(no reg loss)standard loss: 0.0054309, inverse loss: 0.0011336
Valid-->> Epoch [1964/3000], Standardized Loss: 0.0127476, Inverse Loss: 0.0026609
Training -->> Epoch: 1965,(no reg loss)standard loss: 0.0056992, inverse loss: 0.0011896
Valid-->> Epoch [1965/3000], Standardized Loss: 0.0128490, Inverse Loss: 0.0026821
Training -->> Epoch: 1966,(no reg loss)standard loss: 0.0053757, inverse loss: 0.0011221
Valid-->> Epoch [1966/3000], Standardized Loss: 0.0126149, Inverse Loss: 0.0026332
Training -->> Epoch: 1967,(no reg loss)standard loss: 0.0054885, inverse loss: 0.0011457
Valid-->> Epoch [1967/3000], Standardized Loss: 0.0128289, Inverse Loss: 0.0026779
Training -->> Epoch: 1968,(no reg loss)standard loss: 0.0054536, inverse loss: 0.0011384
Valid-->> Epoch [1968/3000], Standardized Loss: 0.0126522, Inverse Loss: 0.0026410
Training -->> Epoch: 1969,(no reg loss)standard loss: 0.0055118, inverse loss: 0.0011505
Valid-->> Epoch [1969/3000], Standardized Loss: 0.0126878, Inverse Loss: 0.0026484
Training -->> Epoch: 1970,(no reg loss)standard loss: 0.0053871, inverse loss: 0.0011245
Valid-->> Epoch [1970/3000], Standardized Loss: 0.0127696, Inverse Loss: 0.0026655
Training -->> Epoch: 1971,(no reg loss)standard loss: 0.0055489, inverse loss: 0.0011583
Valid-->> Epoch [1971/3000], Standardized Loss: 0.0127180, Inverse Loss: 0.0026547
Training -->> Epoch: 1972,(no reg loss)standard loss: 0.0054850, inverse loss: 0.0011449
Valid-->> Epoch [1972/3000], Standardized Loss: 0.0126414, Inverse Loss: 0.0026387
Training -->> Epoch: 1973,(no reg loss)standard loss: 0.0052623, inverse loss: 0.0010985
Valid-->> Epoch [1973/3000], Standardized Loss: 0.0126894, Inverse Loss: 0.0026488
Training -->> Epoch: 1974,(no reg loss)standard loss: 0.0057014, inverse loss: 0.0011901
Valid-->> Epoch [1974/3000], Standardized Loss: 0.0126846, Inverse Loss: 0.0026478
Training -->> Epoch: 1975,(no reg loss)standard loss: 0.0054392, inverse loss: 0.0011354
Valid-->> Epoch [1975/3000], Standardized Loss: 0.0126926, Inverse Loss: 0.0026494
Training -->> Epoch: 1976,(no reg loss)standard loss: 0.0052360, inverse loss: 0.0010929
Valid-->> Epoch [1976/3000], Standardized Loss: 0.0125804, Inverse Loss: 0.0026260
Training -->> Epoch: 1977,(no reg loss)standard loss: 0.0055225, inverse loss: 0.0011528
Valid-->> Epoch [1977/3000], Standardized Loss: 0.0126443, Inverse Loss: 0.0026394
Training -->> Epoch: 1978,(no reg loss)standard loss: 0.0054169, inverse loss: 0.0011307
Valid-->> Epoch [1978/3000], Standardized Loss: 0.0127008, Inverse Loss: 0.0026511
Training -->> Epoch: 1979,(no reg loss)standard loss: 0.0055126, inverse loss: 0.0011507
Valid-->> Epoch [1979/3000], Standardized Loss: 0.0126811, Inverse Loss: 0.0026470
Training -->> Epoch: 1980,(no reg loss)standard loss: 0.0054826, inverse loss: 0.0011444
Valid-->> Epoch [1980/3000], Standardized Loss: 0.0126208, Inverse Loss: 0.0026344
Training -->> Epoch: 1981,(no reg loss)standard loss: 0.0053951, inverse loss: 0.0011262
Valid-->> Epoch [1981/3000], Standardized Loss: 0.0126614, Inverse Loss: 0.0026429
Training -->> Epoch: 1982,(no reg loss)standard loss: 0.0055930, inverse loss: 0.0011675
Valid-->> Epoch [1982/3000], Standardized Loss: 0.0126427, Inverse Loss: 0.0026390
Training -->> Epoch: 1983,(no reg loss)standard loss: 0.0054145, inverse loss: 0.0011302
Valid-->> Epoch [1983/3000], Standardized Loss: 0.0127017, Inverse Loss: 0.0026513
Training -->> Epoch: 1984,(no reg loss)standard loss: 0.0055375, inverse loss: 0.0011559
Valid-->> Epoch [1984/3000], Standardized Loss: 0.0127383, Inverse Loss: 0.0026590
Training -->> Epoch: 1985,(no reg loss)standard loss: 0.0054687, inverse loss: 0.0011415
Valid-->> Epoch [1985/3000], Standardized Loss: 0.0126554, Inverse Loss: 0.0026417
Training -->> Epoch: 1986,(no reg loss)standard loss: 0.0055142, inverse loss: 0.0011510
Valid-->> Epoch [1986/3000], Standardized Loss: 0.0127871, Inverse Loss: 0.0026691
Training -->> Epoch: 1987,(no reg loss)standard loss: 0.0054951, inverse loss: 0.0011470
Valid-->> Epoch [1987/3000], Standardized Loss: 0.0126711, Inverse Loss: 0.0026449
Training -->> Epoch: 1988,(no reg loss)standard loss: 0.0055002, inverse loss: 0.0011481
Valid-->> Epoch [1988/3000], Standardized Loss: 0.0126899, Inverse Loss: 0.0026489
Training -->> Epoch: 1989,(no reg loss)standard loss: 0.0057584, inverse loss: 0.0012020
Valid-->> Epoch [1989/3000], Standardized Loss: 0.0125760, Inverse Loss: 0.0026251
Training -->> Epoch: 1990,(no reg loss)standard loss: 0.0052488, inverse loss: 0.0010956
Valid-->> Epoch [1990/3000], Standardized Loss: 0.0126593, Inverse Loss: 0.0026425
Training -->> Epoch: 1991,(no reg loss)standard loss: 0.0057781, inverse loss: 0.0012061
Valid-->> Epoch [1991/3000], Standardized Loss: 0.0126536, Inverse Loss: 0.0026413
Training -->> Epoch: 1992,(no reg loss)standard loss: 0.0054460, inverse loss: 0.0011368
Valid-->> Epoch [1992/3000], Standardized Loss: 0.0126410, Inverse Loss: 0.0026387
Training -->> Epoch: 1993,(no reg loss)standard loss: 0.0055188, inverse loss: 0.0011520
Valid-->> Epoch [1993/3000], Standardized Loss: 0.0126727, Inverse Loss: 0.0026453
Training -->> Epoch: 1994,(no reg loss)standard loss: 0.0053470, inverse loss: 0.0011161
Valid-->> Epoch [1994/3000], Standardized Loss: 0.0127517, Inverse Loss: 0.0026618
Training -->> Epoch: 1995,(no reg loss)standard loss: 0.0058289, inverse loss: 0.0012167
Valid-->> Epoch [1995/3000], Standardized Loss: 0.0126098, Inverse Loss: 0.0026321
Training -->> Epoch: 1996,(no reg loss)standard loss: 0.0053484, inverse loss: 0.0011164
Valid-->> Epoch [1996/3000], Standardized Loss: 0.0126035, Inverse Loss: 0.0026308
Training -->> Epoch: 1997,(no reg loss)standard loss: 0.0055030, inverse loss: 0.0011487
Valid-->> Epoch [1997/3000], Standardized Loss: 0.0126735, Inverse Loss: 0.0026454
Training -->> Epoch: 1998,(no reg loss)standard loss: 0.0054004, inverse loss: 0.0011273
Valid-->> Epoch [1998/3000], Standardized Loss: 0.0126327, Inverse Loss: 0.0026369
Training -->> Epoch: 1999,(no reg loss)standard loss: 0.0057047, inverse loss: 0.0011908
Valid-->> Epoch [1999/3000], Standardized Loss: 0.0126716, Inverse Loss: 0.0026451
Training -->> Epoch: 2000,(no reg loss)standard loss: 0.0053980, inverse loss: 0.0011268
Valid-->> Epoch [2000/3000], Standardized Loss: 0.0127042, Inverse Loss: 0.0026519
Training -->> Epoch: 2001,(no reg loss)standard loss: 0.0057215, inverse loss: 0.0011943
Valid-->> Epoch [2001/3000], Standardized Loss: 0.0126977, Inverse Loss: 0.0026505
Training -->> Epoch: 2002,(no reg loss)standard loss: 0.0054866, inverse loss: 0.0011453
Valid-->> Epoch [2002/3000], Standardized Loss: 0.0126951, Inverse Loss: 0.0026500
Training -->> Epoch: 2003,(no reg loss)standard loss: 0.0056084, inverse loss: 0.0011707
Valid-->> Epoch [2003/3000], Standardized Loss: 0.0126630, Inverse Loss: 0.0026433
Training -->> Epoch: 2004,(no reg loss)standard loss: 0.0056040, inverse loss: 0.0011698
Valid-->> Epoch [2004/3000], Standardized Loss: 0.0126905, Inverse Loss: 0.0026490
Training -->> Epoch: 2005,(no reg loss)standard loss: 0.0053684, inverse loss: 0.0011206
Valid-->> Epoch [2005/3000], Standardized Loss: 0.0126395, Inverse Loss: 0.0026384
Training -->> Epoch: 2006,(no reg loss)standard loss: 0.0057032, inverse loss: 0.0011905
Valid-->> Epoch [2006/3000], Standardized Loss: 0.0126454, Inverse Loss: 0.0026396
Training -->> Epoch: 2007,(no reg loss)standard loss: 0.0054824, inverse loss: 0.0011444
Valid-->> Epoch [2007/3000], Standardized Loss: 0.0126077, Inverse Loss: 0.0026317
Training -->> Epoch: 2008,(no reg loss)standard loss: 0.0056357, inverse loss: 0.0011764
Valid-->> Epoch [2008/3000], Standardized Loss: 0.0125433, Inverse Loss: 0.0026183
Valid-->> Lowest loss found at epoch 2008, loss: 0.0026183
Epoch 2008, Masked params (inverse standardized): tensor([3.229810e+01, 4.430482e+01, 7.349586e-02, 2.639149e+01, 2.771935e+01,
        1.781467e+01, 2.143765e+01, 1.642122e+00, 1.066143e+02, 2.759054e+01,
        2.618610e+01, 4.462012e+01, 2.444541e+01, 1.504242e+01, 8.461649e+01,
        2.837097e+01, 5.672506e+00, 2.983068e+01, 1.197376e+01, 3.204558e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 2009,(no reg loss)standard loss: 0.0053993, inverse loss: 0.0011270
Valid-->> Epoch [2009/3000], Standardized Loss: 0.0126834, Inverse Loss: 0.0026475
Training -->> Epoch: 2010,(no reg loss)standard loss: 0.0056628, inverse loss: 0.0011820
Valid-->> Epoch [2010/3000], Standardized Loss: 0.0126449, Inverse Loss: 0.0026395
Training -->> Epoch: 2011,(no reg loss)standard loss: 0.0054880, inverse loss: 0.0011456
Valid-->> Epoch [2011/3000], Standardized Loss: 0.0126760, Inverse Loss: 0.0026460
Training -->> Epoch: 2012,(no reg loss)standard loss: 0.0056374, inverse loss: 0.0011767
Valid-->> Epoch [2012/3000], Standardized Loss: 0.0126481, Inverse Loss: 0.0026401
Training -->> Epoch: 2013,(no reg loss)standard loss: 0.0056614, inverse loss: 0.0011817
Valid-->> Epoch [2013/3000], Standardized Loss: 0.0127314, Inverse Loss: 0.0026575
Training -->> Epoch: 2014,(no reg loss)standard loss: 0.0055408, inverse loss: 0.0011566
Valid-->> Epoch [2014/3000], Standardized Loss: 0.0126568, Inverse Loss: 0.0026420
Training -->> Epoch: 2015,(no reg loss)standard loss: 0.0056166, inverse loss: 0.0011724
Valid-->> Epoch [2015/3000], Standardized Loss: 0.0127971, Inverse Loss: 0.0026713
Training -->> Epoch: 2016,(no reg loss)standard loss: 0.0055027, inverse loss: 0.0011486
Valid-->> Epoch [2016/3000], Standardized Loss: 0.0127412, Inverse Loss: 0.0026596
Training -->> Epoch: 2017,(no reg loss)standard loss: 0.0057534, inverse loss: 0.0012010
Valid-->> Epoch [2017/3000], Standardized Loss: 0.0127143, Inverse Loss: 0.0026540
Training -->> Epoch: 2018,(no reg loss)standard loss: 0.0056484, inverse loss: 0.0011790
Valid-->> Epoch [2018/3000], Standardized Loss: 0.0126094, Inverse Loss: 0.0026321
Training -->> Epoch: 2019,(no reg loss)standard loss: 0.0055799, inverse loss: 0.0011647
Valid-->> Epoch [2019/3000], Standardized Loss: 0.0127058, Inverse Loss: 0.0026522
Training -->> Epoch: 2020,(no reg loss)standard loss: 0.0057517, inverse loss: 0.0012006
Valid-->> Epoch [2020/3000], Standardized Loss: 0.0125700, Inverse Loss: 0.0026238
Training -->> Epoch: 2021,(no reg loss)standard loss: 0.0053723, inverse loss: 0.0011214
Valid-->> Epoch [2021/3000], Standardized Loss: 0.0126612, Inverse Loss: 0.0026429
Training -->> Epoch: 2022,(no reg loss)standard loss: 0.0057624, inverse loss: 0.0012028
Valid-->> Epoch [2022/3000], Standardized Loss: 0.0126689, Inverse Loss: 0.0026445
Training -->> Epoch: 2023,(no reg loss)standard loss: 0.0057741, inverse loss: 0.0012053
Valid-->> Epoch [2023/3000], Standardized Loss: 0.0126079, Inverse Loss: 0.0026317
Training -->> Epoch: 2024,(no reg loss)standard loss: 0.0055526, inverse loss: 0.0011591
Valid-->> Epoch [2024/3000], Standardized Loss: 0.0127058, Inverse Loss: 0.0026522
Training -->> Epoch: 2025,(no reg loss)standard loss: 0.0058236, inverse loss: 0.0012156
Valid-->> Epoch [2025/3000], Standardized Loss: 0.0126409, Inverse Loss: 0.0026387
Training -->> Epoch: 2026,(no reg loss)standard loss: 0.0054986, inverse loss: 0.0011478
Valid-->> Epoch [2026/3000], Standardized Loss: 0.0126106, Inverse Loss: 0.0026323
Training -->> Epoch: 2027,(no reg loss)standard loss: 0.0056072, inverse loss: 0.0011704
Valid-->> Epoch [2027/3000], Standardized Loss: 0.0126131, Inverse Loss: 0.0026328
Training -->> Epoch: 2028,(no reg loss)standard loss: 0.0055356, inverse loss: 0.0011555
Valid-->> Epoch [2028/3000], Standardized Loss: 0.0126981, Inverse Loss: 0.0026506
Training -->> Epoch: 2029,(no reg loss)standard loss: 0.0057730, inverse loss: 0.0012050
Valid-->> Epoch [2029/3000], Standardized Loss: 0.0126510, Inverse Loss: 0.0026407
Training -->> Epoch: 2030,(no reg loss)standard loss: 0.0057637, inverse loss: 0.0012031
Valid-->> Epoch [2030/3000], Standardized Loss: 0.0125526, Inverse Loss: 0.0026202
Training -->> Epoch: 2031,(no reg loss)standard loss: 0.0053855, inverse loss: 0.0011242
Valid-->> Epoch [2031/3000], Standardized Loss: 0.0126043, Inverse Loss: 0.0026310
Training -->> Epoch: 2032,(no reg loss)standard loss: 0.0057994, inverse loss: 0.0012106
Valid-->> Epoch [2032/3000], Standardized Loss: 0.0125842, Inverse Loss: 0.0026268
Training -->> Epoch: 2033,(no reg loss)standard loss: 0.0052734, inverse loss: 0.0011008
Valid-->> Epoch [2033/3000], Standardized Loss: 0.0125129, Inverse Loss: 0.0026119
Valid-->> Lowest loss found at epoch 2033, loss: 0.0026119
Epoch 2033, Masked params (inverse standardized): tensor([3.229609e+01, 4.430156e+01, 6.184769e-02, 2.639403e+01, 2.772115e+01,
        1.780936e+01, 2.143540e+01, 1.631231e+00, 1.066127e+02, 2.759241e+01,
        2.618874e+01, 4.460989e+01, 2.444554e+01, 1.503626e+01, 8.461805e+01,
        2.837181e+01, 5.661343e+00, 2.983076e+01, 1.196482e+01, 3.204348e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 2034,(no reg loss)standard loss: 0.0057422, inverse loss: 0.0011986
Valid-->> Epoch [2034/3000], Standardized Loss: 0.0126816, Inverse Loss: 0.0026471
Training -->> Epoch: 2035,(no reg loss)standard loss: 0.0056099, inverse loss: 0.0011710
Valid-->> Epoch [2035/3000], Standardized Loss: 0.0125710, Inverse Loss: 0.0026240
Training -->> Epoch: 2036,(no reg loss)standard loss: 0.0056078, inverse loss: 0.0011706
Valid-->> Epoch [2036/3000], Standardized Loss: 0.0126889, Inverse Loss: 0.0026487
Training -->> Epoch: 2037,(no reg loss)standard loss: 0.0057934, inverse loss: 0.0012093
Valid-->> Epoch [2037/3000], Standardized Loss: 0.0126079, Inverse Loss: 0.0026317
Training -->> Epoch: 2038,(no reg loss)standard loss: 0.0056409, inverse loss: 0.0011775
Valid-->> Epoch [2038/3000], Standardized Loss: 0.0126294, Inverse Loss: 0.0026362
Training -->> Epoch: 2039,(no reg loss)standard loss: 0.0056589, inverse loss: 0.0011812
Valid-->> Epoch [2039/3000], Standardized Loss: 0.0125641, Inverse Loss: 0.0026226
Training -->> Epoch: 2040,(no reg loss)standard loss: 0.0055509, inverse loss: 0.0011587
Valid-->> Epoch [2040/3000], Standardized Loss: 0.0126269, Inverse Loss: 0.0026357
Training -->> Epoch: 2041,(no reg loss)standard loss: 0.0056094, inverse loss: 0.0011709
Valid-->> Epoch [2041/3000], Standardized Loss: 0.0126477, Inverse Loss: 0.0026401
Training -->> Epoch: 2042,(no reg loss)standard loss: 0.0057352, inverse loss: 0.0011972
Valid-->> Epoch [2042/3000], Standardized Loss: 0.0127801, Inverse Loss: 0.0026677
Training -->> Epoch: 2043,(no reg loss)standard loss: 0.0056575, inverse loss: 0.0011809
Valid-->> Epoch [2043/3000], Standardized Loss: 0.0125833, Inverse Loss: 0.0026266
Training -->> Epoch: 2044,(no reg loss)standard loss: 0.0057049, inverse loss: 0.0011908
Valid-->> Epoch [2044/3000], Standardized Loss: 0.0126083, Inverse Loss: 0.0026318
Training -->> Epoch: 2045,(no reg loss)standard loss: 0.0054048, inverse loss: 0.0011282
Valid-->> Epoch [2045/3000], Standardized Loss: 0.0125799, Inverse Loss: 0.0026259
Training -->> Epoch: 2046,(no reg loss)standard loss: 0.0060458, inverse loss: 0.0012620
Valid-->> Epoch [2046/3000], Standardized Loss: 0.0126201, Inverse Loss: 0.0026343
Training -->> Epoch: 2047,(no reg loss)standard loss: 0.0056196, inverse loss: 0.0011730
Valid-->> Epoch [2047/3000], Standardized Loss: 0.0125173, Inverse Loss: 0.0026128
Training -->> Epoch: 2048,(no reg loss)standard loss: 0.0057004, inverse loss: 0.0011899
Valid-->> Epoch [2048/3000], Standardized Loss: 0.0125938, Inverse Loss: 0.0026288
Training -->> Epoch: 2049,(no reg loss)standard loss: 0.0054960, inverse loss: 0.0011472
Valid-->> Epoch [2049/3000], Standardized Loss: 0.0125870, Inverse Loss: 0.0026274
Training -->> Epoch: 2050,(no reg loss)standard loss: 0.0057426, inverse loss: 0.0011987
Valid-->> Epoch [2050/3000], Standardized Loss: 0.0126694, Inverse Loss: 0.0026446
Training -->> Epoch: 2051,(no reg loss)standard loss: 0.0057772, inverse loss: 0.0012059
Valid-->> Epoch [2051/3000], Standardized Loss: 0.0125944, Inverse Loss: 0.0026289
Training -->> Epoch: 2052,(no reg loss)standard loss: 0.0054916, inverse loss: 0.0011463
Valid-->> Epoch [2052/3000], Standardized Loss: 0.0125650, Inverse Loss: 0.0026228
Training -->> Epoch: 2053,(no reg loss)standard loss: 0.0058109, inverse loss: 0.0012130
Valid-->> Epoch [2053/3000], Standardized Loss: 0.0125852, Inverse Loss: 0.0026270
Training -->> Epoch: 2054,(no reg loss)standard loss: 0.0056191, inverse loss: 0.0011729
Valid-->> Epoch [2054/3000], Standardized Loss: 0.0125610, Inverse Loss: 0.0026220
Training -->> Epoch: 2055,(no reg loss)standard loss: 0.0057216, inverse loss: 0.0011943
Valid-->> Epoch [2055/3000], Standardized Loss: 0.0126812, Inverse Loss: 0.0026471
Training -->> Epoch: 2056,(no reg loss)standard loss: 0.0057886, inverse loss: 0.0012083
Valid-->> Epoch [2056/3000], Standardized Loss: 0.0126151, Inverse Loss: 0.0026332
Training -->> Epoch: 2057,(no reg loss)standard loss: 0.0056795, inverse loss: 0.0011855
Valid-->> Epoch [2057/3000], Standardized Loss: 0.0126112, Inverse Loss: 0.0026325
Training -->> Epoch: 2058,(no reg loss)standard loss: 0.0056629, inverse loss: 0.0011821
Valid-->> Epoch [2058/3000], Standardized Loss: 0.0126502, Inverse Loss: 0.0026406
Training -->> Epoch: 2059,(no reg loss)standard loss: 0.0056776, inverse loss: 0.0011851
Valid-->> Epoch [2059/3000], Standardized Loss: 0.0125783, Inverse Loss: 0.0026256
Training -->> Epoch: 2060,(no reg loss)standard loss: 0.0057080, inverse loss: 0.0011915
Valid-->> Epoch [2060/3000], Standardized Loss: 0.0126445, Inverse Loss: 0.0026394
Training -->> Epoch: 2061,(no reg loss)standard loss: 0.0056184, inverse loss: 0.0011728
Valid-->> Epoch [2061/3000], Standardized Loss: 0.0126012, Inverse Loss: 0.0026304
Training -->> Epoch: 2062,(no reg loss)standard loss: 0.0057024, inverse loss: 0.0011903
Valid-->> Epoch [2062/3000], Standardized Loss: 0.0127839, Inverse Loss: 0.0026685
Training -->> Epoch: 2063,(no reg loss)standard loss: 0.0059700, inverse loss: 0.0012462
Valid-->> Epoch [2063/3000], Standardized Loss: 0.0125632, Inverse Loss: 0.0026224
Training -->> Epoch: 2064,(no reg loss)standard loss: 0.0056303, inverse loss: 0.0011753
Valid-->> Epoch [2064/3000], Standardized Loss: 0.0126107, Inverse Loss: 0.0026323
Training -->> Epoch: 2065,(no reg loss)standard loss: 0.0057535, inverse loss: 0.0012010
Valid-->> Epoch [2065/3000], Standardized Loss: 0.0126766, Inverse Loss: 0.0026461
Training -->> Epoch: 2066,(no reg loss)standard loss: 0.0057611, inverse loss: 0.0012026
Valid-->> Epoch [2066/3000], Standardized Loss: 0.0125908, Inverse Loss: 0.0026282
Training -->> Epoch: 2067,(no reg loss)standard loss: 0.0058144, inverse loss: 0.0012137
Valid-->> Epoch [2067/3000], Standardized Loss: 0.0126377, Inverse Loss: 0.0026380
Training -->> Epoch: 2068,(no reg loss)standard loss: 0.0058120, inverse loss: 0.0012132
Valid-->> Epoch [2068/3000], Standardized Loss: 0.0126336, Inverse Loss: 0.0026371
Training -->> Epoch: 2069,(no reg loss)standard loss: 0.0057054, inverse loss: 0.0011909
Valid-->> Epoch [2069/3000], Standardized Loss: 0.0125870, Inverse Loss: 0.0026274
Training -->> Epoch: 2070,(no reg loss)standard loss: 0.0057130, inverse loss: 0.0011925
Valid-->> Epoch [2070/3000], Standardized Loss: 0.0125990, Inverse Loss: 0.0026299
Training -->> Epoch: 2071,(no reg loss)standard loss: 0.0058394, inverse loss: 0.0012189
Valid-->> Epoch [2071/3000], Standardized Loss: 0.0125610, Inverse Loss: 0.0026220
Training -->> Epoch: 2072,(no reg loss)standard loss: 0.0056429, inverse loss: 0.0011779
Valid-->> Epoch [2072/3000], Standardized Loss: 0.0126248, Inverse Loss: 0.0026353
Training -->> Epoch: 2073,(no reg loss)standard loss: 0.0058511, inverse loss: 0.0012214
Valid-->> Epoch [2073/3000], Standardized Loss: 0.0125594, Inverse Loss: 0.0026216
Training -->> Epoch: 2074,(no reg loss)standard loss: 0.0057820, inverse loss: 0.0012069
Valid-->> Epoch [2074/3000], Standardized Loss: 0.0125921, Inverse Loss: 0.0026285
Training -->> Epoch: 2075,(no reg loss)standard loss: 0.0055969, inverse loss: 0.0011683
Valid-->> Epoch [2075/3000], Standardized Loss: 0.0125986, Inverse Loss: 0.0026298
Training -->> Epoch: 2076,(no reg loss)standard loss: 0.0059025, inverse loss: 0.0012321
Valid-->> Epoch [2076/3000], Standardized Loss: 0.0126393, Inverse Loss: 0.0026383
Training -->> Epoch: 2077,(no reg loss)standard loss: 0.0056577, inverse loss: 0.0011810
Valid-->> Epoch [2077/3000], Standardized Loss: 0.0126366, Inverse Loss: 0.0026377
Training -->> Epoch: 2078,(no reg loss)standard loss: 0.0060113, inverse loss: 0.0012548
Valid-->> Epoch [2078/3000], Standardized Loss: 0.0126920, Inverse Loss: 0.0026493
Training -->> Epoch: 2079,(no reg loss)standard loss: 0.0056828, inverse loss: 0.0011862
Valid-->> Epoch [2079/3000], Standardized Loss: 0.0125502, Inverse Loss: 0.0026197
Training -->> Epoch: 2080,(no reg loss)standard loss: 0.0058415, inverse loss: 0.0012194
Valid-->> Epoch [2080/3000], Standardized Loss: 0.0126813, Inverse Loss: 0.0026471
Training -->> Epoch: 2081,(no reg loss)standard loss: 0.0058204, inverse loss: 0.0012150
Valid-->> Epoch [2081/3000], Standardized Loss: 0.0126381, Inverse Loss: 0.0026381
Training -->> Epoch: 2082,(no reg loss)standard loss: 0.0057782, inverse loss: 0.0012061
Valid-->> Epoch [2082/3000], Standardized Loss: 0.0126683, Inverse Loss: 0.0026444
Training -->> Epoch: 2083,(no reg loss)standard loss: 0.0058925, inverse loss: 0.0012300
Valid-->> Epoch [2083/3000], Standardized Loss: 0.0125835, Inverse Loss: 0.0026267
Training -->> Epoch: 2084,(no reg loss)standard loss: 0.0057851, inverse loss: 0.0012076
Valid-->> Epoch [2084/3000], Standardized Loss: 0.0126472, Inverse Loss: 0.0026400
Training -->> Epoch: 2085,(no reg loss)standard loss: 0.0057705, inverse loss: 0.0012045
Valid-->> Epoch [2085/3000], Standardized Loss: 0.0126744, Inverse Loss: 0.0026456
Training -->> Epoch: 2086,(no reg loss)standard loss: 0.0058014, inverse loss: 0.0012110
Valid-->> Epoch [2086/3000], Standardized Loss: 0.0125738, Inverse Loss: 0.0026246
Training -->> Epoch: 2087,(no reg loss)standard loss: 0.0059307, inverse loss: 0.0012380
Valid-->> Epoch [2087/3000], Standardized Loss: 0.0127349, Inverse Loss: 0.0026583
Training -->> Epoch: 2088,(no reg loss)standard loss: 0.0058097, inverse loss: 0.0012127
Valid-->> Epoch [2088/3000], Standardized Loss: 0.0125932, Inverse Loss: 0.0026287
Training -->> Epoch: 2089,(no reg loss)standard loss: 0.0058857, inverse loss: 0.0012286
Valid-->> Epoch [2089/3000], Standardized Loss: 0.0126221, Inverse Loss: 0.0026347
Training -->> Epoch: 2090,(no reg loss)standard loss: 0.0058142, inverse loss: 0.0012136
Valid-->> Epoch [2090/3000], Standardized Loss: 0.0126209, Inverse Loss: 0.0026345
Training -->> Epoch: 2091,(no reg loss)standard loss: 0.0058839, inverse loss: 0.0012282
Valid-->> Epoch [2091/3000], Standardized Loss: 0.0126644, Inverse Loss: 0.0026435
Training -->> Epoch: 2092,(no reg loss)standard loss: 0.0061177, inverse loss: 0.0012770
Valid-->> Epoch [2092/3000], Standardized Loss: 0.0125252, Inverse Loss: 0.0026145
Training -->> Epoch: 2093,(no reg loss)standard loss: 0.0054765, inverse loss: 0.0011431
Valid-->> Epoch [2093/3000], Standardized Loss: 0.0125045, Inverse Loss: 0.0026102
Valid-->> Lowest loss found at epoch 2093, loss: 0.0026102
Epoch 2093, Masked params (inverse standardized): tensor([3.229624e+01, 4.430164e+01, 6.231308e-02, 2.639538e+01, 2.772237e+01,
        1.780878e+01, 2.143549e+01, 1.631594e+00, 1.066121e+02, 2.759392e+01,
        2.618970e+01, 4.461019e+01, 2.444637e+01, 1.503588e+01, 8.461578e+01,
        2.837276e+01, 5.662361e+00, 2.983126e+01, 1.196463e+01, 3.204377e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 2094,(no reg loss)standard loss: 0.0061512, inverse loss: 0.0012840
Valid-->> Epoch [2094/3000], Standardized Loss: 0.0125869, Inverse Loss: 0.0026274
Training -->> Epoch: 2095,(no reg loss)standard loss: 0.0057067, inverse loss: 0.0011912
Valid-->> Epoch [2095/3000], Standardized Loss: 0.0125434, Inverse Loss: 0.0026183
Training -->> Epoch: 2096,(no reg loss)standard loss: 0.0058385, inverse loss: 0.0012187
Valid-->> Epoch [2096/3000], Standardized Loss: 0.0125906, Inverse Loss: 0.0026281
Training -->> Epoch: 2097,(no reg loss)standard loss: 0.0058648, inverse loss: 0.0012242
Valid-->> Epoch [2097/3000], Standardized Loss: 0.0126312, Inverse Loss: 0.0026366
Training -->> Epoch: 2098,(no reg loss)standard loss: 0.0059366, inverse loss: 0.0012392
Valid-->> Epoch [2098/3000], Standardized Loss: 0.0125603, Inverse Loss: 0.0026218
Training -->> Epoch: 2099,(no reg loss)standard loss: 0.0055469, inverse loss: 0.0011578
Valid-->> Epoch [2099/3000], Standardized Loss: 0.0124616, Inverse Loss: 0.0026012
Valid-->> Lowest loss found at epoch 2099, loss: 0.0026012
Epoch 2099, Masked params (inverse standardized): tensor([3.229755e+01, 4.430210e+01, 6.587982e-02, 2.639646e+01, 2.772313e+01,
        1.781028e+01, 2.143654e+01, 1.634636e+00, 1.066126e+02, 2.759479e+01,
        2.619070e+01, 4.461356e+01, 2.444695e+01, 1.503850e+01, 8.461765e+01,
        2.837308e+01, 5.665607e+00, 2.983231e+01, 1.196787e+01, 3.204490e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 2100,(no reg loss)standard loss: 0.0058293, inverse loss: 0.0012168
Valid-->> Epoch [2100/3000], Standardized Loss: 0.0126113, Inverse Loss: 0.0026325
Training -->> Epoch: 2101,(no reg loss)standard loss: 0.0058975, inverse loss: 0.0012310
Valid-->> Epoch [2101/3000], Standardized Loss: 0.0127068, Inverse Loss: 0.0026524
Training -->> Epoch: 2102,(no reg loss)standard loss: 0.0059072, inverse loss: 0.0012331
Valid-->> Epoch [2102/3000], Standardized Loss: 0.0125820, Inverse Loss: 0.0026263
Training -->> Epoch: 2103,(no reg loss)standard loss: 0.0057682, inverse loss: 0.0012040
Valid-->> Epoch [2103/3000], Standardized Loss: 0.0126272, Inverse Loss: 0.0026358
Training -->> Epoch: 2104,(no reg loss)standard loss: 0.0061251, inverse loss: 0.0012785
Valid-->> Epoch [2104/3000], Standardized Loss: 0.0126238, Inverse Loss: 0.0026351
Training -->> Epoch: 2105,(no reg loss)standard loss: 0.0055860, inverse loss: 0.0011660
Valid-->> Epoch [2105/3000], Standardized Loss: 0.0124425, Inverse Loss: 0.0025972
Valid-->> Lowest loss found at epoch 2105, loss: 0.0025972
Epoch 2105, Masked params (inverse standardized): tensor([3.229867e+01, 4.430271e+01, 6.512070e-02, 2.639673e+01, 2.772355e+01,
        1.781204e+01, 2.143808e+01, 1.633680e+00, 1.066137e+02, 2.759530e+01,
        2.619104e+01, 4.461332e+01, 2.444781e+01, 1.503919e+01, 8.461675e+01,
        2.837406e+01, 5.665442e+00, 2.983288e+01, 1.196796e+01, 3.204610e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 2106,(no reg loss)standard loss: 0.0058386, inverse loss: 0.0012187
Valid-->> Epoch [2106/3000], Standardized Loss: 0.0126886, Inverse Loss: 0.0026486
Training -->> Epoch: 2107,(no reg loss)standard loss: 0.0060503, inverse loss: 0.0012629
Valid-->> Epoch [2107/3000], Standardized Loss: 0.0126403, Inverse Loss: 0.0026385
Training -->> Epoch: 2108,(no reg loss)standard loss: 0.0058064, inverse loss: 0.0012120
Valid-->> Epoch [2108/3000], Standardized Loss: 0.0125167, Inverse Loss: 0.0026127
Training -->> Epoch: 2109,(no reg loss)standard loss: 0.0057908, inverse loss: 0.0012088
Valid-->> Epoch [2109/3000], Standardized Loss: 0.0125725, Inverse Loss: 0.0026244
Training -->> Epoch: 2110,(no reg loss)standard loss: 0.0059055, inverse loss: 0.0012327
Valid-->> Epoch [2110/3000], Standardized Loss: 0.0126136, Inverse Loss: 0.0026329
Training -->> Epoch: 2111,(no reg loss)standard loss: 0.0060971, inverse loss: 0.0012727
Valid-->> Epoch [2111/3000], Standardized Loss: 0.0125666, Inverse Loss: 0.0026231
Training -->> Epoch: 2112,(no reg loss)standard loss: 0.0058707, inverse loss: 0.0012254
Valid-->> Epoch [2112/3000], Standardized Loss: 0.0125054, Inverse Loss: 0.0026104
Training -->> Epoch: 2113,(no reg loss)standard loss: 0.0058158, inverse loss: 0.0012140
Valid-->> Epoch [2113/3000], Standardized Loss: 0.0125969, Inverse Loss: 0.0026295
Training -->> Epoch: 2114,(no reg loss)standard loss: 0.0060281, inverse loss: 0.0012583
Valid-->> Epoch [2114/3000], Standardized Loss: 0.0125777, Inverse Loss: 0.0026255
Training -->> Epoch: 2115,(no reg loss)standard loss: 0.0057913, inverse loss: 0.0012089
Valid-->> Epoch [2115/3000], Standardized Loss: 0.0126533, Inverse Loss: 0.0026412
Training -->> Epoch: 2116,(no reg loss)standard loss: 0.0060797, inverse loss: 0.0012691
Valid-->> Epoch [2116/3000], Standardized Loss: 0.0125515, Inverse Loss: 0.0026200
Training -->> Epoch: 2117,(no reg loss)standard loss: 0.0060679, inverse loss: 0.0012666
Valid-->> Epoch [2117/3000], Standardized Loss: 0.0125519, Inverse Loss: 0.0026201
Training -->> Epoch: 2118,(no reg loss)standard loss: 0.0057068, inverse loss: 0.0011912
Valid-->> Epoch [2118/3000], Standardized Loss: 0.0125615, Inverse Loss: 0.0026221
Training -->> Epoch: 2119,(no reg loss)standard loss: 0.0061362, inverse loss: 0.0012809
Valid-->> Epoch [2119/3000], Standardized Loss: 0.0126467, Inverse Loss: 0.0026399
Training -->> Epoch: 2120,(no reg loss)standard loss: 0.0058465, inverse loss: 0.0012204
Valid-->> Epoch [2120/3000], Standardized Loss: 0.0125692, Inverse Loss: 0.0026237
Training -->> Epoch: 2121,(no reg loss)standard loss: 0.0059619, inverse loss: 0.0012445
Valid-->> Epoch [2121/3000], Standardized Loss: 0.0125821, Inverse Loss: 0.0026264
Training -->> Epoch: 2122,(no reg loss)standard loss: 0.0060844, inverse loss: 0.0012701
Valid-->> Epoch [2122/3000], Standardized Loss: 0.0125379, Inverse Loss: 0.0026171
Training -->> Epoch: 2123,(no reg loss)standard loss: 0.0058564, inverse loss: 0.0012225
Valid-->> Epoch [2123/3000], Standardized Loss: 0.0125493, Inverse Loss: 0.0026195
Training -->> Epoch: 2124,(no reg loss)standard loss: 0.0059758, inverse loss: 0.0012474
Valid-->> Epoch [2124/3000], Standardized Loss: 0.0126022, Inverse Loss: 0.0026306
Training -->> Epoch: 2125,(no reg loss)standard loss: 0.0059377, inverse loss: 0.0012394
Valid-->> Epoch [2125/3000], Standardized Loss: 0.0127113, Inverse Loss: 0.0026533
Training -->> Epoch: 2126,(no reg loss)standard loss: 0.0062772, inverse loss: 0.0013103
Valid-->> Epoch [2126/3000], Standardized Loss: 0.0126130, Inverse Loss: 0.0026328
Training -->> Epoch: 2127,(no reg loss)standard loss: 0.0058451, inverse loss: 0.0012201
Valid-->> Epoch [2127/3000], Standardized Loss: 0.0125491, Inverse Loss: 0.0026195
Training -->> Epoch: 2128,(no reg loss)standard loss: 0.0062382, inverse loss: 0.0013022
Valid-->> Epoch [2128/3000], Standardized Loss: 0.0125345, Inverse Loss: 0.0026164
Training -->> Epoch: 2129,(no reg loss)standard loss: 0.0059754, inverse loss: 0.0012473
Valid-->> Epoch [2129/3000], Standardized Loss: 0.0125838, Inverse Loss: 0.0026267
Training -->> Epoch: 2130,(no reg loss)standard loss: 0.0058486, inverse loss: 0.0012208
Valid-->> Epoch [2130/3000], Standardized Loss: 0.0125280, Inverse Loss: 0.0026151
Training -->> Epoch: 2131,(no reg loss)standard loss: 0.0062204, inverse loss: 0.0012984
Valid-->> Epoch [2131/3000], Standardized Loss: 0.0126420, Inverse Loss: 0.0026389
Training -->> Epoch: 2132,(no reg loss)standard loss: 0.0059427, inverse loss: 0.0012405
Valid-->> Epoch [2132/3000], Standardized Loss: 0.0125331, Inverse Loss: 0.0026161
Training -->> Epoch: 2133,(no reg loss)standard loss: 0.0060252, inverse loss: 0.0012577
Valid-->> Epoch [2133/3000], Standardized Loss: 0.0125448, Inverse Loss: 0.0026186
Training -->> Epoch: 2134,(no reg loss)standard loss: 0.0059819, inverse loss: 0.0012486
Valid-->> Epoch [2134/3000], Standardized Loss: 0.0125282, Inverse Loss: 0.0026151
Training -->> Epoch: 2135,(no reg loss)standard loss: 0.0060010, inverse loss: 0.0012526
Valid-->> Epoch [2135/3000], Standardized Loss: 0.0125880, Inverse Loss: 0.0026276
Training -->> Epoch: 2136,(no reg loss)standard loss: 0.0059155, inverse loss: 0.0012348
Valid-->> Epoch [2136/3000], Standardized Loss: 0.0125747, Inverse Loss: 0.0026248
Training -->> Epoch: 2137,(no reg loss)standard loss: 0.0061577, inverse loss: 0.0012853
Valid-->> Epoch [2137/3000], Standardized Loss: 0.0125222, Inverse Loss: 0.0026139
Training -->> Epoch: 2138,(no reg loss)standard loss: 0.0060544, inverse loss: 0.0012638
Valid-->> Epoch [2138/3000], Standardized Loss: 0.0125514, Inverse Loss: 0.0026200
Training -->> Epoch: 2139,(no reg loss)standard loss: 0.0059953, inverse loss: 0.0012514
Valid-->> Epoch [2139/3000], Standardized Loss: 0.0125661, Inverse Loss: 0.0026230
Training -->> Epoch: 2140,(no reg loss)standard loss: 0.0058878, inverse loss: 0.0012290
Valid-->> Epoch [2140/3000], Standardized Loss: 0.0125570, Inverse Loss: 0.0026211
Training -->> Epoch: 2141,(no reg loss)standard loss: 0.0059565, inverse loss: 0.0012433
Valid-->> Epoch [2141/3000], Standardized Loss: 0.0125635, Inverse Loss: 0.0026225
Training -->> Epoch: 2142,(no reg loss)standard loss: 0.0059718, inverse loss: 0.0012465
Valid-->> Epoch [2142/3000], Standardized Loss: 0.0126564, Inverse Loss: 0.0026419
Training -->> Epoch: 2143,(no reg loss)standard loss: 0.0062021, inverse loss: 0.0012946
Valid-->> Epoch [2143/3000], Standardized Loss: 0.0125417, Inverse Loss: 0.0026179
Training -->> Epoch: 2144,(no reg loss)standard loss: 0.0057926, inverse loss: 0.0012091
Valid-->> Epoch [2144/3000], Standardized Loss: 0.0125000, Inverse Loss: 0.0026092
Training -->> Epoch: 2145,(no reg loss)standard loss: 0.0060308, inverse loss: 0.0012589
Valid-->> Epoch [2145/3000], Standardized Loss: 0.0127068, Inverse Loss: 0.0026524
Training -->> Epoch: 2146,(no reg loss)standard loss: 0.0062215, inverse loss: 0.0012987
Valid-->> Epoch [2146/3000], Standardized Loss: 0.0125900, Inverse Loss: 0.0026280
Training -->> Epoch: 2147,(no reg loss)standard loss: 0.0059389, inverse loss: 0.0012397
Valid-->> Epoch [2147/3000], Standardized Loss: 0.0125641, Inverse Loss: 0.0026226
Training -->> Epoch: 2148,(no reg loss)standard loss: 0.0061360, inverse loss: 0.0012808
Valid-->> Epoch [2148/3000], Standardized Loss: 0.0126133, Inverse Loss: 0.0026329
Training -->> Epoch: 2149,(no reg loss)standard loss: 0.0059739, inverse loss: 0.0012470
Valid-->> Epoch [2149/3000], Standardized Loss: 0.0125395, Inverse Loss: 0.0026175
Training -->> Epoch: 2150,(no reg loss)standard loss: 0.0060687, inverse loss: 0.0012668
Valid-->> Epoch [2150/3000], Standardized Loss: 0.0126159, Inverse Loss: 0.0026334
Training -->> Epoch: 2151,(no reg loss)standard loss: 0.0061257, inverse loss: 0.0012787
Valid-->> Epoch [2151/3000], Standardized Loss: 0.0124239, Inverse Loss: 0.0025933
Valid-->> Lowest loss found at epoch 2151, loss: 0.0025933
Epoch 2151, Masked params (inverse standardized): tensor([3.230011e+01, 4.430247e+01, 7.196426e-02, 2.639776e+01, 2.772423e+01,
        1.781649e+01, 2.143961e+01, 1.639751e+00, 1.066127e+02, 2.759616e+01,
        2.619183e+01, 4.461965e+01, 2.444807e+01, 1.504393e+01, 8.461580e+01,
        2.837415e+01, 5.671549e+00, 2.983337e+01, 1.197417e+01, 3.204760e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 2152,(no reg loss)standard loss: 0.0061995, inverse loss: 0.0012941
Valid-->> Epoch [2152/3000], Standardized Loss: 0.0125664, Inverse Loss: 0.0026231
Training -->> Epoch: 2153,(no reg loss)standard loss: 0.0059326, inverse loss: 0.0012384
Valid-->> Epoch [2153/3000], Standardized Loss: 0.0125507, Inverse Loss: 0.0026198
Training -->> Epoch: 2154,(no reg loss)standard loss: 0.0061598, inverse loss: 0.0012858
Valid-->> Epoch [2154/3000], Standardized Loss: 0.0125328, Inverse Loss: 0.0026161
Training -->> Epoch: 2155,(no reg loss)standard loss: 0.0060717, inverse loss: 0.0012674
Valid-->> Epoch [2155/3000], Standardized Loss: 0.0125918, Inverse Loss: 0.0026284
Training -->> Epoch: 2156,(no reg loss)standard loss: 0.0061902, inverse loss: 0.0012921
Valid-->> Epoch [2156/3000], Standardized Loss: 0.0125650, Inverse Loss: 0.0026228
Training -->> Epoch: 2157,(no reg loss)standard loss: 0.0060241, inverse loss: 0.0012575
Valid-->> Epoch [2157/3000], Standardized Loss: 0.0126333, Inverse Loss: 0.0026371
Training -->> Epoch: 2158,(no reg loss)standard loss: 0.0062139, inverse loss: 0.0012971
Valid-->> Epoch [2158/3000], Standardized Loss: 0.0125499, Inverse Loss: 0.0026196
Training -->> Epoch: 2159,(no reg loss)standard loss: 0.0062288, inverse loss: 0.0013002
Valid-->> Epoch [2159/3000], Standardized Loss: 0.0125256, Inverse Loss: 0.0026146
Training -->> Epoch: 2160,(no reg loss)standard loss: 0.0058608, inverse loss: 0.0012234
Valid-->> Epoch [2160/3000], Standardized Loss: 0.0124903, Inverse Loss: 0.0026072
Training -->> Epoch: 2161,(no reg loss)standard loss: 0.0062964, inverse loss: 0.0013143
Valid-->> Epoch [2161/3000], Standardized Loss: 0.0126121, Inverse Loss: 0.0026326
Training -->> Epoch: 2162,(no reg loss)standard loss: 0.0061310, inverse loss: 0.0012798
Valid-->> Epoch [2162/3000], Standardized Loss: 0.0126586, Inverse Loss: 0.0026423
Training -->> Epoch: 2163,(no reg loss)standard loss: 0.0060749, inverse loss: 0.0012681
Valid-->> Epoch [2163/3000], Standardized Loss: 0.0125688, Inverse Loss: 0.0026236
Training -->> Epoch: 2164,(no reg loss)standard loss: 0.0061491, inverse loss: 0.0012836
Valid-->> Epoch [2164/3000], Standardized Loss: 0.0125628, Inverse Loss: 0.0026223
Training -->> Epoch: 2165,(no reg loss)standard loss: 0.0060827, inverse loss: 0.0012697
Valid-->> Epoch [2165/3000], Standardized Loss: 0.0126324, Inverse Loss: 0.0026369
Training -->> Epoch: 2166,(no reg loss)standard loss: 0.0064003, inverse loss: 0.0013360
Valid-->> Epoch [2166/3000], Standardized Loss: 0.0124551, Inverse Loss: 0.0025999
Training -->> Epoch: 2167,(no reg loss)standard loss: 0.0060456, inverse loss: 0.0012619
Valid-->> Epoch [2167/3000], Standardized Loss: 0.0126218, Inverse Loss: 0.0026347
Training -->> Epoch: 2168,(no reg loss)standard loss: 0.0061082, inverse loss: 0.0012750
Valid-->> Epoch [2168/3000], Standardized Loss: 0.0124988, Inverse Loss: 0.0026090
Training -->> Epoch: 2169,(no reg loss)standard loss: 0.0060993, inverse loss: 0.0012732
Valid-->> Epoch [2169/3000], Standardized Loss: 0.0125495, Inverse Loss: 0.0026196
Training -->> Epoch: 2170,(no reg loss)standard loss: 0.0060744, inverse loss: 0.0012680
Valid-->> Epoch [2170/3000], Standardized Loss: 0.0125461, Inverse Loss: 0.0026189
Training -->> Epoch: 2171,(no reg loss)standard loss: 0.0061122, inverse loss: 0.0012758
Valid-->> Epoch [2171/3000], Standardized Loss: 0.0125278, Inverse Loss: 0.0026150
Training -->> Epoch: 2172,(no reg loss)standard loss: 0.0061269, inverse loss: 0.0012789
Valid-->> Epoch [2172/3000], Standardized Loss: 0.0125263, Inverse Loss: 0.0026147
Training -->> Epoch: 2173,(no reg loss)standard loss: 0.0061002, inverse loss: 0.0012733
Valid-->> Epoch [2173/3000], Standardized Loss: 0.0125736, Inverse Loss: 0.0026246
Training -->> Epoch: 2174,(no reg loss)standard loss: 0.0062114, inverse loss: 0.0012966
Valid-->> Epoch [2174/3000], Standardized Loss: 0.0125402, Inverse Loss: 0.0026176
Training -->> Epoch: 2175,(no reg loss)standard loss: 0.0062119, inverse loss: 0.0012967
Valid-->> Epoch [2175/3000], Standardized Loss: 0.0125696, Inverse Loss: 0.0026238
Training -->> Epoch: 2176,(no reg loss)standard loss: 0.0059793, inverse loss: 0.0012481
Valid-->> Epoch [2176/3000], Standardized Loss: 0.0124775, Inverse Loss: 0.0026045
Training -->> Epoch: 2177,(no reg loss)standard loss: 0.0061673, inverse loss: 0.0012874
Valid-->> Epoch [2177/3000], Standardized Loss: 0.0125267, Inverse Loss: 0.0026148
Training -->> Epoch: 2178,(no reg loss)standard loss: 0.0062236, inverse loss: 0.0012991
Valid-->> Epoch [2178/3000], Standardized Loss: 0.0125585, Inverse Loss: 0.0026214
Training -->> Epoch: 2179,(no reg loss)standard loss: 0.0062416, inverse loss: 0.0013029
Valid-->> Epoch [2179/3000], Standardized Loss: 0.0125237, Inverse Loss: 0.0026142
Training -->> Epoch: 2180,(no reg loss)standard loss: 0.0059323, inverse loss: 0.0012383
Valid-->> Epoch [2180/3000], Standardized Loss: 0.0125009, Inverse Loss: 0.0026094
Training -->> Epoch: 2181,(no reg loss)standard loss: 0.0063219, inverse loss: 0.0013196
Valid-->> Epoch [2181/3000], Standardized Loss: 0.0125235, Inverse Loss: 0.0026141
Training -->> Epoch: 2182,(no reg loss)standard loss: 0.0061610, inverse loss: 0.0012860
Valid-->> Epoch [2182/3000], Standardized Loss: 0.0125029, Inverse Loss: 0.0026098
Training -->> Epoch: 2183,(no reg loss)standard loss: 0.0060783, inverse loss: 0.0012688
Valid-->> Epoch [2183/3000], Standardized Loss: 0.0125137, Inverse Loss: 0.0026121
Training -->> Epoch: 2184,(no reg loss)standard loss: 0.0062727, inverse loss: 0.0013094
Valid-->> Epoch [2184/3000], Standardized Loss: 0.0126034, Inverse Loss: 0.0026308
Training -->> Epoch: 2185,(no reg loss)standard loss: 0.0061361, inverse loss: 0.0012808
Valid-->> Epoch [2185/3000], Standardized Loss: 0.0126035, Inverse Loss: 0.0026308
Training -->> Epoch: 2186,(no reg loss)standard loss: 0.0065400, inverse loss: 0.0013652
Valid-->> Epoch [2186/3000], Standardized Loss: 0.0125780, Inverse Loss: 0.0026255
Training -->> Epoch: 2187,(no reg loss)standard loss: 0.0060205, inverse loss: 0.0012567
Valid-->> Epoch [2187/3000], Standardized Loss: 0.0124503, Inverse Loss: 0.0025989
Training -->> Epoch: 2188,(no reg loss)standard loss: 0.0062000, inverse loss: 0.0012942
Valid-->> Epoch [2188/3000], Standardized Loss: 0.0126674, Inverse Loss: 0.0026442
Training -->> Epoch: 2189,(no reg loss)standard loss: 0.0063576, inverse loss: 0.0013271
Valid-->> Epoch [2189/3000], Standardized Loss: 0.0125381, Inverse Loss: 0.0026172
Training -->> Epoch: 2190,(no reg loss)standard loss: 0.0061652, inverse loss: 0.0012869
Valid-->> Epoch [2190/3000], Standardized Loss: 0.0125057, Inverse Loss: 0.0026104
Training -->> Epoch: 2191,(no reg loss)standard loss: 0.0063347, inverse loss: 0.0013223
Valid-->> Epoch [2191/3000], Standardized Loss: 0.0125743, Inverse Loss: 0.0026247
Training -->> Epoch: 2192,(no reg loss)standard loss: 0.0060836, inverse loss: 0.0012699
Valid-->> Epoch [2192/3000], Standardized Loss: 0.0126537, Inverse Loss: 0.0026413
Training -->> Epoch: 2193,(no reg loss)standard loss: 0.0063642, inverse loss: 0.0013285
Valid-->> Epoch [2193/3000], Standardized Loss: 0.0124915, Inverse Loss: 0.0026075
Training -->> Epoch: 2194,(no reg loss)standard loss: 0.0062872, inverse loss: 0.0013124
Valid-->> Epoch [2194/3000], Standardized Loss: 0.0124817, Inverse Loss: 0.0026054
Training -->> Epoch: 2195,(no reg loss)standard loss: 0.0062349, inverse loss: 0.0013015
Valid-->> Epoch [2195/3000], Standardized Loss: 0.0125618, Inverse Loss: 0.0026221
Training -->> Epoch: 2196,(no reg loss)standard loss: 0.0062223, inverse loss: 0.0012988
Valid-->> Epoch [2196/3000], Standardized Loss: 0.0125104, Inverse Loss: 0.0026114
Training -->> Epoch: 2197,(no reg loss)standard loss: 0.0061509, inverse loss: 0.0012839
Valid-->> Epoch [2197/3000], Standardized Loss: 0.0125067, Inverse Loss: 0.0026106
Training -->> Epoch: 2198,(no reg loss)standard loss: 0.0061719, inverse loss: 0.0012883
Valid-->> Epoch [2198/3000], Standardized Loss: 0.0125870, Inverse Loss: 0.0026274
Training -->> Epoch: 2199,(no reg loss)standard loss: 0.0064169, inverse loss: 0.0013395
Valid-->> Epoch [2199/3000], Standardized Loss: 0.0126073, Inverse Loss: 0.0026316
Training -->> Epoch: 2200,(no reg loss)standard loss: 0.0062546, inverse loss: 0.0013056
Valid-->> Epoch [2200/3000], Standardized Loss: 0.0124634, Inverse Loss: 0.0026016
Training -->> Epoch: 2201,(no reg loss)standard loss: 0.0062054, inverse loss: 0.0012953
Valid-->> Epoch [2201/3000], Standardized Loss: 0.0125527, Inverse Loss: 0.0026202
Training -->> Epoch: 2202,(no reg loss)standard loss: 0.0062360, inverse loss: 0.0013017
Valid-->> Epoch [2202/3000], Standardized Loss: 0.0125364, Inverse Loss: 0.0026168
Training -->> Epoch: 2203,(no reg loss)standard loss: 0.0062467, inverse loss: 0.0013039
Valid-->> Epoch [2203/3000], Standardized Loss: 0.0124948, Inverse Loss: 0.0026081
Training -->> Epoch: 2204,(no reg loss)standard loss: 0.0063610, inverse loss: 0.0013278
Valid-->> Epoch [2204/3000], Standardized Loss: 0.0124361, Inverse Loss: 0.0025959
Training -->> Epoch: 2205,(no reg loss)standard loss: 0.0061258, inverse loss: 0.0012787
Valid-->> Epoch [2205/3000], Standardized Loss: 0.0126126, Inverse Loss: 0.0026327
Training -->> Epoch: 2206,(no reg loss)standard loss: 0.0064196, inverse loss: 0.0013400
Valid-->> Epoch [2206/3000], Standardized Loss: 0.0125794, Inverse Loss: 0.0026258
Training -->> Epoch: 2207,(no reg loss)standard loss: 0.0062803, inverse loss: 0.0013109
Valid-->> Epoch [2207/3000], Standardized Loss: 0.0125482, Inverse Loss: 0.0026193
Training -->> Epoch: 2208,(no reg loss)standard loss: 0.0064533, inverse loss: 0.0013471
Valid-->> Epoch [2208/3000], Standardized Loss: 0.0125752, Inverse Loss: 0.0026249
Training -->> Epoch: 2209,(no reg loss)standard loss: 0.0063444, inverse loss: 0.0013243
Valid-->> Epoch [2209/3000], Standardized Loss: 0.0125243, Inverse Loss: 0.0026143
Training -->> Epoch: 2210,(no reg loss)standard loss: 0.0061746, inverse loss: 0.0012889
Valid-->> Epoch [2210/3000], Standardized Loss: 0.0125749, Inverse Loss: 0.0026249
Training -->> Epoch: 2211,(no reg loss)standard loss: 0.0067390, inverse loss: 0.0014067
Valid-->> Epoch [2211/3000], Standardized Loss: 0.0124992, Inverse Loss: 0.0026091
Training -->> Epoch: 2212,(no reg loss)standard loss: 0.0061872, inverse loss: 0.0012915
Valid-->> Epoch [2212/3000], Standardized Loss: 0.0125352, Inverse Loss: 0.0026166
Training -->> Epoch: 2213,(no reg loss)standard loss: 0.0065911, inverse loss: 0.0013758
Valid-->> Epoch [2213/3000], Standardized Loss: 0.0125948, Inverse Loss: 0.0026290
Training -->> Epoch: 2214,(no reg loss)standard loss: 0.0064044, inverse loss: 0.0013368
Valid-->> Epoch [2214/3000], Standardized Loss: 0.0124532, Inverse Loss: 0.0025995
Training -->> Epoch: 2215,(no reg loss)standard loss: 0.0062802, inverse loss: 0.0013109
Valid-->> Epoch [2215/3000], Standardized Loss: 0.0125145, Inverse Loss: 0.0026122
Training -->> Epoch: 2216,(no reg loss)standard loss: 0.0063699, inverse loss: 0.0013296
Valid-->> Epoch [2216/3000], Standardized Loss: 0.0125134, Inverse Loss: 0.0026120
Training -->> Epoch: 2217,(no reg loss)standard loss: 0.0061935, inverse loss: 0.0012928
Valid-->> Epoch [2217/3000], Standardized Loss: 0.0125841, Inverse Loss: 0.0026268
Training -->> Epoch: 2218,(no reg loss)standard loss: 0.0064791, inverse loss: 0.0013524
Valid-->> Epoch [2218/3000], Standardized Loss: 0.0125550, Inverse Loss: 0.0026207
Training -->> Epoch: 2219,(no reg loss)standard loss: 0.0063549, inverse loss: 0.0013265
Valid-->> Epoch [2219/3000], Standardized Loss: 0.0125448, Inverse Loss: 0.0026186
Training -->> Epoch: 2220,(no reg loss)standard loss: 0.0064845, inverse loss: 0.0013536
Valid-->> Epoch [2220/3000], Standardized Loss: 0.0124696, Inverse Loss: 0.0026029
Training -->> Epoch: 2221,(no reg loss)standard loss: 0.0062746, inverse loss: 0.0013097
Valid-->> Epoch [2221/3000], Standardized Loss: 0.0125274, Inverse Loss: 0.0026150
Training -->> Epoch: 2222,(no reg loss)standard loss: 0.0065016, inverse loss: 0.0013571
Valid-->> Epoch [2222/3000], Standardized Loss: 0.0124584, Inverse Loss: 0.0026006
Training -->> Epoch: 2223,(no reg loss)standard loss: 0.0062747, inverse loss: 0.0013098
Valid-->> Epoch [2223/3000], Standardized Loss: 0.0125990, Inverse Loss: 0.0026299
Training -->> Epoch: 2224,(no reg loss)standard loss: 0.0064177, inverse loss: 0.0013396
Valid-->> Epoch [2224/3000], Standardized Loss: 0.0125511, Inverse Loss: 0.0026199
Training -->> Epoch: 2225,(no reg loss)standard loss: 0.0063393, inverse loss: 0.0013233
Valid-->> Epoch [2225/3000], Standardized Loss: 0.0126699, Inverse Loss: 0.0026447
Training -->> Epoch: 2226,(no reg loss)standard loss: 0.0065500, inverse loss: 0.0013672
Valid-->> Epoch [2226/3000], Standardized Loss: 0.0125577, Inverse Loss: 0.0026213
Training -->> Epoch: 2227,(no reg loss)standard loss: 0.0063562, inverse loss: 0.0013268
Valid-->> Epoch [2227/3000], Standardized Loss: 0.0124947, Inverse Loss: 0.0026081
Training -->> Epoch: 2228,(no reg loss)standard loss: 0.0062413, inverse loss: 0.0013028
Valid-->> Epoch [2228/3000], Standardized Loss: 0.0126062, Inverse Loss: 0.0026314
Training -->> Epoch: 2229,(no reg loss)standard loss: 0.0066839, inverse loss: 0.0013952
Valid-->> Epoch [2229/3000], Standardized Loss: 0.0126912, Inverse Loss: 0.0026492
Training -->> Epoch: 2230,(no reg loss)standard loss: 0.0062082, inverse loss: 0.0012959
Valid-->> Epoch [2230/3000], Standardized Loss: 0.0124478, Inverse Loss: 0.0025983
Training -->> Epoch: 2231,(no reg loss)standard loss: 0.0065510, inverse loss: 0.0013675
Valid-->> Epoch [2231/3000], Standardized Loss: 0.0125967, Inverse Loss: 0.0026294
Training -->> Epoch: 2232,(no reg loss)standard loss: 0.0064561, inverse loss: 0.0013476
Valid-->> Epoch [2232/3000], Standardized Loss: 0.0124524, Inverse Loss: 0.0025993
Training -->> Epoch: 2233,(no reg loss)standard loss: 0.0064630, inverse loss: 0.0013491
Valid-->> Epoch [2233/3000], Standardized Loss: 0.0125165, Inverse Loss: 0.0026127
Training -->> Epoch: 2234,(no reg loss)standard loss: 0.0064893, inverse loss: 0.0013546
Valid-->> Epoch [2234/3000], Standardized Loss: 0.0125344, Inverse Loss: 0.0026164
Training -->> Epoch: 2235,(no reg loss)standard loss: 0.0063199, inverse loss: 0.0013192
Valid-->> Epoch [2235/3000], Standardized Loss: 0.0125303, Inverse Loss: 0.0026156
Training -->> Epoch: 2236,(no reg loss)standard loss: 0.0064653, inverse loss: 0.0013496
Valid-->> Epoch [2236/3000], Standardized Loss: 0.0125955, Inverse Loss: 0.0026292
Training -->> Epoch: 2237,(no reg loss)standard loss: 0.0064558, inverse loss: 0.0013476
Valid-->> Epoch [2237/3000], Standardized Loss: 0.0125815, Inverse Loss: 0.0026262
Training -->> Epoch: 2238,(no reg loss)standard loss: 0.0063829, inverse loss: 0.0013324
Valid-->> Epoch [2238/3000], Standardized Loss: 0.0125213, Inverse Loss: 0.0026137
Training -->> Epoch: 2239,(no reg loss)standard loss: 0.0066459, inverse loss: 0.0013873
Valid-->> Epoch [2239/3000], Standardized Loss: 0.0125789, Inverse Loss: 0.0026257
Training -->> Epoch: 2240,(no reg loss)standard loss: 0.0063713, inverse loss: 0.0013299
Valid-->> Epoch [2240/3000], Standardized Loss: 0.0125671, Inverse Loss: 0.0026232
Training -->> Epoch: 2241,(no reg loss)standard loss: 0.0065097, inverse loss: 0.0013588
Valid-->> Epoch [2241/3000], Standardized Loss: 0.0125870, Inverse Loss: 0.0026274
Training -->> Epoch: 2242,(no reg loss)standard loss: 0.0064272, inverse loss: 0.0013416
Valid-->> Epoch [2242/3000], Standardized Loss: 0.0125307, Inverse Loss: 0.0026156
Training -->> Epoch: 2243,(no reg loss)standard loss: 0.0065194, inverse loss: 0.0013608
Valid-->> Epoch [2243/3000], Standardized Loss: 0.0125213, Inverse Loss: 0.0026137
Training -->> Epoch: 2244,(no reg loss)standard loss: 0.0063003, inverse loss: 0.0013151
Valid-->> Epoch [2244/3000], Standardized Loss: 0.0126339, Inverse Loss: 0.0026372
Training -->> Epoch: 2245,(no reg loss)standard loss: 0.0067439, inverse loss: 0.0014077
Valid-->> Epoch [2245/3000], Standardized Loss: 0.0124587, Inverse Loss: 0.0026006
Training -->> Epoch: 2246,(no reg loss)standard loss: 0.0064109, inverse loss: 0.0013382
Valid-->> Epoch [2246/3000], Standardized Loss: 0.0125713, Inverse Loss: 0.0026241
Training -->> Epoch: 2247,(no reg loss)standard loss: 0.0065032, inverse loss: 0.0013575
Valid-->> Epoch [2247/3000], Standardized Loss: 0.0125258, Inverse Loss: 0.0026146
Training -->> Epoch: 2248,(no reg loss)standard loss: 0.0065955, inverse loss: 0.0013767
Valid-->> Epoch [2248/3000], Standardized Loss: 0.0125368, Inverse Loss: 0.0026169
Training -->> Epoch: 2249,(no reg loss)standard loss: 0.0065672, inverse loss: 0.0013708
Valid-->> Epoch [2249/3000], Standardized Loss: 0.0125525, Inverse Loss: 0.0026202
Training -->> Epoch: 2250,(no reg loss)standard loss: 0.0061278, inverse loss: 0.0012791
Valid-->> Epoch [2250/3000], Standardized Loss: 0.0123617, Inverse Loss: 0.0025804
Valid-->> Lowest loss found at epoch 2250, loss: 0.0025804
Epoch 2250, Masked params (inverse standardized): tensor([3.229829e+01, 4.430082e+01, 6.455421e-02, 2.640301e+01, 2.772817e+01,
        1.780885e+01, 2.143702e+01, 1.632744e+00, 1.066123e+02, 2.760068e+01,
        2.619674e+01, 4.461240e+01, 2.444931e+01, 1.503784e+01, 8.461699e+01,
        2.837632e+01, 5.665428e+00, 2.983477e+01, 1.196675e+01, 3.204538e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 2251,(no reg loss)standard loss: 0.0066626, inverse loss: 0.0013907
Valid-->> Epoch [2251/3000], Standardized Loss: 0.0125899, Inverse Loss: 0.0026280
Training -->> Epoch: 2252,(no reg loss)standard loss: 0.0064607, inverse loss: 0.0013486
Valid-->> Epoch [2252/3000], Standardized Loss: 0.0124920, Inverse Loss: 0.0026076
Training -->> Epoch: 2253,(no reg loss)standard loss: 0.0064471, inverse loss: 0.0013457
Valid-->> Epoch [2253/3000], Standardized Loss: 0.0125289, Inverse Loss: 0.0026153
Training -->> Epoch: 2254,(no reg loss)standard loss: 0.0065975, inverse loss: 0.0013771
Valid-->> Epoch [2254/3000], Standardized Loss: 0.0126553, Inverse Loss: 0.0026416
Training -->> Epoch: 2255,(no reg loss)standard loss: 0.0067722, inverse loss: 0.0014136
Valid-->> Epoch [2255/3000], Standardized Loss: 0.0125611, Inverse Loss: 0.0026220
Training -->> Epoch: 2256,(no reg loss)standard loss: 0.0065426, inverse loss: 0.0013657
Valid-->> Epoch [2256/3000], Standardized Loss: 0.0125181, Inverse Loss: 0.0026130
Training -->> Epoch: 2257,(no reg loss)standard loss: 0.0064912, inverse loss: 0.0013550
Valid-->> Epoch [2257/3000], Standardized Loss: 0.0125575, Inverse Loss: 0.0026212
Training -->> Epoch: 2258,(no reg loss)standard loss: 0.0068833, inverse loss: 0.0014368
Valid-->> Epoch [2258/3000], Standardized Loss: 0.0125960, Inverse Loss: 0.0026293
Training -->> Epoch: 2259,(no reg loss)standard loss: 0.0064796, inverse loss: 0.0013525
Valid-->> Epoch [2259/3000], Standardized Loss: 0.0123939, Inverse Loss: 0.0025871
Training -->> Epoch: 2260,(no reg loss)standard loss: 0.0061877, inverse loss: 0.0012916
Valid-->> Epoch [2260/3000], Standardized Loss: 0.0123954, Inverse Loss: 0.0025874
Training -->> Epoch: 2261,(no reg loss)standard loss: 0.0067532, inverse loss: 0.0014096
Valid-->> Epoch [2261/3000], Standardized Loss: 0.0125760, Inverse Loss: 0.0026251
Training -->> Epoch: 2262,(no reg loss)standard loss: 0.0065145, inverse loss: 0.0013598
Valid-->> Epoch [2262/3000], Standardized Loss: 0.0124369, Inverse Loss: 0.0025961
Training -->> Epoch: 2263,(no reg loss)standard loss: 0.0065858, inverse loss: 0.0013747
Valid-->> Epoch [2263/3000], Standardized Loss: 0.0125404, Inverse Loss: 0.0026177
Training -->> Epoch: 2264,(no reg loss)standard loss: 0.0066420, inverse loss: 0.0013864
Valid-->> Epoch [2264/3000], Standardized Loss: 0.0125652, Inverse Loss: 0.0026229
Training -->> Epoch: 2265,(no reg loss)standard loss: 0.0066066, inverse loss: 0.0013790
Valid-->> Epoch [2265/3000], Standardized Loss: 0.0125094, Inverse Loss: 0.0026112
Training -->> Epoch: 2266,(no reg loss)standard loss: 0.0067626, inverse loss: 0.0014116
Valid-->> Epoch [2266/3000], Standardized Loss: 0.0125586, Inverse Loss: 0.0026215
Training -->> Epoch: 2267,(no reg loss)standard loss: 0.0066908, inverse loss: 0.0013966
Valid-->> Epoch [2267/3000], Standardized Loss: 0.0124667, Inverse Loss: 0.0026023
Training -->> Epoch: 2268,(no reg loss)standard loss: 0.0065214, inverse loss: 0.0013613
Valid-->> Epoch [2268/3000], Standardized Loss: 0.0125557, Inverse Loss: 0.0026209
Training -->> Epoch: 2269,(no reg loss)standard loss: 0.0066963, inverse loss: 0.0013978
Valid-->> Epoch [2269/3000], Standardized Loss: 0.0124764, Inverse Loss: 0.0026043
Training -->> Epoch: 2270,(no reg loss)standard loss: 0.0064017, inverse loss: 0.0013363
Valid-->> Epoch [2270/3000], Standardized Loss: 0.0124522, Inverse Loss: 0.0025993
Training -->> Epoch: 2271,(no reg loss)standard loss: 0.0067502, inverse loss: 0.0014090
Valid-->> Epoch [2271/3000], Standardized Loss: 0.0126160, Inverse Loss: 0.0026334
Training -->> Epoch: 2272,(no reg loss)standard loss: 0.0067131, inverse loss: 0.0014013
Valid-->> Epoch [2272/3000], Standardized Loss: 0.0125510, Inverse Loss: 0.0026199
Training -->> Epoch: 2273,(no reg loss)standard loss: 0.0065227, inverse loss: 0.0013615
Valid-->> Epoch [2273/3000], Standardized Loss: 0.0125164, Inverse Loss: 0.0026127
Training -->> Epoch: 2274,(no reg loss)standard loss: 0.0067351, inverse loss: 0.0014059
Valid-->> Epoch [2274/3000], Standardized Loss: 0.0126032, Inverse Loss: 0.0026308
Training -->> Epoch: 2275,(no reg loss)standard loss: 0.0066878, inverse loss: 0.0013960
Valid-->> Epoch [2275/3000], Standardized Loss: 0.0124565, Inverse Loss: 0.0026002
Training -->> Epoch: 2276,(no reg loss)standard loss: 0.0065769, inverse loss: 0.0013728
Valid-->> Epoch [2276/3000], Standardized Loss: 0.0124802, Inverse Loss: 0.0026051
Training -->> Epoch: 2277,(no reg loss)standard loss: 0.0063763, inverse loss: 0.0013310
Valid-->> Epoch [2277/3000], Standardized Loss: 0.0124590, Inverse Loss: 0.0026007
Training -->> Epoch: 2278,(no reg loss)standard loss: 0.0068738, inverse loss: 0.0014348
Valid-->> Epoch [2278/3000], Standardized Loss: 0.0125549, Inverse Loss: 0.0026207
Training -->> Epoch: 2279,(no reg loss)standard loss: 0.0065515, inverse loss: 0.0013676
Valid-->> Epoch [2279/3000], Standardized Loss: 0.0124175, Inverse Loss: 0.0025920
Training -->> Epoch: 2280,(no reg loss)standard loss: 0.0065444, inverse loss: 0.0013661
Valid-->> Epoch [2280/3000], Standardized Loss: 0.0124188, Inverse Loss: 0.0025923
Training -->> Epoch: 2281,(no reg loss)standard loss: 0.0063641, inverse loss: 0.0013284
Valid-->> Epoch [2281/3000], Standardized Loss: 0.0124338, Inverse Loss: 0.0025954
Training -->> Epoch: 2282,(no reg loss)standard loss: 0.0068369, inverse loss: 0.0014271
Valid-->> Epoch [2282/3000], Standardized Loss: 0.0125066, Inverse Loss: 0.0026106
Training -->> Epoch: 2283,(no reg loss)standard loss: 0.0067911, inverse loss: 0.0014176
Valid-->> Epoch [2283/3000], Standardized Loss: 0.0124507, Inverse Loss: 0.0025989
Training -->> Epoch: 2284,(no reg loss)standard loss: 0.0065279, inverse loss: 0.0013626
Valid-->> Epoch [2284/3000], Standardized Loss: 0.0124896, Inverse Loss: 0.0026071
Training -->> Epoch: 2285,(no reg loss)standard loss: 0.0068141, inverse loss: 0.0014224
Valid-->> Epoch [2285/3000], Standardized Loss: 0.0124061, Inverse Loss: 0.0025896
Training -->> Epoch: 2286,(no reg loss)standard loss: 0.0066811, inverse loss: 0.0013946
Valid-->> Epoch [2286/3000], Standardized Loss: 0.0124804, Inverse Loss: 0.0026051
Training -->> Epoch: 2287,(no reg loss)standard loss: 0.0068652, inverse loss: 0.0014330
Valid-->> Epoch [2287/3000], Standardized Loss: 0.0125074, Inverse Loss: 0.0026108
Training -->> Epoch: 2288,(no reg loss)standard loss: 0.0066543, inverse loss: 0.0013890
Valid-->> Epoch [2288/3000], Standardized Loss: 0.0124584, Inverse Loss: 0.0026005
Training -->> Epoch: 2289,(no reg loss)standard loss: 0.0065390, inverse loss: 0.0013649
Valid-->> Epoch [2289/3000], Standardized Loss: 0.0125208, Inverse Loss: 0.0026136
Training -->> Epoch: 2290,(no reg loss)standard loss: 0.0067668, inverse loss: 0.0014125
Valid-->> Epoch [2290/3000], Standardized Loss: 0.0125825, Inverse Loss: 0.0026265
Training -->> Epoch: 2291,(no reg loss)standard loss: 0.0068653, inverse loss: 0.0014331
Valid-->> Epoch [2291/3000], Standardized Loss: 0.0125233, Inverse Loss: 0.0026141
Training -->> Epoch: 2292,(no reg loss)standard loss: 0.0066339, inverse loss: 0.0013847
Valid-->> Epoch [2292/3000], Standardized Loss: 0.0124942, Inverse Loss: 0.0026080
Training -->> Epoch: 2293,(no reg loss)standard loss: 0.0067638, inverse loss: 0.0014119
Valid-->> Epoch [2293/3000], Standardized Loss: 0.0124498, Inverse Loss: 0.0025987
Training -->> Epoch: 2294,(no reg loss)standard loss: 0.0064978, inverse loss: 0.0013563
Valid-->> Epoch [2294/3000], Standardized Loss: 0.0123810, Inverse Loss: 0.0025844
Training -->> Epoch: 2295,(no reg loss)standard loss: 0.0067512, inverse loss: 0.0014092
Valid-->> Epoch [2295/3000], Standardized Loss: 0.0127513, Inverse Loss: 0.0026617
Training -->> Epoch: 2296,(no reg loss)standard loss: 0.0069120, inverse loss: 0.0014428
Valid-->> Epoch [2296/3000], Standardized Loss: 0.0124079, Inverse Loss: 0.0025900
Training -->> Epoch: 2297,(no reg loss)standard loss: 0.0067292, inverse loss: 0.0014046
Valid-->> Epoch [2297/3000], Standardized Loss: 0.0125244, Inverse Loss: 0.0026143
Training -->> Epoch: 2298,(no reg loss)standard loss: 0.0067208, inverse loss: 0.0014029
Valid-->> Epoch [2298/3000], Standardized Loss: 0.0125048, Inverse Loss: 0.0026102
Training -->> Epoch: 2299,(no reg loss)standard loss: 0.0068717, inverse loss: 0.0014344
Valid-->> Epoch [2299/3000], Standardized Loss: 0.0124955, Inverse Loss: 0.0026083
Training -->> Epoch: 2300,(no reg loss)standard loss: 0.0066342, inverse loss: 0.0013848
Valid-->> Epoch [2300/3000], Standardized Loss: 0.0125152, Inverse Loss: 0.0026124
Training -->> Epoch: 2301,(no reg loss)standard loss: 0.0068067, inverse loss: 0.0014208
Valid-->> Epoch [2301/3000], Standardized Loss: 0.0124507, Inverse Loss: 0.0025989
Training -->> Epoch: 2302,(no reg loss)standard loss: 0.0067411, inverse loss: 0.0014071
Valid-->> Epoch [2302/3000], Standardized Loss: 0.0125381, Inverse Loss: 0.0026172
Training -->> Epoch: 2303,(no reg loss)standard loss: 0.0070170, inverse loss: 0.0014647
Valid-->> Epoch [2303/3000], Standardized Loss: 0.0123923, Inverse Loss: 0.0025868
Training -->> Epoch: 2304,(no reg loss)standard loss: 0.0066792, inverse loss: 0.0013942
Valid-->> Epoch [2304/3000], Standardized Loss: 0.0124248, Inverse Loss: 0.0025935
Training -->> Epoch: 2305,(no reg loss)standard loss: 0.0068333, inverse loss: 0.0014264
Valid-->> Epoch [2305/3000], Standardized Loss: 0.0125454, Inverse Loss: 0.0026187
Training -->> Epoch: 2306,(no reg loss)standard loss: 0.0069113, inverse loss: 0.0014427
Valid-->> Epoch [2306/3000], Standardized Loss: 0.0124634, Inverse Loss: 0.0026016
Training -->> Epoch: 2307,(no reg loss)standard loss: 0.0066717, inverse loss: 0.0013926
Valid-->> Epoch [2307/3000], Standardized Loss: 0.0124685, Inverse Loss: 0.0026027
Training -->> Epoch: 2308,(no reg loss)standard loss: 0.0068760, inverse loss: 0.0014353
Valid-->> Epoch [2308/3000], Standardized Loss: 0.0125481, Inverse Loss: 0.0026193
Training -->> Epoch: 2309,(no reg loss)standard loss: 0.0067286, inverse loss: 0.0014045
Valid-->> Epoch [2309/3000], Standardized Loss: 0.0124147, Inverse Loss: 0.0025914
Training -->> Epoch: 2310,(no reg loss)standard loss: 0.0069299, inverse loss: 0.0014465
Valid-->> Epoch [2310/3000], Standardized Loss: 0.0126615, Inverse Loss: 0.0026430
Training -->> Epoch: 2311,(no reg loss)standard loss: 0.0068435, inverse loss: 0.0014285
Valid-->> Epoch [2311/3000], Standardized Loss: 0.0125112, Inverse Loss: 0.0026116
Training -->> Epoch: 2312,(no reg loss)standard loss: 0.0069287, inverse loss: 0.0014463
Valid-->> Epoch [2312/3000], Standardized Loss: 0.0124344, Inverse Loss: 0.0025955
Training -->> Epoch: 2313,(no reg loss)standard loss: 0.0067012, inverse loss: 0.0013988
Valid-->> Epoch [2313/3000], Standardized Loss: 0.0124960, Inverse Loss: 0.0026084
Training -->> Epoch: 2314,(no reg loss)standard loss: 0.0068845, inverse loss: 0.0014371
Valid-->> Epoch [2314/3000], Standardized Loss: 0.0125014, Inverse Loss: 0.0026095
Training -->> Epoch: 2315,(no reg loss)standard loss: 0.0071128, inverse loss: 0.0014847
Valid-->> Epoch [2315/3000], Standardized Loss: 0.0124780, Inverse Loss: 0.0026046
Training -->> Epoch: 2316,(no reg loss)standard loss: 0.0066328, inverse loss: 0.0013845
Valid-->> Epoch [2316/3000], Standardized Loss: 0.0124563, Inverse Loss: 0.0026001
Training -->> Epoch: 2317,(no reg loss)standard loss: 0.0068730, inverse loss: 0.0014347
Valid-->> Epoch [2317/3000], Standardized Loss: 0.0126894, Inverse Loss: 0.0026488
Training -->> Epoch: 2318,(no reg loss)standard loss: 0.0070615, inverse loss: 0.0014740
Valid-->> Epoch [2318/3000], Standardized Loss: 0.0124684, Inverse Loss: 0.0026026
Training -->> Epoch: 2319,(no reg loss)standard loss: 0.0067595, inverse loss: 0.0014110
Valid-->> Epoch [2319/3000], Standardized Loss: 0.0125035, Inverse Loss: 0.0026100
Training -->> Epoch: 2320,(no reg loss)standard loss: 0.0068601, inverse loss: 0.0014320
Valid-->> Epoch [2320/3000], Standardized Loss: 0.0124686, Inverse Loss: 0.0026027
Training -->> Epoch: 2321,(no reg loss)standard loss: 0.0069691, inverse loss: 0.0014547
Valid-->> Epoch [2321/3000], Standardized Loss: 0.0125264, Inverse Loss: 0.0026147
Training -->> Epoch: 2322,(no reg loss)standard loss: 0.0068314, inverse loss: 0.0014260
Valid-->> Epoch [2322/3000], Standardized Loss: 0.0124102, Inverse Loss: 0.0025905
Training -->> Epoch: 2323,(no reg loss)standard loss: 0.0068567, inverse loss: 0.0014313
Valid-->> Epoch [2323/3000], Standardized Loss: 0.0124778, Inverse Loss: 0.0026046
Training -->> Epoch: 2324,(no reg loss)standard loss: 0.0070074, inverse loss: 0.0014627
Valid-->> Epoch [2324/3000], Standardized Loss: 0.0124563, Inverse Loss: 0.0026001
Training -->> Epoch: 2325,(no reg loss)standard loss: 0.0068149, inverse loss: 0.0014225
Valid-->> Epoch [2325/3000], Standardized Loss: 0.0125798, Inverse Loss: 0.0026259
Training -->> Epoch: 2326,(no reg loss)standard loss: 0.0071505, inverse loss: 0.0014926
Valid-->> Epoch [2326/3000], Standardized Loss: 0.0125677, Inverse Loss: 0.0026234
Training -->> Epoch: 2327,(no reg loss)standard loss: 0.0068348, inverse loss: 0.0014267
Valid-->> Epoch [2327/3000], Standardized Loss: 0.0124088, Inverse Loss: 0.0025902
Training -->> Epoch: 2328,(no reg loss)standard loss: 0.0069183, inverse loss: 0.0014441
Valid-->> Epoch [2328/3000], Standardized Loss: 0.0125872, Inverse Loss: 0.0026274
Training -->> Epoch: 2329,(no reg loss)standard loss: 0.0067999, inverse loss: 0.0014194
Valid-->> Epoch [2329/3000], Standardized Loss: 0.0123934, Inverse Loss: 0.0025870
Training -->> Epoch: 2330,(no reg loss)standard loss: 0.0069393, inverse loss: 0.0014485
Valid-->> Epoch [2330/3000], Standardized Loss: 0.0125621, Inverse Loss: 0.0026222
Training -->> Epoch: 2331,(no reg loss)standard loss: 0.0071230, inverse loss: 0.0014868
Valid-->> Epoch [2331/3000], Standardized Loss: 0.0124715, Inverse Loss: 0.0026033
Training -->> Epoch: 2332,(no reg loss)standard loss: 0.0066688, inverse loss: 0.0013920
Valid-->> Epoch [2332/3000], Standardized Loss: 0.0123759, Inverse Loss: 0.0025833
Training -->> Epoch: 2333,(no reg loss)standard loss: 0.0069643, inverse loss: 0.0014537
Valid-->> Epoch [2333/3000], Standardized Loss: 0.0124947, Inverse Loss: 0.0026081
Training -->> Epoch: 2334,(no reg loss)standard loss: 0.0069618, inverse loss: 0.0014532
Valid-->> Epoch [2334/3000], Standardized Loss: 0.0124383, Inverse Loss: 0.0025964
Training -->> Epoch: 2335,(no reg loss)standard loss: 0.0068613, inverse loss: 0.0014322
Valid-->> Epoch [2335/3000], Standardized Loss: 0.0125178, Inverse Loss: 0.0026129
Training -->> Epoch: 2336,(no reg loss)standard loss: 0.0071187, inverse loss: 0.0014859
Valid-->> Epoch [2336/3000], Standardized Loss: 0.0124495, Inverse Loss: 0.0025987
Training -->> Epoch: 2337,(no reg loss)standard loss: 0.0067433, inverse loss: 0.0014076
Valid-->> Epoch [2337/3000], Standardized Loss: 0.0124517, Inverse Loss: 0.0025992
Training -->> Epoch: 2338,(no reg loss)standard loss: 0.0071884, inverse loss: 0.0015005
Valid-->> Epoch [2338/3000], Standardized Loss: 0.0126671, Inverse Loss: 0.0026441
Training -->> Epoch: 2339,(no reg loss)standard loss: 0.0069359, inverse loss: 0.0014478
Valid-->> Epoch [2339/3000], Standardized Loss: 0.0124180, Inverse Loss: 0.0025921
Training -->> Epoch: 2340,(no reg loss)standard loss: 0.0068115, inverse loss: 0.0014218
Valid-->> Epoch [2340/3000], Standardized Loss: 0.0124294, Inverse Loss: 0.0025945
Training -->> Epoch: 2341,(no reg loss)standard loss: 0.0071536, inverse loss: 0.0014932
Valid-->> Epoch [2341/3000], Standardized Loss: 0.0125250, Inverse Loss: 0.0026145
Training -->> Epoch: 2342,(no reg loss)standard loss: 0.0070812, inverse loss: 0.0014781
Valid-->> Epoch [2342/3000], Standardized Loss: 0.0124396, Inverse Loss: 0.0025966
Training -->> Epoch: 2343,(no reg loss)standard loss: 0.0069017, inverse loss: 0.0014406
Valid-->> Epoch [2343/3000], Standardized Loss: 0.0124587, Inverse Loss: 0.0026006
Training -->> Epoch: 2344,(no reg loss)standard loss: 0.0069868, inverse loss: 0.0014584
Valid-->> Epoch [2344/3000], Standardized Loss: 0.0125378, Inverse Loss: 0.0026171
Training -->> Epoch: 2345,(no reg loss)standard loss: 0.0072749, inverse loss: 0.0015186
Valid-->> Epoch [2345/3000], Standardized Loss: 0.0125448, Inverse Loss: 0.0026186
Training -->> Epoch: 2346,(no reg loss)standard loss: 0.0069988, inverse loss: 0.0014609
Valid-->> Epoch [2346/3000], Standardized Loss: 0.0123932, Inverse Loss: 0.0025869
Training -->> Epoch: 2347,(no reg loss)standard loss: 0.0067662, inverse loss: 0.0014124
Valid-->> Epoch [2347/3000], Standardized Loss: 0.0124632, Inverse Loss: 0.0026015
Training -->> Epoch: 2348,(no reg loss)standard loss: 0.0071233, inverse loss: 0.0014869
Valid-->> Epoch [2348/3000], Standardized Loss: 0.0126142, Inverse Loss: 0.0026331
Training -->> Epoch: 2349,(no reg loss)standard loss: 0.0071957, inverse loss: 0.0015020
Valid-->> Epoch [2349/3000], Standardized Loss: 0.0124217, Inverse Loss: 0.0025929
Training -->> Epoch: 2350,(no reg loss)standard loss: 0.0071274, inverse loss: 0.0014878
Valid-->> Epoch [2350/3000], Standardized Loss: 0.0124915, Inverse Loss: 0.0026074
Training -->> Epoch: 2351,(no reg loss)standard loss: 0.0071097, inverse loss: 0.0014841
Valid-->> Epoch [2351/3000], Standardized Loss: 0.0124619, Inverse Loss: 0.0026013
Training -->> Epoch: 2352,(no reg loss)standard loss: 0.0069888, inverse loss: 0.0014588
Valid-->> Epoch [2352/3000], Standardized Loss: 0.0124693, Inverse Loss: 0.0026028
Training -->> Epoch: 2353,(no reg loss)standard loss: 0.0071001, inverse loss: 0.0014821
Valid-->> Epoch [2353/3000], Standardized Loss: 0.0124413, Inverse Loss: 0.0025970
Training -->> Epoch: 2354,(no reg loss)standard loss: 0.0069955, inverse loss: 0.0014602
Valid-->> Epoch [2354/3000], Standardized Loss: 0.0123983, Inverse Loss: 0.0025880
Training -->> Epoch: 2355,(no reg loss)standard loss: 0.0073391, inverse loss: 0.0015319
Valid-->> Epoch [2355/3000], Standardized Loss: 0.0124629, Inverse Loss: 0.0026015
Training -->> Epoch: 2356,(no reg loss)standard loss: 0.0068393, inverse loss: 0.0014276
Valid-->> Epoch [2356/3000], Standardized Loss: 0.0124386, Inverse Loss: 0.0025964
Training -->> Epoch: 2357,(no reg loss)standard loss: 0.0071551, inverse loss: 0.0014935
Valid-->> Epoch [2357/3000], Standardized Loss: 0.0124731, Inverse Loss: 0.0026036
Training -->> Epoch: 2358,(no reg loss)standard loss: 0.0071189, inverse loss: 0.0014860
Valid-->> Epoch [2358/3000], Standardized Loss: 0.0123955, Inverse Loss: 0.0025874
Training -->> Epoch: 2359,(no reg loss)standard loss: 0.0073198, inverse loss: 0.0015279
Valid-->> Epoch [2359/3000], Standardized Loss: 0.0124700, Inverse Loss: 0.0026030
Training -->> Epoch: 2360,(no reg loss)standard loss: 0.0070211, inverse loss: 0.0014656
Valid-->> Epoch [2360/3000], Standardized Loss: 0.0123725, Inverse Loss: 0.0025826
Training -->> Epoch: 2361,(no reg loss)standard loss: 0.0070445, inverse loss: 0.0014704
Valid-->> Epoch [2361/3000], Standardized Loss: 0.0125776, Inverse Loss: 0.0026254
Training -->> Epoch: 2362,(no reg loss)standard loss: 0.0073823, inverse loss: 0.0015410
Valid-->> Epoch [2362/3000], Standardized Loss: 0.0124265, Inverse Loss: 0.0025939
Training -->> Epoch: 2363,(no reg loss)standard loss: 0.0069473, inverse loss: 0.0014502
Valid-->> Epoch [2363/3000], Standardized Loss: 0.0124236, Inverse Loss: 0.0025933
Training -->> Epoch: 2364,(no reg loss)standard loss: 0.0072661, inverse loss: 0.0015167
Valid-->> Epoch [2364/3000], Standardized Loss: 0.0126151, Inverse Loss: 0.0026333
Training -->> Epoch: 2365,(no reg loss)standard loss: 0.0070998, inverse loss: 0.0014820
Valid-->> Epoch [2365/3000], Standardized Loss: 0.0124489, Inverse Loss: 0.0025986
Training -->> Epoch: 2366,(no reg loss)standard loss: 0.0073328, inverse loss: 0.0015306
Valid-->> Epoch [2366/3000], Standardized Loss: 0.0124135, Inverse Loss: 0.0025912
Training -->> Epoch: 2367,(no reg loss)standard loss: 0.0069148, inverse loss: 0.0014434
Valid-->> Epoch [2367/3000], Standardized Loss: 0.0124347, Inverse Loss: 0.0025956
Training -->> Epoch: 2368,(no reg loss)standard loss: 0.0071372, inverse loss: 0.0014898
Valid-->> Epoch [2368/3000], Standardized Loss: 0.0124767, Inverse Loss: 0.0026044
Training -->> Epoch: 2369,(no reg loss)standard loss: 0.0071135, inverse loss: 0.0014849
Valid-->> Epoch [2369/3000], Standardized Loss: 0.0124906, Inverse Loss: 0.0026073
Training -->> Epoch: 2370,(no reg loss)standard loss: 0.0073580, inverse loss: 0.0015359
Valid-->> Epoch [2370/3000], Standardized Loss: 0.0125430, Inverse Loss: 0.0026182
Training -->> Epoch: 2371,(no reg loss)standard loss: 0.0072373, inverse loss: 0.0015107
Valid-->> Epoch [2371/3000], Standardized Loss: 0.0123659, Inverse Loss: 0.0025812
Training -->> Epoch: 2372,(no reg loss)standard loss: 0.0069813, inverse loss: 0.0014573
Valid-->> Epoch [2372/3000], Standardized Loss: 0.0126168, Inverse Loss: 0.0026336
Training -->> Epoch: 2373,(no reg loss)standard loss: 0.0074374, inverse loss: 0.0015525
Valid-->> Epoch [2373/3000], Standardized Loss: 0.0124614, Inverse Loss: 0.0026012
Training -->> Epoch: 2374,(no reg loss)standard loss: 0.0072051, inverse loss: 0.0015040
Valid-->> Epoch [2374/3000], Standardized Loss: 0.0124346, Inverse Loss: 0.0025956
Training -->> Epoch: 2375,(no reg loss)standard loss: 0.0071070, inverse loss: 0.0014835
Valid-->> Epoch [2375/3000], Standardized Loss: 0.0124032, Inverse Loss: 0.0025890
Training -->> Epoch: 2376,(no reg loss)standard loss: 0.0072426, inverse loss: 0.0015118
Valid-->> Epoch [2376/3000], Standardized Loss: 0.0126090, Inverse Loss: 0.0026320
Training -->> Epoch: 2377,(no reg loss)standard loss: 0.0071624, inverse loss: 0.0014951
Valid-->> Epoch [2377/3000], Standardized Loss: 0.0124085, Inverse Loss: 0.0025901
Training -->> Epoch: 2378,(no reg loss)standard loss: 0.0072526, inverse loss: 0.0015139
Valid-->> Epoch [2378/3000], Standardized Loss: 0.0124431, Inverse Loss: 0.0025974
Training -->> Epoch: 2379,(no reg loss)standard loss: 0.0072525, inverse loss: 0.0015139
Valid-->> Epoch [2379/3000], Standardized Loss: 0.0125615, Inverse Loss: 0.0026221
Training -->> Epoch: 2380,(no reg loss)standard loss: 0.0073168, inverse loss: 0.0015273
Valid-->> Epoch [2380/3000], Standardized Loss: 0.0124957, Inverse Loss: 0.0026083
Training -->> Epoch: 2381,(no reg loss)standard loss: 0.0071100, inverse loss: 0.0014841
Valid-->> Epoch [2381/3000], Standardized Loss: 0.0124579, Inverse Loss: 0.0026004
Training -->> Epoch: 2382,(no reg loss)standard loss: 0.0074279, inverse loss: 0.0015505
Valid-->> Epoch [2382/3000], Standardized Loss: 0.0124231, Inverse Loss: 0.0025932
Training -->> Epoch: 2383,(no reg loss)standard loss: 0.0072293, inverse loss: 0.0015090
Valid-->> Epoch [2383/3000], Standardized Loss: 0.0123920, Inverse Loss: 0.0025867
Training -->> Epoch: 2384,(no reg loss)standard loss: 0.0071871, inverse loss: 0.0015002
Valid-->> Epoch [2384/3000], Standardized Loss: 0.0124230, Inverse Loss: 0.0025932
Training -->> Epoch: 2385,(no reg loss)standard loss: 0.0073668, inverse loss: 0.0015377
Valid-->> Epoch [2385/3000], Standardized Loss: 0.0124978, Inverse Loss: 0.0026088
Training -->> Epoch: 2386,(no reg loss)standard loss: 0.0072443, inverse loss: 0.0015122
Valid-->> Epoch [2386/3000], Standardized Loss: 0.0123721, Inverse Loss: 0.0025825
Training -->> Epoch: 2387,(no reg loss)standard loss: 0.0072498, inverse loss: 0.0015133
Valid-->> Epoch [2387/3000], Standardized Loss: 0.0124628, Inverse Loss: 0.0026015
Training -->> Epoch: 2388,(no reg loss)standard loss: 0.0073478, inverse loss: 0.0015338
Valid-->> Epoch [2388/3000], Standardized Loss: 0.0124341, Inverse Loss: 0.0025955
Training -->> Epoch: 2389,(no reg loss)standard loss: 0.0073203, inverse loss: 0.0015280
Valid-->> Epoch [2389/3000], Standardized Loss: 0.0124067, Inverse Loss: 0.0025898
Training -->> Epoch: 2390,(no reg loss)standard loss: 0.0070893, inverse loss: 0.0014798
Valid-->> Epoch [2390/3000], Standardized Loss: 0.0124072, Inverse Loss: 0.0025899
Training -->> Epoch: 2391,(no reg loss)standard loss: 0.0072638, inverse loss: 0.0015162
Valid-->> Epoch [2391/3000], Standardized Loss: 0.0124316, Inverse Loss: 0.0025949
Training -->> Epoch: 2392,(no reg loss)standard loss: 0.0072973, inverse loss: 0.0015232
Valid-->> Epoch [2392/3000], Standardized Loss: 0.0125341, Inverse Loss: 0.0026163
Training -->> Epoch: 2393,(no reg loss)standard loss: 0.0071738, inverse loss: 0.0014974
Valid-->> Epoch [2393/3000], Standardized Loss: 0.0124544, Inverse Loss: 0.0025997
Training -->> Epoch: 2394,(no reg loss)standard loss: 0.0074888, inverse loss: 0.0015632
Valid-->> Epoch [2394/3000], Standardized Loss: 0.0125164, Inverse Loss: 0.0026127
Training -->> Epoch: 2395,(no reg loss)standard loss: 0.0074057, inverse loss: 0.0015459
Valid-->> Epoch [2395/3000], Standardized Loss: 0.0124301, Inverse Loss: 0.0025946
Training -->> Epoch: 2396,(no reg loss)standard loss: 0.0072847, inverse loss: 0.0015206
Valid-->> Epoch [2396/3000], Standardized Loss: 0.0124026, Inverse Loss: 0.0025889
Training -->> Epoch: 2397,(no reg loss)standard loss: 0.0071585, inverse loss: 0.0014943
Valid-->> Epoch [2397/3000], Standardized Loss: 0.0124220, Inverse Loss: 0.0025929
Training -->> Epoch: 2398,(no reg loss)standard loss: 0.0074185, inverse loss: 0.0015485
Valid-->> Epoch [2398/3000], Standardized Loss: 0.0124428, Inverse Loss: 0.0025973
Training -->> Epoch: 2399,(no reg loss)standard loss: 0.0073653, inverse loss: 0.0015374
Valid-->> Epoch [2399/3000], Standardized Loss: 0.0124740, Inverse Loss: 0.0026038
Training -->> Epoch: 2400,(no reg loss)standard loss: 0.0074882, inverse loss: 0.0015631
Valid-->> Epoch [2400/3000], Standardized Loss: 0.0123628, Inverse Loss: 0.0025806
Training -->> Epoch: 2401,(no reg loss)standard loss: 0.0071144, inverse loss: 0.0014850
Valid-->> Epoch [2401/3000], Standardized Loss: 0.0123897, Inverse Loss: 0.0025862
Training -->> Epoch: 2402,(no reg loss)standard loss: 0.0073848, inverse loss: 0.0015415
Valid-->> Epoch [2402/3000], Standardized Loss: 0.0125903, Inverse Loss: 0.0026281
Training -->> Epoch: 2403,(no reg loss)standard loss: 0.0075079, inverse loss: 0.0015672
Valid-->> Epoch [2403/3000], Standardized Loss: 0.0125031, Inverse Loss: 0.0026099
Training -->> Epoch: 2404,(no reg loss)standard loss: 0.0074649, inverse loss: 0.0015582
Valid-->> Epoch [2404/3000], Standardized Loss: 0.0124007, Inverse Loss: 0.0025885
Training -->> Epoch: 2405,(no reg loss)standard loss: 0.0074957, inverse loss: 0.0015646
Valid-->> Epoch [2405/3000], Standardized Loss: 0.0126053, Inverse Loss: 0.0026312
Training -->> Epoch: 2406,(no reg loss)standard loss: 0.0073668, inverse loss: 0.0015377
Valid-->> Epoch [2406/3000], Standardized Loss: 0.0123700, Inverse Loss: 0.0025821
Training -->> Epoch: 2407,(no reg loss)standard loss: 0.0072377, inverse loss: 0.0015108
Valid-->> Epoch [2407/3000], Standardized Loss: 0.0124953, Inverse Loss: 0.0026083
Training -->> Epoch: 2408,(no reg loss)standard loss: 0.0074665, inverse loss: 0.0015585
Valid-->> Epoch [2408/3000], Standardized Loss: 0.0124610, Inverse Loss: 0.0026011
Training -->> Epoch: 2409,(no reg loss)standard loss: 0.0074818, inverse loss: 0.0015617
Valid-->> Epoch [2409/3000], Standardized Loss: 0.0124140, Inverse Loss: 0.0025913
Training -->> Epoch: 2410,(no reg loss)standard loss: 0.0073337, inverse loss: 0.0015308
Valid-->> Epoch [2410/3000], Standardized Loss: 0.0123922, Inverse Loss: 0.0025867
Training -->> Epoch: 2411,(no reg loss)standard loss: 0.0073254, inverse loss: 0.0015291
Valid-->> Epoch [2411/3000], Standardized Loss: 0.0123413, Inverse Loss: 0.0025761
Valid-->> Lowest loss found at epoch 2411, loss: 0.0025761
Epoch 2411, Masked params (inverse standardized): tensor([3.229929e+01, 4.430486e+01, 7.132149e-02, 2.640723e+01, 2.773113e+01,
        1.781401e+01, 2.143844e+01, 1.638752e+00, 1.066154e+02, 2.760458e+01,
        2.620015e+01, 4.461834e+01, 2.444987e+01, 1.504195e+01, 8.461714e+01,
        2.837748e+01, 5.671045e+00, 2.983498e+01, 1.197236e+01, 3.204657e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 2412,(no reg loss)standard loss: 0.0074206, inverse loss: 0.0015490
Valid-->> Epoch [2412/3000], Standardized Loss: 0.0124007, Inverse Loss: 0.0025885
Training -->> Epoch: 2413,(no reg loss)standard loss: 0.0072172, inverse loss: 0.0015065
Valid-->> Epoch [2413/3000], Standardized Loss: 0.0123973, Inverse Loss: 0.0025878
Training -->> Epoch: 2414,(no reg loss)standard loss: 0.0075462, inverse loss: 0.0015752
Valid-->> Epoch [2414/3000], Standardized Loss: 0.0125284, Inverse Loss: 0.0026152
Training -->> Epoch: 2415,(no reg loss)standard loss: 0.0074016, inverse loss: 0.0015450
Valid-->> Epoch [2415/3000], Standardized Loss: 0.0123994, Inverse Loss: 0.0025882
Training -->> Epoch: 2416,(no reg loss)standard loss: 0.0075378, inverse loss: 0.0015734
Valid-->> Epoch [2416/3000], Standardized Loss: 0.0125399, Inverse Loss: 0.0026176
Training -->> Epoch: 2417,(no reg loss)standard loss: 0.0074831, inverse loss: 0.0015620
Valid-->> Epoch [2417/3000], Standardized Loss: 0.0124164, Inverse Loss: 0.0025918
Training -->> Epoch: 2418,(no reg loss)standard loss: 0.0074045, inverse loss: 0.0015456
Valid-->> Epoch [2418/3000], Standardized Loss: 0.0125204, Inverse Loss: 0.0026135
Training -->> Epoch: 2419,(no reg loss)standard loss: 0.0075483, inverse loss: 0.0015756
Valid-->> Epoch [2419/3000], Standardized Loss: 0.0124507, Inverse Loss: 0.0025989
Training -->> Epoch: 2420,(no reg loss)standard loss: 0.0075536, inverse loss: 0.0015767
Valid-->> Epoch [2420/3000], Standardized Loss: 0.0123693, Inverse Loss: 0.0025820
Training -->> Epoch: 2421,(no reg loss)standard loss: 0.0074432, inverse loss: 0.0015537
Valid-->> Epoch [2421/3000], Standardized Loss: 0.0123898, Inverse Loss: 0.0025862
Training -->> Epoch: 2422,(no reg loss)standard loss: 0.0073880, inverse loss: 0.0015422
Valid-->> Epoch [2422/3000], Standardized Loss: 0.0124521, Inverse Loss: 0.0025992
Training -->> Epoch: 2423,(no reg loss)standard loss: 0.0074067, inverse loss: 0.0015461
Valid-->> Epoch [2423/3000], Standardized Loss: 0.0124633, Inverse Loss: 0.0026016
Training -->> Epoch: 2424,(no reg loss)standard loss: 0.0076088, inverse loss: 0.0015882
Valid-->> Epoch [2424/3000], Standardized Loss: 0.0124890, Inverse Loss: 0.0026069
Training -->> Epoch: 2425,(no reg loss)standard loss: 0.0074119, inverse loss: 0.0015471
Valid-->> Epoch [2425/3000], Standardized Loss: 0.0124769, Inverse Loss: 0.0026044
Training -->> Epoch: 2426,(no reg loss)standard loss: 0.0075779, inverse loss: 0.0015818
Valid-->> Epoch [2426/3000], Standardized Loss: 0.0124155, Inverse Loss: 0.0025916
Training -->> Epoch: 2427,(no reg loss)standard loss: 0.0077711, inverse loss: 0.0016221
Valid-->> Epoch [2427/3000], Standardized Loss: 0.0124864, Inverse Loss: 0.0026064
Training -->> Epoch: 2428,(no reg loss)standard loss: 0.0073907, inverse loss: 0.0015427
Valid-->> Epoch [2428/3000], Standardized Loss: 0.0123615, Inverse Loss: 0.0025803
Training -->> Epoch: 2429,(no reg loss)standard loss: 0.0074926, inverse loss: 0.0015640
Valid-->> Epoch [2429/3000], Standardized Loss: 0.0125093, Inverse Loss: 0.0026112
Training -->> Epoch: 2430,(no reg loss)standard loss: 0.0076741, inverse loss: 0.0016019
Valid-->> Epoch [2430/3000], Standardized Loss: 0.0123279, Inverse Loss: 0.0025733
Valid-->> Lowest loss found at epoch 2430, loss: 0.0025733
Epoch 2430, Masked params (inverse standardized): tensor([3.230145e+01, 4.430412e+01, 7.285690e-02, 2.640767e+01, 2.773141e+01,
        1.781607e+01, 2.144044e+01, 1.640249e+00, 1.066143e+02, 2.760497e+01,
        2.620051e+01, 4.462088e+01, 2.445049e+01, 1.504531e+01, 8.461790e+01,
        2.837774e+01, 5.672892e+00, 2.983606e+01, 1.197525e+01, 3.204850e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 2431,(no reg loss)standard loss: 0.0072686, inverse loss: 0.0015172
Valid-->> Epoch [2431/3000], Standardized Loss: 0.0124699, Inverse Loss: 0.0026029
Training -->> Epoch: 2432,(no reg loss)standard loss: 0.0077791, inverse loss: 0.0016238
Valid-->> Epoch [2432/3000], Standardized Loss: 0.0125635, Inverse Loss: 0.0026225
Training -->> Epoch: 2433,(no reg loss)standard loss: 0.0075611, inverse loss: 0.0015783
Valid-->> Epoch [2433/3000], Standardized Loss: 0.0123948, Inverse Loss: 0.0025873
Training -->> Epoch: 2434,(no reg loss)standard loss: 0.0076500, inverse loss: 0.0015968
Valid-->> Epoch [2434/3000], Standardized Loss: 0.0125044, Inverse Loss: 0.0026101
Training -->> Epoch: 2435,(no reg loss)standard loss: 0.0076083, inverse loss: 0.0015881
Valid-->> Epoch [2435/3000], Standardized Loss: 0.0123932, Inverse Loss: 0.0025869
Training -->> Epoch: 2436,(no reg loss)standard loss: 0.0076805, inverse loss: 0.0016032
Valid-->> Epoch [2436/3000], Standardized Loss: 0.0125170, Inverse Loss: 0.0026128
Training -->> Epoch: 2437,(no reg loss)standard loss: 0.0073313, inverse loss: 0.0015303
Valid-->> Epoch [2437/3000], Standardized Loss: 0.0123965, Inverse Loss: 0.0025876
Training -->> Epoch: 2438,(no reg loss)standard loss: 0.0077513, inverse loss: 0.0016180
Valid-->> Epoch [2438/3000], Standardized Loss: 0.0125690, Inverse Loss: 0.0026236
Training -->> Epoch: 2439,(no reg loss)standard loss: 0.0076389, inverse loss: 0.0015945
Valid-->> Epoch [2439/3000], Standardized Loss: 0.0124115, Inverse Loss: 0.0025907
Training -->> Epoch: 2440,(no reg loss)standard loss: 0.0074441, inverse loss: 0.0015539
Valid-->> Epoch [2440/3000], Standardized Loss: 0.0124489, Inverse Loss: 0.0025986
Training -->> Epoch: 2441,(no reg loss)standard loss: 0.0077565, inverse loss: 0.0016191
Valid-->> Epoch [2441/3000], Standardized Loss: 0.0125562, Inverse Loss: 0.0026210
Training -->> Epoch: 2442,(no reg loss)standard loss: 0.0076719, inverse loss: 0.0016014
Valid-->> Epoch [2442/3000], Standardized Loss: 0.0123941, Inverse Loss: 0.0025871
Training -->> Epoch: 2443,(no reg loss)standard loss: 0.0077283, inverse loss: 0.0016132
Valid-->> Epoch [2443/3000], Standardized Loss: 0.0125185, Inverse Loss: 0.0026131
Training -->> Epoch: 2444,(no reg loss)standard loss: 0.0075310, inverse loss: 0.0015720
Valid-->> Epoch [2444/3000], Standardized Loss: 0.0123957, Inverse Loss: 0.0025875
Training -->> Epoch: 2445,(no reg loss)standard loss: 0.0079368, inverse loss: 0.0016567
Valid-->> Epoch [2445/3000], Standardized Loss: 0.0124410, Inverse Loss: 0.0025969
Training -->> Epoch: 2446,(no reg loss)standard loss: 0.0075251, inverse loss: 0.0015708
Valid-->> Epoch [2446/3000], Standardized Loss: 0.0124898, Inverse Loss: 0.0026071
Training -->> Epoch: 2447,(no reg loss)standard loss: 0.0078034, inverse loss: 0.0016289
Valid-->> Epoch [2447/3000], Standardized Loss: 0.0124318, Inverse Loss: 0.0025950
Training -->> Epoch: 2448,(no reg loss)standard loss: 0.0076514, inverse loss: 0.0015971
Valid-->> Epoch [2448/3000], Standardized Loss: 0.0123516, Inverse Loss: 0.0025783
Training -->> Epoch: 2449,(no reg loss)standard loss: 0.0074226, inverse loss: 0.0015494
Valid-->> Epoch [2449/3000], Standardized Loss: 0.0123430, Inverse Loss: 0.0025765
Training -->> Epoch: 2450,(no reg loss)standard loss: 0.0077366, inverse loss: 0.0016149
Valid-->> Epoch [2450/3000], Standardized Loss: 0.0123762, Inverse Loss: 0.0025834
Training -->> Epoch: 2451,(no reg loss)standard loss: 0.0075851, inverse loss: 0.0015833
Valid-->> Epoch [2451/3000], Standardized Loss: 0.0124062, Inverse Loss: 0.0025897
Training -->> Epoch: 2452,(no reg loss)standard loss: 0.0076556, inverse loss: 0.0015980
Valid-->> Epoch [2452/3000], Standardized Loss: 0.0123869, Inverse Loss: 0.0025856
Training -->> Epoch: 2453,(no reg loss)standard loss: 0.0076741, inverse loss: 0.0016019
Valid-->> Epoch [2453/3000], Standardized Loss: 0.0124170, Inverse Loss: 0.0025919
Training -->> Epoch: 2454,(no reg loss)standard loss: 0.0077688, inverse loss: 0.0016216
Valid-->> Epoch [2454/3000], Standardized Loss: 0.0124171, Inverse Loss: 0.0025919
Training -->> Epoch: 2455,(no reg loss)standard loss: 0.0075592, inverse loss: 0.0015779
Valid-->> Epoch [2455/3000], Standardized Loss: 0.0124796, Inverse Loss: 0.0026050
Training -->> Epoch: 2456,(no reg loss)standard loss: 0.0077295, inverse loss: 0.0016134
Valid-->> Epoch [2456/3000], Standardized Loss: 0.0124126, Inverse Loss: 0.0025910
Training -->> Epoch: 2457,(no reg loss)standard loss: 0.0075928, inverse loss: 0.0015849
Valid-->> Epoch [2457/3000], Standardized Loss: 0.0124323, Inverse Loss: 0.0025951
Training -->> Epoch: 2458,(no reg loss)standard loss: 0.0077200, inverse loss: 0.0016115
Valid-->> Epoch [2458/3000], Standardized Loss: 0.0124031, Inverse Loss: 0.0025890
Training -->> Epoch: 2459,(no reg loss)standard loss: 0.0075034, inverse loss: 0.0015662
Valid-->> Epoch [2459/3000], Standardized Loss: 0.0124332, Inverse Loss: 0.0025953
Training -->> Epoch: 2460,(no reg loss)standard loss: 0.0078306, inverse loss: 0.0016346
Valid-->> Epoch [2460/3000], Standardized Loss: 0.0124248, Inverse Loss: 0.0025935
Training -->> Epoch: 2461,(no reg loss)standard loss: 0.0077737, inverse loss: 0.0016227
Valid-->> Epoch [2461/3000], Standardized Loss: 0.0124052, Inverse Loss: 0.0025894
Training -->> Epoch: 2462,(no reg loss)standard loss: 0.0076174, inverse loss: 0.0015900
Valid-->> Epoch [2462/3000], Standardized Loss: 0.0124300, Inverse Loss: 0.0025946
Training -->> Epoch: 2463,(no reg loss)standard loss: 0.0078802, inverse loss: 0.0016449
Valid-->> Epoch [2463/3000], Standardized Loss: 0.0125580, Inverse Loss: 0.0026214
Training -->> Epoch: 2464,(no reg loss)standard loss: 0.0079403, inverse loss: 0.0016574
Valid-->> Epoch [2464/3000], Standardized Loss: 0.0124078, Inverse Loss: 0.0025900
Training -->> Epoch: 2465,(no reg loss)standard loss: 0.0077061, inverse loss: 0.0016086
Valid-->> Epoch [2465/3000], Standardized Loss: 0.0123187, Inverse Loss: 0.0025714
Valid-->> Lowest loss found at epoch 2465, loss: 0.0025714
Epoch 2465, Masked params (inverse standardized): tensor([3.230096e+01, 4.430458e+01, 7.076263e-02, 2.640928e+01, 2.773250e+01,
        1.781224e+01, 2.143979e+01, 1.636425e+00, 1.066150e+02, 2.760614e+01,
        2.620212e+01, 4.461744e+01, 2.445129e+01, 1.504165e+01, 8.461900e+01,
        2.837858e+01, 5.671314e+00, 2.983679e+01, 1.197124e+01, 3.204806e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 2466,(no reg loss)standard loss: 0.0076882, inverse loss: 0.0016048
Valid-->> Epoch [2466/3000], Standardized Loss: 0.0123889, Inverse Loss: 0.0025860
Training -->> Epoch: 2467,(no reg loss)standard loss: 0.0075967, inverse loss: 0.0015857
Valid-->> Epoch [2467/3000], Standardized Loss: 0.0124189, Inverse Loss: 0.0025923
Training -->> Epoch: 2468,(no reg loss)standard loss: 0.0078891, inverse loss: 0.0016468
Valid-->> Epoch [2468/3000], Standardized Loss: 0.0123782, Inverse Loss: 0.0025838
Training -->> Epoch: 2469,(no reg loss)standard loss: 0.0076425, inverse loss: 0.0015953
Valid-->> Epoch [2469/3000], Standardized Loss: 0.0124811, Inverse Loss: 0.0026053
Training -->> Epoch: 2470,(no reg loss)standard loss: 0.0078629, inverse loss: 0.0016413
Valid-->> Epoch [2470/3000], Standardized Loss: 0.0124689, Inverse Loss: 0.0026027
Training -->> Epoch: 2471,(no reg loss)standard loss: 0.0080568, inverse loss: 0.0016818
Valid-->> Epoch [2471/3000], Standardized Loss: 0.0124052, Inverse Loss: 0.0025894
Training -->> Epoch: 2472,(no reg loss)standard loss: 0.0076049, inverse loss: 0.0015874
Valid-->> Epoch [2472/3000], Standardized Loss: 0.0124303, Inverse Loss: 0.0025947
Training -->> Epoch: 2473,(no reg loss)standard loss: 0.0079560, inverse loss: 0.0016607
Valid-->> Epoch [2473/3000], Standardized Loss: 0.0124608, Inverse Loss: 0.0026010
Training -->> Epoch: 2474,(no reg loss)standard loss: 0.0077628, inverse loss: 0.0016204
Valid-->> Epoch [2474/3000], Standardized Loss: 0.0123729, Inverse Loss: 0.0025827
Training -->> Epoch: 2475,(no reg loss)standard loss: 0.0078257, inverse loss: 0.0016335
Valid-->> Epoch [2475/3000], Standardized Loss: 0.0124899, Inverse Loss: 0.0026071
Training -->> Epoch: 2476,(no reg loss)standard loss: 0.0078378, inverse loss: 0.0016360
Valid-->> Epoch [2476/3000], Standardized Loss: 0.0124402, Inverse Loss: 0.0025967
Training -->> Epoch: 2477,(no reg loss)standard loss: 0.0080421, inverse loss: 0.0016787
Valid-->> Epoch [2477/3000], Standardized Loss: 0.0124000, Inverse Loss: 0.0025884
Training -->> Epoch: 2478,(no reg loss)standard loss: 0.0077569, inverse loss: 0.0016192
Valid-->> Epoch [2478/3000], Standardized Loss: 0.0124192, Inverse Loss: 0.0025924
Training -->> Epoch: 2479,(no reg loss)standard loss: 0.0078476, inverse loss: 0.0016381
Valid-->> Epoch [2479/3000], Standardized Loss: 0.0124086, Inverse Loss: 0.0025902
Training -->> Epoch: 2480,(no reg loss)standard loss: 0.0079282, inverse loss: 0.0016549
Valid-->> Epoch [2480/3000], Standardized Loss: 0.0124065, Inverse Loss: 0.0025897
Training -->> Epoch: 2481,(no reg loss)standard loss: 0.0076709, inverse loss: 0.0016012
Valid-->> Epoch [2481/3000], Standardized Loss: 0.0124202, Inverse Loss: 0.0025926
Training -->> Epoch: 2482,(no reg loss)standard loss: 0.0082981, inverse loss: 0.0017321
Valid-->> Epoch [2482/3000], Standardized Loss: 0.0124752, Inverse Loss: 0.0026041
Training -->> Epoch: 2483,(no reg loss)standard loss: 0.0076618, inverse loss: 0.0015993
Valid-->> Epoch [2483/3000], Standardized Loss: 0.0123327, Inverse Loss: 0.0025743
Training -->> Epoch: 2484,(no reg loss)standard loss: 0.0079330, inverse loss: 0.0016559
Valid-->> Epoch [2484/3000], Standardized Loss: 0.0124802, Inverse Loss: 0.0026051
Training -->> Epoch: 2485,(no reg loss)standard loss: 0.0078950, inverse loss: 0.0016480
Valid-->> Epoch [2485/3000], Standardized Loss: 0.0124069, Inverse Loss: 0.0025898
Training -->> Epoch: 2486,(no reg loss)standard loss: 0.0077299, inverse loss: 0.0016135
Valid-->> Epoch [2486/3000], Standardized Loss: 0.0123711, Inverse Loss: 0.0025823
Training -->> Epoch: 2487,(no reg loss)standard loss: 0.0078900, inverse loss: 0.0016469
Valid-->> Epoch [2487/3000], Standardized Loss: 0.0124551, Inverse Loss: 0.0025999
Training -->> Epoch: 2488,(no reg loss)standard loss: 0.0078854, inverse loss: 0.0016460
Valid-->> Epoch [2488/3000], Standardized Loss: 0.0124756, Inverse Loss: 0.0026041
Training -->> Epoch: 2489,(no reg loss)standard loss: 0.0081245, inverse loss: 0.0016959
Valid-->> Epoch [2489/3000], Standardized Loss: 0.0124768, Inverse Loss: 0.0026044
Training -->> Epoch: 2490,(no reg loss)standard loss: 0.0078426, inverse loss: 0.0016371
Valid-->> Epoch [2490/3000], Standardized Loss: 0.0124237, Inverse Loss: 0.0025933
Training -->> Epoch: 2491,(no reg loss)standard loss: 0.0083529, inverse loss: 0.0017436
Valid-->> Epoch [2491/3000], Standardized Loss: 0.0124320, Inverse Loss: 0.0025950
Training -->> Epoch: 2492,(no reg loss)standard loss: 0.0077632, inverse loss: 0.0016205
Valid-->> Epoch [2492/3000], Standardized Loss: 0.0123670, Inverse Loss: 0.0025815
Training -->> Epoch: 2493,(no reg loss)standard loss: 0.0080016, inverse loss: 0.0016702
Valid-->> Epoch [2493/3000], Standardized Loss: 0.0124871, Inverse Loss: 0.0026065
Training -->> Epoch: 2494,(no reg loss)standard loss: 0.0080478, inverse loss: 0.0016799
Valid-->> Epoch [2494/3000], Standardized Loss: 0.0124428, Inverse Loss: 0.0025973
Training -->> Epoch: 2495,(no reg loss)standard loss: 0.0079383, inverse loss: 0.0016570
Valid-->> Epoch [2495/3000], Standardized Loss: 0.0124254, Inverse Loss: 0.0025937
Training -->> Epoch: 2496,(no reg loss)standard loss: 0.0080171, inverse loss: 0.0016735
Valid-->> Epoch [2496/3000], Standardized Loss: 0.0123107, Inverse Loss: 0.0025697
Valid-->> Lowest loss found at epoch 2496, loss: 0.0025697
Epoch 2496, Masked params (inverse standardized): tensor([3.230087e+01, 4.430387e+01, 7.254791e-02, 2.641020e+01, 2.773334e+01,
        1.781429e+01, 2.143983e+01, 1.639292e+00, 1.066140e+02, 2.760721e+01,
        2.620288e+01, 4.462000e+01, 2.445118e+01, 1.504354e+01, 8.461642e+01,
        2.837888e+01, 5.672594e+00, 2.983651e+01, 1.197392e+01, 3.204799e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 2497,(no reg loss)standard loss: 0.0077892, inverse loss: 0.0016259
Valid-->> Epoch [2497/3000], Standardized Loss: 0.0125429, Inverse Loss: 0.0026182
Training -->> Epoch: 2498,(no reg loss)standard loss: 0.0082187, inverse loss: 0.0017156
Valid-->> Epoch [2498/3000], Standardized Loss: 0.0125578, Inverse Loss: 0.0026213
Training -->> Epoch: 2499,(no reg loss)standard loss: 0.0079878, inverse loss: 0.0016674
Valid-->> Epoch [2499/3000], Standardized Loss: 0.0123695, Inverse Loss: 0.0025820
Training -->> Epoch: 2500,(no reg loss)standard loss: 0.0080226, inverse loss: 0.0016746
Valid-->> Epoch [2500/3000], Standardized Loss: 0.0124603, Inverse Loss: 0.0026009
Training -->> Epoch: 2501,(no reg loss)standard loss: 0.0080918, inverse loss: 0.0016891
Valid-->> Epoch [2501/3000], Standardized Loss: 0.0124723, Inverse Loss: 0.0026034
Training -->> Epoch: 2502,(no reg loss)standard loss: 0.0080226, inverse loss: 0.0016746
Valid-->> Epoch [2502/3000], Standardized Loss: 0.0123758, Inverse Loss: 0.0025833
Training -->> Epoch: 2503,(no reg loss)standard loss: 0.0078375, inverse loss: 0.0016360
Valid-->> Epoch [2503/3000], Standardized Loss: 0.0124259, Inverse Loss: 0.0025938
Training -->> Epoch: 2504,(no reg loss)standard loss: 0.0080509, inverse loss: 0.0016805
Valid-->> Epoch [2504/3000], Standardized Loss: 0.0123663, Inverse Loss: 0.0025813
Training -->> Epoch: 2505,(no reg loss)standard loss: 0.0081035, inverse loss: 0.0016915
Valid-->> Epoch [2505/3000], Standardized Loss: 0.0123903, Inverse Loss: 0.0025863
Training -->> Epoch: 2506,(no reg loss)standard loss: 0.0081127, inverse loss: 0.0016934
Valid-->> Epoch [2506/3000], Standardized Loss: 0.0123916, Inverse Loss: 0.0025866
Training -->> Epoch: 2507,(no reg loss)standard loss: 0.0078951, inverse loss: 0.0016480
Valid-->> Epoch [2507/3000], Standardized Loss: 0.0124110, Inverse Loss: 0.0025907
Training -->> Epoch: 2508,(no reg loss)standard loss: 0.0080435, inverse loss: 0.0016790
Valid-->> Epoch [2508/3000], Standardized Loss: 0.0125482, Inverse Loss: 0.0026193
Training -->> Epoch: 2509,(no reg loss)standard loss: 0.0081393, inverse loss: 0.0016990
Valid-->> Epoch [2509/3000], Standardized Loss: 0.0123465, Inverse Loss: 0.0025772
Training -->> Epoch: 2510,(no reg loss)standard loss: 0.0081677, inverse loss: 0.0017049
Valid-->> Epoch [2510/3000], Standardized Loss: 0.0123662, Inverse Loss: 0.0025813
Training -->> Epoch: 2511,(no reg loss)standard loss: 0.0079831, inverse loss: 0.0016664
Valid-->> Epoch [2511/3000], Standardized Loss: 0.0123489, Inverse Loss: 0.0025777
Training -->> Epoch: 2512,(no reg loss)standard loss: 0.0080820, inverse loss: 0.0016870
Valid-->> Epoch [2512/3000], Standardized Loss: 0.0124348, Inverse Loss: 0.0025956
Training -->> Epoch: 2513,(no reg loss)standard loss: 0.0078988, inverse loss: 0.0016488
Valid-->> Epoch [2513/3000], Standardized Loss: 0.0123677, Inverse Loss: 0.0025816
Training -->> Epoch: 2514,(no reg loss)standard loss: 0.0083458, inverse loss: 0.0017421
Valid-->> Epoch [2514/3000], Standardized Loss: 0.0124253, Inverse Loss: 0.0025936
Training -->> Epoch: 2515,(no reg loss)standard loss: 0.0080769, inverse loss: 0.0016859
Valid-->> Epoch [2515/3000], Standardized Loss: 0.0123367, Inverse Loss: 0.0025751
Training -->> Epoch: 2516,(no reg loss)standard loss: 0.0080889, inverse loss: 0.0016885
Valid-->> Epoch [2516/3000], Standardized Loss: 0.0123695, Inverse Loss: 0.0025820
Training -->> Epoch: 2517,(no reg loss)standard loss: 0.0082350, inverse loss: 0.0017190
Valid-->> Epoch [2517/3000], Standardized Loss: 0.0123755, Inverse Loss: 0.0025833
Training -->> Epoch: 2518,(no reg loss)standard loss: 0.0081370, inverse loss: 0.0016985
Valid-->> Epoch [2518/3000], Standardized Loss: 0.0124088, Inverse Loss: 0.0025902
Training -->> Epoch: 2519,(no reg loss)standard loss: 0.0081251, inverse loss: 0.0016960
Valid-->> Epoch [2519/3000], Standardized Loss: 0.0123623, Inverse Loss: 0.0025805
Training -->> Epoch: 2520,(no reg loss)standard loss: 0.0080275, inverse loss: 0.0016756
Valid-->> Epoch [2520/3000], Standardized Loss: 0.0125313, Inverse Loss: 0.0026158
Training -->> Epoch: 2521,(no reg loss)standard loss: 0.0083845, inverse loss: 0.0017502
Valid-->> Epoch [2521/3000], Standardized Loss: 0.0124470, Inverse Loss: 0.0025982
Training -->> Epoch: 2522,(no reg loss)standard loss: 0.0081197, inverse loss: 0.0016949
Valid-->> Epoch [2522/3000], Standardized Loss: 0.0123781, Inverse Loss: 0.0025838
Training -->> Epoch: 2523,(no reg loss)standard loss: 0.0080847, inverse loss: 0.0016876
Valid-->> Epoch [2523/3000], Standardized Loss: 0.0124600, Inverse Loss: 0.0026009
Training -->> Epoch: 2524,(no reg loss)standard loss: 0.0082014, inverse loss: 0.0017119
Valid-->> Epoch [2524/3000], Standardized Loss: 0.0123869, Inverse Loss: 0.0025856
Training -->> Epoch: 2525,(no reg loss)standard loss: 0.0081956, inverse loss: 0.0017107
Valid-->> Epoch [2525/3000], Standardized Loss: 0.0123783, Inverse Loss: 0.0025838
Training -->> Epoch: 2526,(no reg loss)standard loss: 0.0082230, inverse loss: 0.0017165
Valid-->> Epoch [2526/3000], Standardized Loss: 0.0124582, Inverse Loss: 0.0026005
Training -->> Epoch: 2527,(no reg loss)standard loss: 0.0081582, inverse loss: 0.0017029
Valid-->> Epoch [2527/3000], Standardized Loss: 0.0125345, Inverse Loss: 0.0026164
Training -->> Epoch: 2528,(no reg loss)standard loss: 0.0083762, inverse loss: 0.0017484
Valid-->> Epoch [2528/3000], Standardized Loss: 0.0124332, Inverse Loss: 0.0025953
Training -->> Epoch: 2529,(no reg loss)standard loss: 0.0081711, inverse loss: 0.0017056
Valid-->> Epoch [2529/3000], Standardized Loss: 0.0123999, Inverse Loss: 0.0025883
Training -->> Epoch: 2530,(no reg loss)standard loss: 0.0082580, inverse loss: 0.0017238
Valid-->> Epoch [2530/3000], Standardized Loss: 0.0123096, Inverse Loss: 0.0025695
Valid-->> Lowest loss found at epoch 2530, loss: 0.0025695
Epoch 2530, Masked params (inverse standardized): tensor([3.230058e+01, 4.430688e+01, 7.345200e-02, 2.641134e+01, 2.773417e+01,
        1.781379e+01, 2.143948e+01, 1.637781e+00, 1.066175e+02, 2.760832e+01,
        2.620372e+01, 4.461992e+01, 2.445128e+01, 1.504300e+01, 8.461931e+01,
        2.837919e+01, 5.673512e+00, 2.983659e+01, 1.197365e+01, 3.204763e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 2531,(no reg loss)standard loss: 0.0080055, inverse loss: 0.0016710
Valid-->> Epoch [2531/3000], Standardized Loss: 0.0123915, Inverse Loss: 0.0025866
Training -->> Epoch: 2532,(no reg loss)standard loss: 0.0083784, inverse loss: 0.0017489
Valid-->> Epoch [2532/3000], Standardized Loss: 0.0124173, Inverse Loss: 0.0025920
Training -->> Epoch: 2533,(no reg loss)standard loss: 0.0084214, inverse loss: 0.0017579
Valid-->> Epoch [2533/3000], Standardized Loss: 0.0123678, Inverse Loss: 0.0025816
Training -->> Epoch: 2534,(no reg loss)standard loss: 0.0081745, inverse loss: 0.0017063
Valid-->> Epoch [2534/3000], Standardized Loss: 0.0124033, Inverse Loss: 0.0025890
Training -->> Epoch: 2535,(no reg loss)standard loss: 0.0082336, inverse loss: 0.0017187
Valid-->> Epoch [2535/3000], Standardized Loss: 0.0123432, Inverse Loss: 0.0025765
Training -->> Epoch: 2536,(no reg loss)standard loss: 0.0081976, inverse loss: 0.0017112
Valid-->> Epoch [2536/3000], Standardized Loss: 0.0123511, Inverse Loss: 0.0025781
Training -->> Epoch: 2537,(no reg loss)standard loss: 0.0082919, inverse loss: 0.0017308
Valid-->> Epoch [2537/3000], Standardized Loss: 0.0123607, Inverse Loss: 0.0025802
Training -->> Epoch: 2538,(no reg loss)standard loss: 0.0081223, inverse loss: 0.0016954
Valid-->> Epoch [2538/3000], Standardized Loss: 0.0123908, Inverse Loss: 0.0025864
Training -->> Epoch: 2539,(no reg loss)standard loss: 0.0083752, inverse loss: 0.0017482
Valid-->> Epoch [2539/3000], Standardized Loss: 0.0125060, Inverse Loss: 0.0026105
Training -->> Epoch: 2540,(no reg loss)standard loss: 0.0082847, inverse loss: 0.0017293
Valid-->> Epoch [2540/3000], Standardized Loss: 0.0123628, Inverse Loss: 0.0025806
Training -->> Epoch: 2541,(no reg loss)standard loss: 0.0083719, inverse loss: 0.0017475
Valid-->> Epoch [2541/3000], Standardized Loss: 0.0123575, Inverse Loss: 0.0025795
Training -->> Epoch: 2542,(no reg loss)standard loss: 0.0081328, inverse loss: 0.0016976
Valid-->> Epoch [2542/3000], Standardized Loss: 0.0123451, Inverse Loss: 0.0025769
Training -->> Epoch: 2543,(no reg loss)standard loss: 0.0083787, inverse loss: 0.0017490
Valid-->> Epoch [2543/3000], Standardized Loss: 0.0125291, Inverse Loss: 0.0026153
Training -->> Epoch: 2544,(no reg loss)standard loss: 0.0083099, inverse loss: 0.0017346
Valid-->> Epoch [2544/3000], Standardized Loss: 0.0123792, Inverse Loss: 0.0025840
Training -->> Epoch: 2545,(no reg loss)standard loss: 0.0083376, inverse loss: 0.0017404
Valid-->> Epoch [2545/3000], Standardized Loss: 0.0124810, Inverse Loss: 0.0026053
Training -->> Epoch: 2546,(no reg loss)standard loss: 0.0083527, inverse loss: 0.0017435
Valid-->> Epoch [2546/3000], Standardized Loss: 0.0124647, Inverse Loss: 0.0026019
Training -->> Epoch: 2547,(no reg loss)standard loss: 0.0083054, inverse loss: 0.0017337
Valid-->> Epoch [2547/3000], Standardized Loss: 0.0123326, Inverse Loss: 0.0025743
Training -->> Epoch: 2548,(no reg loss)standard loss: 0.0083149, inverse loss: 0.0017356
Valid-->> Epoch [2548/3000], Standardized Loss: 0.0124226, Inverse Loss: 0.0025931
Training -->> Epoch: 2549,(no reg loss)standard loss: 0.0082851, inverse loss: 0.0017294
Valid-->> Epoch [2549/3000], Standardized Loss: 0.0123596, Inverse Loss: 0.0025799
Training -->> Epoch: 2550,(no reg loss)standard loss: 0.0082550, inverse loss: 0.0017231
Valid-->> Epoch [2550/3000], Standardized Loss: 0.0124072, Inverse Loss: 0.0025899
Training -->> Epoch: 2551,(no reg loss)standard loss: 0.0085037, inverse loss: 0.0017750
Valid-->> Epoch [2551/3000], Standardized Loss: 0.0125256, Inverse Loss: 0.0026146
Training -->> Epoch: 2552,(no reg loss)standard loss: 0.0084618, inverse loss: 0.0017663
Valid-->> Epoch [2552/3000], Standardized Loss: 0.0123585, Inverse Loss: 0.0025797
Training -->> Epoch: 2553,(no reg loss)standard loss: 0.0080505, inverse loss: 0.0016804
Valid-->> Epoch [2553/3000], Standardized Loss: 0.0122975, Inverse Loss: 0.0025670
Valid-->> Lowest loss found at epoch 2553, loss: 0.0025670
Epoch 2553, Masked params (inverse standardized): tensor([3.229891e+01, 4.430352e+01, 6.578445e-02, 2.641319e+01, 2.773548e+01,
        1.780977e+01, 2.143789e+01, 1.633738e+00, 1.066146e+02, 2.760984e+01,
        2.620544e+01, 4.461305e+01, 2.445130e+01, 1.503815e+01, 8.461618e+01,
        2.837978e+01, 5.667410e+00, 2.983606e+01, 1.196727e+01, 3.204619e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 2554,(no reg loss)standard loss: 0.0087377, inverse loss: 0.0018239
Valid-->> Epoch [2554/3000], Standardized Loss: 0.0124190, Inverse Loss: 0.0025923
Training -->> Epoch: 2555,(no reg loss)standard loss: 0.0083481, inverse loss: 0.0017426
Valid-->> Epoch [2555/3000], Standardized Loss: 0.0123338, Inverse Loss: 0.0025746
Training -->> Epoch: 2556,(no reg loss)standard loss: 0.0084869, inverse loss: 0.0017716
Valid-->> Epoch [2556/3000], Standardized Loss: 0.0124148, Inverse Loss: 0.0025915
Training -->> Epoch: 2557,(no reg loss)standard loss: 0.0083318, inverse loss: 0.0017392
Valid-->> Epoch [2557/3000], Standardized Loss: 0.0124255, Inverse Loss: 0.0025937
Training -->> Epoch: 2558,(no reg loss)standard loss: 0.0084747, inverse loss: 0.0017690
Valid-->> Epoch [2558/3000], Standardized Loss: 0.0123879, Inverse Loss: 0.0025858
Training -->> Epoch: 2559,(no reg loss)standard loss: 0.0083315, inverse loss: 0.0017391
Valid-->> Epoch [2559/3000], Standardized Loss: 0.0123780, Inverse Loss: 0.0025838
Training -->> Epoch: 2560,(no reg loss)standard loss: 0.0084030, inverse loss: 0.0017540
Valid-->> Epoch [2560/3000], Standardized Loss: 0.0123619, Inverse Loss: 0.0025804
Training -->> Epoch: 2561,(no reg loss)standard loss: 0.0083697, inverse loss: 0.0017471
Valid-->> Epoch [2561/3000], Standardized Loss: 0.0124216, Inverse Loss: 0.0025929
Training -->> Epoch: 2562,(no reg loss)standard loss: 0.0087968, inverse loss: 0.0018362
Valid-->> Epoch [2562/3000], Standardized Loss: 0.0124088, Inverse Loss: 0.0025902
Training -->> Epoch: 2563,(no reg loss)standard loss: 0.0086670, inverse loss: 0.0018091
Valid-->> Epoch [2563/3000], Standardized Loss: 0.0123814, Inverse Loss: 0.0025845
Training -->> Epoch: 2564,(no reg loss)standard loss: 0.0082031, inverse loss: 0.0017123
Valid-->> Epoch [2564/3000], Standardized Loss: 0.0123480, Inverse Loss: 0.0025775
Training -->> Epoch: 2565,(no reg loss)standard loss: 0.0084529, inverse loss: 0.0017644
Valid-->> Epoch [2565/3000], Standardized Loss: 0.0124744, Inverse Loss: 0.0026039
Training -->> Epoch: 2566,(no reg loss)standard loss: 0.0085811, inverse loss: 0.0017912
Valid-->> Epoch [2566/3000], Standardized Loss: 0.0124077, Inverse Loss: 0.0025900
Training -->> Epoch: 2567,(no reg loss)standard loss: 0.0084162, inverse loss: 0.0017568
Valid-->> Epoch [2567/3000], Standardized Loss: 0.0123586, Inverse Loss: 0.0025797
Training -->> Epoch: 2568,(no reg loss)standard loss: 0.0085582, inverse loss: 0.0017864
Valid-->> Epoch [2568/3000], Standardized Loss: 0.0123840, Inverse Loss: 0.0025850
Training -->> Epoch: 2569,(no reg loss)standard loss: 0.0083638, inverse loss: 0.0017458
Valid-->> Epoch [2569/3000], Standardized Loss: 0.0123768, Inverse Loss: 0.0025835
Training -->> Epoch: 2570,(no reg loss)standard loss: 0.0086790, inverse loss: 0.0018116
Valid-->> Epoch [2570/3000], Standardized Loss: 0.0124651, Inverse Loss: 0.0026020
Training -->> Epoch: 2571,(no reg loss)standard loss: 0.0085486, inverse loss: 0.0017844
Valid-->> Epoch [2571/3000], Standardized Loss: 0.0122994, Inverse Loss: 0.0025674
Training -->> Epoch: 2572,(no reg loss)standard loss: 0.0083874, inverse loss: 0.0017508
Valid-->> Epoch [2572/3000], Standardized Loss: 0.0123990, Inverse Loss: 0.0025881
Training -->> Epoch: 2573,(no reg loss)standard loss: 0.0086107, inverse loss: 0.0017974
Valid-->> Epoch [2573/3000], Standardized Loss: 0.0124090, Inverse Loss: 0.0025902
Training -->> Epoch: 2574,(no reg loss)standard loss: 0.0085811, inverse loss: 0.0017912
Valid-->> Epoch [2574/3000], Standardized Loss: 0.0123604, Inverse Loss: 0.0025801
Training -->> Epoch: 2575,(no reg loss)standard loss: 0.0084610, inverse loss: 0.0017661
Valid-->> Epoch [2575/3000], Standardized Loss: 0.0123945, Inverse Loss: 0.0025872
Training -->> Epoch: 2576,(no reg loss)standard loss: 0.0086004, inverse loss: 0.0017952
Valid-->> Epoch [2576/3000], Standardized Loss: 0.0125505, Inverse Loss: 0.0026198
Training -->> Epoch: 2577,(no reg loss)standard loss: 0.0089026, inverse loss: 0.0018583
Valid-->> Epoch [2577/3000], Standardized Loss: 0.0123110, Inverse Loss: 0.0025698
Training -->> Epoch: 2578,(no reg loss)standard loss: 0.0082928, inverse loss: 0.0017310
Valid-->> Epoch [2578/3000], Standardized Loss: 0.0123405, Inverse Loss: 0.0025759
Training -->> Epoch: 2579,(no reg loss)standard loss: 0.0086792, inverse loss: 0.0018117
Valid-->> Epoch [2579/3000], Standardized Loss: 0.0125038, Inverse Loss: 0.0026100
Training -->> Epoch: 2580,(no reg loss)standard loss: 0.0087651, inverse loss: 0.0018296
Valid-->> Epoch [2580/3000], Standardized Loss: 0.0123968, Inverse Loss: 0.0025877
Training -->> Epoch: 2581,(no reg loss)standard loss: 0.0084601, inverse loss: 0.0017659
Valid-->> Epoch [2581/3000], Standardized Loss: 0.0123936, Inverse Loss: 0.0025870
Training -->> Epoch: 2582,(no reg loss)standard loss: 0.0085110, inverse loss: 0.0017766
Valid-->> Epoch [2582/3000], Standardized Loss: 0.0123716, Inverse Loss: 0.0025824
Training -->> Epoch: 2583,(no reg loss)standard loss: 0.0087808, inverse loss: 0.0018329
Valid-->> Epoch [2583/3000], Standardized Loss: 0.0123838, Inverse Loss: 0.0025850
Training -->> Epoch: 2584,(no reg loss)standard loss: 0.0085893, inverse loss: 0.0017929
Valid-->> Epoch [2584/3000], Standardized Loss: 0.0124053, Inverse Loss: 0.0025895
Training -->> Epoch: 2585,(no reg loss)standard loss: 0.0085649, inverse loss: 0.0017878
Valid-->> Epoch [2585/3000], Standardized Loss: 0.0124152, Inverse Loss: 0.0025915
Training -->> Epoch: 2586,(no reg loss)standard loss: 0.0085723, inverse loss: 0.0017894
Valid-->> Epoch [2586/3000], Standardized Loss: 0.0124308, Inverse Loss: 0.0025948
Training -->> Epoch: 2587,(no reg loss)standard loss: 0.0087399, inverse loss: 0.0018244
Valid-->> Epoch [2587/3000], Standardized Loss: 0.0124297, Inverse Loss: 0.0025946
Training -->> Epoch: 2588,(no reg loss)standard loss: 0.0088461, inverse loss: 0.0018465
Valid-->> Epoch [2588/3000], Standardized Loss: 0.0125444, Inverse Loss: 0.0026185
Training -->> Epoch: 2589,(no reg loss)standard loss: 0.0086797, inverse loss: 0.0018118
Valid-->> Epoch [2589/3000], Standardized Loss: 0.0123811, Inverse Loss: 0.0025844
Training -->> Epoch: 2590,(no reg loss)standard loss: 0.0087486, inverse loss: 0.0018262
Valid-->> Epoch [2590/3000], Standardized Loss: 0.0124511, Inverse Loss: 0.0025990
Training -->> Epoch: 2591,(no reg loss)standard loss: 0.0087691, inverse loss: 0.0018305
Valid-->> Epoch [2591/3000], Standardized Loss: 0.0124124, Inverse Loss: 0.0025909
Training -->> Epoch: 2592,(no reg loss)standard loss: 0.0086246, inverse loss: 0.0018003
Valid-->> Epoch [2592/3000], Standardized Loss: 0.0123275, Inverse Loss: 0.0025732
Training -->> Epoch: 2593,(no reg loss)standard loss: 0.0088281, inverse loss: 0.0018428
Valid-->> Epoch [2593/3000], Standardized Loss: 0.0123417, Inverse Loss: 0.0025762
Training -->> Epoch: 2594,(no reg loss)standard loss: 0.0085555, inverse loss: 0.0017859
Valid-->> Epoch [2594/3000], Standardized Loss: 0.0123441, Inverse Loss: 0.0025767
Training -->> Epoch: 2595,(no reg loss)standard loss: 0.0086448, inverse loss: 0.0018045
Valid-->> Epoch [2595/3000], Standardized Loss: 0.0123544, Inverse Loss: 0.0025788
Training -->> Epoch: 2596,(no reg loss)standard loss: 0.0087742, inverse loss: 0.0018315
Valid-->> Epoch [2596/3000], Standardized Loss: 0.0124398, Inverse Loss: 0.0025967
Training -->> Epoch: 2597,(no reg loss)standard loss: 0.0087762, inverse loss: 0.0018319
Valid-->> Epoch [2597/3000], Standardized Loss: 0.0122968, Inverse Loss: 0.0025668
Valid-->> Lowest loss found at epoch 2597, loss: 0.0025668
Epoch 2597, Masked params (inverse standardized): tensor([3.230157e+01, 4.430558e+01, 7.237816e-02, 2.641362e+01, 2.773581e+01,
        1.781568e+01, 2.144075e+01, 1.638573e+00, 1.066160e+02, 2.761039e+01,
        2.620571e+01, 4.461953e+01, 2.445179e+01, 1.504381e+01, 8.461932e+01,
        2.837988e+01, 5.672314e+00, 2.983699e+01, 1.197384e+01, 3.204890e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 2598,(no reg loss)standard loss: 0.0085435, inverse loss: 0.0017833
Valid-->> Epoch [2598/3000], Standardized Loss: 0.0123221, Inverse Loss: 0.0025721
Training -->> Epoch: 2599,(no reg loss)standard loss: 0.0088953, inverse loss: 0.0018568
Valid-->> Epoch [2599/3000], Standardized Loss: 0.0123158, Inverse Loss: 0.0025708
Training -->> Epoch: 2600,(no reg loss)standard loss: 0.0089746, inverse loss: 0.0018733
Valid-->> Epoch [2600/3000], Standardized Loss: 0.0124738, Inverse Loss: 0.0026038
Training -->> Epoch: 2601,(no reg loss)standard loss: 0.0086595, inverse loss: 0.0018076
Valid-->> Epoch [2601/3000], Standardized Loss: 0.0122746, Inverse Loss: 0.0025622
Valid-->> Lowest loss found at epoch 2601, loss: 0.0025622
Epoch 2601, Masked params (inverse standardized): tensor([3.230188e+01, 4.430455e+01, 6.989861e-02, 2.641443e+01, 2.773652e+01,
        1.781369e+01, 2.144074e+01, 1.636627e+00, 1.066152e+02, 2.761107e+01,
        2.620650e+01, 4.461761e+01, 2.445263e+01, 1.504246e+01, 8.461920e+01,
        2.838059e+01, 5.671009e+00, 2.983803e+01, 1.197195e+01, 3.204900e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 2602,(no reg loss)standard loss: 0.0087300, inverse loss: 0.0018223
Valid-->> Epoch [2602/3000], Standardized Loss: 0.0125445, Inverse Loss: 0.0026185
Training -->> Epoch: 2603,(no reg loss)standard loss: 0.0089240, inverse loss: 0.0018628
Valid-->> Epoch [2603/3000], Standardized Loss: 0.0124586, Inverse Loss: 0.0026006
Training -->> Epoch: 2604,(no reg loss)standard loss: 0.0086692, inverse loss: 0.0018096
Valid-->> Epoch [2604/3000], Standardized Loss: 0.0122869, Inverse Loss: 0.0025647
Training -->> Epoch: 2605,(no reg loss)standard loss: 0.0086445, inverse loss: 0.0018044
Valid-->> Epoch [2605/3000], Standardized Loss: 0.0124212, Inverse Loss: 0.0025928
Training -->> Epoch: 2606,(no reg loss)standard loss: 0.0087871, inverse loss: 0.0018342
Valid-->> Epoch [2606/3000], Standardized Loss: 0.0124662, Inverse Loss: 0.0026022
Training -->> Epoch: 2607,(no reg loss)standard loss: 0.0089580, inverse loss: 0.0018699
Valid-->> Epoch [2607/3000], Standardized Loss: 0.0123147, Inverse Loss: 0.0025705
Training -->> Epoch: 2608,(no reg loss)standard loss: 0.0089644, inverse loss: 0.0018712
Valid-->> Epoch [2608/3000], Standardized Loss: 0.0123962, Inverse Loss: 0.0025876
Training -->> Epoch: 2609,(no reg loss)standard loss: 0.0087610, inverse loss: 0.0018288
Valid-->> Epoch [2609/3000], Standardized Loss: 0.0123637, Inverse Loss: 0.0025808
Training -->> Epoch: 2610,(no reg loss)standard loss: 0.0086576, inverse loss: 0.0018072
Valid-->> Epoch [2610/3000], Standardized Loss: 0.0124535, Inverse Loss: 0.0025995
Training -->> Epoch: 2611,(no reg loss)standard loss: 0.0090242, inverse loss: 0.0018837
Valid-->> Epoch [2611/3000], Standardized Loss: 0.0124342, Inverse Loss: 0.0025955
Training -->> Epoch: 2612,(no reg loss)standard loss: 0.0087888, inverse loss: 0.0018346
Valid-->> Epoch [2612/3000], Standardized Loss: 0.0123973, Inverse Loss: 0.0025878
Training -->> Epoch: 2613,(no reg loss)standard loss: 0.0090284, inverse loss: 0.0018846
Valid-->> Epoch [2613/3000], Standardized Loss: 0.0123703, Inverse Loss: 0.0025822
Training -->> Epoch: 2614,(no reg loss)standard loss: 0.0087022, inverse loss: 0.0018165
Valid-->> Epoch [2614/3000], Standardized Loss: 0.0124181, Inverse Loss: 0.0025921
Training -->> Epoch: 2615,(no reg loss)standard loss: 0.0090573, inverse loss: 0.0018906
Valid-->> Epoch [2615/3000], Standardized Loss: 0.0125143, Inverse Loss: 0.0026122
Training -->> Epoch: 2616,(no reg loss)standard loss: 0.0088713, inverse loss: 0.0018518
Valid-->> Epoch [2616/3000], Standardized Loss: 0.0123091, Inverse Loss: 0.0025694
Training -->> Epoch: 2617,(no reg loss)standard loss: 0.0090452, inverse loss: 0.0018881
Valid-->> Epoch [2617/3000], Standardized Loss: 0.0123960, Inverse Loss: 0.0025875
Training -->> Epoch: 2618,(no reg loss)standard loss: 0.0086307, inverse loss: 0.0018016
Valid-->> Epoch [2618/3000], Standardized Loss: 0.0123291, Inverse Loss: 0.0025736
Training -->> Epoch: 2619,(no reg loss)standard loss: 0.0092332, inverse loss: 0.0019273
Valid-->> Epoch [2619/3000], Standardized Loss: 0.0124616, Inverse Loss: 0.0026012
Training -->> Epoch: 2620,(no reg loss)standard loss: 0.0088116, inverse loss: 0.0018393
Valid-->> Epoch [2620/3000], Standardized Loss: 0.0123290, Inverse Loss: 0.0025735
Training -->> Epoch: 2621,(no reg loss)standard loss: 0.0088992, inverse loss: 0.0018576
Valid-->> Epoch [2621/3000], Standardized Loss: 0.0124295, Inverse Loss: 0.0025945
Training -->> Epoch: 2622,(no reg loss)standard loss: 0.0089493, inverse loss: 0.0018681
Valid-->> Epoch [2622/3000], Standardized Loss: 0.0123195, Inverse Loss: 0.0025716
Training -->> Epoch: 2623,(no reg loss)standard loss: 0.0092858, inverse loss: 0.0019383
Valid-->> Epoch [2623/3000], Standardized Loss: 0.0124607, Inverse Loss: 0.0026010
Training -->> Epoch: 2624,(no reg loss)standard loss: 0.0089630, inverse loss: 0.0018709
Valid-->> Epoch [2624/3000], Standardized Loss: 0.0123228, Inverse Loss: 0.0025722
Training -->> Epoch: 2625,(no reg loss)standard loss: 0.0087193, inverse loss: 0.0018201
Valid-->> Epoch [2625/3000], Standardized Loss: 0.0122961, Inverse Loss: 0.0025667
Training -->> Epoch: 2626,(no reg loss)standard loss: 0.0088380, inverse loss: 0.0018448
Valid-->> Epoch [2626/3000], Standardized Loss: 0.0124369, Inverse Loss: 0.0025961
Training -->> Epoch: 2627,(no reg loss)standard loss: 0.0091267, inverse loss: 0.0019051
Valid-->> Epoch [2627/3000], Standardized Loss: 0.0123260, Inverse Loss: 0.0025729
Training -->> Epoch: 2628,(no reg loss)standard loss: 0.0088482, inverse loss: 0.0018470
Valid-->> Epoch [2628/3000], Standardized Loss: 0.0123194, Inverse Loss: 0.0025715
Training -->> Epoch: 2629,(no reg loss)standard loss: 0.0089306, inverse loss: 0.0018642
Valid-->> Epoch [2629/3000], Standardized Loss: 0.0124983, Inverse Loss: 0.0026089
Training -->> Epoch: 2630,(no reg loss)standard loss: 0.0090964, inverse loss: 0.0018988
Valid-->> Epoch [2630/3000], Standardized Loss: 0.0123772, Inverse Loss: 0.0025836
Training -->> Epoch: 2631,(no reg loss)standard loss: 0.0089407, inverse loss: 0.0018663
Valid-->> Epoch [2631/3000], Standardized Loss: 0.0122961, Inverse Loss: 0.0025667
Training -->> Epoch: 2632,(no reg loss)standard loss: 0.0091137, inverse loss: 0.0019024
Valid-->> Epoch [2632/3000], Standardized Loss: 0.0123637, Inverse Loss: 0.0025808
Training -->> Epoch: 2633,(no reg loss)standard loss: 0.0089697, inverse loss: 0.0018723
Valid-->> Epoch [2633/3000], Standardized Loss: 0.0123913, Inverse Loss: 0.0025865
Training -->> Epoch: 2634,(no reg loss)standard loss: 0.0091749, inverse loss: 0.0019152
Valid-->> Epoch [2634/3000], Standardized Loss: 0.0123936, Inverse Loss: 0.0025870
Training -->> Epoch: 2635,(no reg loss)standard loss: 0.0089268, inverse loss: 0.0018634
Valid-->> Epoch [2635/3000], Standardized Loss: 0.0124019, Inverse Loss: 0.0025888
Training -->> Epoch: 2636,(no reg loss)standard loss: 0.0091722, inverse loss: 0.0019146
Valid-->> Epoch [2636/3000], Standardized Loss: 0.0124156, Inverse Loss: 0.0025916
Training -->> Epoch: 2637,(no reg loss)standard loss: 0.0090151, inverse loss: 0.0018818
Valid-->> Epoch [2637/3000], Standardized Loss: 0.0124425, Inverse Loss: 0.0025972
Training -->> Epoch: 2638,(no reg loss)standard loss: 0.0091339, inverse loss: 0.0019066
Valid-->> Epoch [2638/3000], Standardized Loss: 0.0123176, Inverse Loss: 0.0025712
Training -->> Epoch: 2639,(no reg loss)standard loss: 0.0086294, inverse loss: 0.0018013
Valid-->> Epoch [2639/3000], Standardized Loss: 0.0122150, Inverse Loss: 0.0025497
Valid-->> Lowest loss found at epoch 2639, loss: 0.0025497
Epoch 2639, Masked params (inverse standardized): tensor([3.229998e+01, 4.430063e+01, 6.285858e-02, 2.641847e+01, 2.773979e+01,
        1.780782e+01, 2.143834e+01, 1.631657e+00, 1.066120e+02, 2.761467e+01,
        2.621047e+01, 4.461101e+01, 2.445365e+01, 1.503772e+01, 8.461549e+01,
        2.838257e+01, 5.665674e+00, 2.983919e+01, 1.196571e+01, 3.204683e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 2640,(no reg loss)standard loss: 0.0092383, inverse loss: 0.0019284
Valid-->> Epoch [2640/3000], Standardized Loss: 0.0125117, Inverse Loss: 0.0026117
Training -->> Epoch: 2641,(no reg loss)standard loss: 0.0090112, inverse loss: 0.0018810
Valid-->> Epoch [2641/3000], Standardized Loss: 0.0123510, Inverse Loss: 0.0025781
Training -->> Epoch: 2642,(no reg loss)standard loss: 0.0090843, inverse loss: 0.0018962
Valid-->> Epoch [2642/3000], Standardized Loss: 0.0124681, Inverse Loss: 0.0026026
Training -->> Epoch: 2643,(no reg loss)standard loss: 0.0091552, inverse loss: 0.0019110
Valid-->> Epoch [2643/3000], Standardized Loss: 0.0124249, Inverse Loss: 0.0025935
Training -->> Epoch: 2644,(no reg loss)standard loss: 0.0091244, inverse loss: 0.0019046
Valid-->> Epoch [2644/3000], Standardized Loss: 0.0123754, Inverse Loss: 0.0025832
Training -->> Epoch: 2645,(no reg loss)standard loss: 0.0091611, inverse loss: 0.0019123
Valid-->> Epoch [2645/3000], Standardized Loss: 0.0123489, Inverse Loss: 0.0025777
Training -->> Epoch: 2646,(no reg loss)standard loss: 0.0090750, inverse loss: 0.0018943
Valid-->> Epoch [2646/3000], Standardized Loss: 0.0123977, Inverse Loss: 0.0025879
Training -->> Epoch: 2647,(no reg loss)standard loss: 0.0095225, inverse loss: 0.0019877
Valid-->> Epoch [2647/3000], Standardized Loss: 0.0123896, Inverse Loss: 0.0025862
Training -->> Epoch: 2648,(no reg loss)standard loss: 0.0091347, inverse loss: 0.0019068
Valid-->> Epoch [2648/3000], Standardized Loss: 0.0123159, Inverse Loss: 0.0025708
Training -->> Epoch: 2649,(no reg loss)standard loss: 0.0090103, inverse loss: 0.0018808
Valid-->> Epoch [2649/3000], Standardized Loss: 0.0123876, Inverse Loss: 0.0025858
Training -->> Epoch: 2650,(no reg loss)standard loss: 0.0090457, inverse loss: 0.0018882
Valid-->> Epoch [2650/3000], Standardized Loss: 0.0123517, Inverse Loss: 0.0025783
Training -->> Epoch: 2651,(no reg loss)standard loss: 0.0091874, inverse loss: 0.0019178
Valid-->> Epoch [2651/3000], Standardized Loss: 0.0124416, Inverse Loss: 0.0025971
Training -->> Epoch: 2652,(no reg loss)standard loss: 0.0094020, inverse loss: 0.0019626
Valid-->> Epoch [2652/3000], Standardized Loss: 0.0123235, Inverse Loss: 0.0025724
Training -->> Epoch: 2653,(no reg loss)standard loss: 0.0089868, inverse loss: 0.0018759
Valid-->> Epoch [2653/3000], Standardized Loss: 0.0123321, Inverse Loss: 0.0025742
Training -->> Epoch: 2654,(no reg loss)standard loss: 0.0091924, inverse loss: 0.0019188
Valid-->> Epoch [2654/3000], Standardized Loss: 0.0124489, Inverse Loss: 0.0025986
Training -->> Epoch: 2655,(no reg loss)standard loss: 0.0093113, inverse loss: 0.0019436
Valid-->> Epoch [2655/3000], Standardized Loss: 0.0123285, Inverse Loss: 0.0025734
Training -->> Epoch: 2656,(no reg loss)standard loss: 0.0091713, inverse loss: 0.0019144
Valid-->> Epoch [2656/3000], Standardized Loss: 0.0123288, Inverse Loss: 0.0025735
Training -->> Epoch: 2657,(no reg loss)standard loss: 0.0091070, inverse loss: 0.0019010
Valid-->> Epoch [2657/3000], Standardized Loss: 0.0124611, Inverse Loss: 0.0026011
Training -->> Epoch: 2658,(no reg loss)standard loss: 0.0093695, inverse loss: 0.0019558
Valid-->> Epoch [2658/3000], Standardized Loss: 0.0124970, Inverse Loss: 0.0026086
Training -->> Epoch: 2659,(no reg loss)standard loss: 0.0092727, inverse loss: 0.0019356
Valid-->> Epoch [2659/3000], Standardized Loss: 0.0123720, Inverse Loss: 0.0025825
Training -->> Epoch: 2660,(no reg loss)standard loss: 0.0092350, inverse loss: 0.0019277
Valid-->> Epoch [2660/3000], Standardized Loss: 0.0123554, Inverse Loss: 0.0025790
Training -->> Epoch: 2661,(no reg loss)standard loss: 0.0094747, inverse loss: 0.0019777
Valid-->> Epoch [2661/3000], Standardized Loss: 0.0123835, Inverse Loss: 0.0025849
Training -->> Epoch: 2662,(no reg loss)standard loss: 0.0093304, inverse loss: 0.0019476
Valid-->> Epoch [2662/3000], Standardized Loss: 0.0123338, Inverse Loss: 0.0025745
Training -->> Epoch: 2663,(no reg loss)standard loss: 0.0091817, inverse loss: 0.0019166
Valid-->> Epoch [2663/3000], Standardized Loss: 0.0123403, Inverse Loss: 0.0025759
Training -->> Epoch: 2664,(no reg loss)standard loss: 0.0092210, inverse loss: 0.0019248
Valid-->> Epoch [2664/3000], Standardized Loss: 0.0124641, Inverse Loss: 0.0026017
Training -->> Epoch: 2665,(no reg loss)standard loss: 0.0094675, inverse loss: 0.0019762
Valid-->> Epoch [2665/3000], Standardized Loss: 0.0123373, Inverse Loss: 0.0025753
Training -->> Epoch: 2666,(no reg loss)standard loss: 0.0092372, inverse loss: 0.0019282
Valid-->> Epoch [2666/3000], Standardized Loss: 0.0123298, Inverse Loss: 0.0025737
Training -->> Epoch: 2667,(no reg loss)standard loss: 0.0092703, inverse loss: 0.0019351
Valid-->> Epoch [2667/3000], Standardized Loss: 0.0123671, Inverse Loss: 0.0025815
Training -->> Epoch: 2668,(no reg loss)standard loss: 0.0093333, inverse loss: 0.0019482
Valid-->> Epoch [2668/3000], Standardized Loss: 0.0123697, Inverse Loss: 0.0025820
Training -->> Epoch: 2669,(no reg loss)standard loss: 0.0093712, inverse loss: 0.0019561
Valid-->> Epoch [2669/3000], Standardized Loss: 0.0123969, Inverse Loss: 0.0025877
Training -->> Epoch: 2670,(no reg loss)standard loss: 0.0093597, inverse loss: 0.0019537
Valid-->> Epoch [2670/3000], Standardized Loss: 0.0123723, Inverse Loss: 0.0025826
Training -->> Epoch: 2671,(no reg loss)standard loss: 0.0094391, inverse loss: 0.0019703
Valid-->> Epoch [2671/3000], Standardized Loss: 0.0123635, Inverse Loss: 0.0025807
Training -->> Epoch: 2672,(no reg loss)standard loss: 0.0093150, inverse loss: 0.0019444
Valid-->> Epoch [2672/3000], Standardized Loss: 0.0123729, Inverse Loss: 0.0025827
Training -->> Epoch: 2673,(no reg loss)standard loss: 0.0093634, inverse loss: 0.0019545
Valid-->> Epoch [2673/3000], Standardized Loss: 0.0123410, Inverse Loss: 0.0025760
Training -->> Epoch: 2674,(no reg loss)standard loss: 0.0090532, inverse loss: 0.0018898
Valid-->> Epoch [2674/3000], Standardized Loss: 0.0122603, Inverse Loss: 0.0025592
Training -->> Epoch: 2675,(no reg loss)standard loss: 0.0095878, inverse loss: 0.0020014
Valid-->> Epoch [2675/3000], Standardized Loss: 0.0123842, Inverse Loss: 0.0025851
Training -->> Epoch: 2676,(no reg loss)standard loss: 0.0094805, inverse loss: 0.0019790
Valid-->> Epoch [2676/3000], Standardized Loss: 0.0123155, Inverse Loss: 0.0025707
Training -->> Epoch: 2677,(no reg loss)standard loss: 0.0090807, inverse loss: 0.0018955
Valid-->> Epoch [2677/3000], Standardized Loss: 0.0123114, Inverse Loss: 0.0025699
Training -->> Epoch: 2678,(no reg loss)standard loss: 0.0097278, inverse loss: 0.0020306
Valid-->> Epoch [2678/3000], Standardized Loss: 0.0124496, Inverse Loss: 0.0025987
Training -->> Epoch: 2679,(no reg loss)standard loss: 0.0094202, inverse loss: 0.0019664
Valid-->> Epoch [2679/3000], Standardized Loss: 0.0122939, Inverse Loss: 0.0025662
Training -->> Epoch: 2680,(no reg loss)standard loss: 0.0093318, inverse loss: 0.0019479
Valid-->> Epoch [2680/3000], Standardized Loss: 0.0124348, Inverse Loss: 0.0025956
Training -->> Epoch: 2681,(no reg loss)standard loss: 0.0094592, inverse loss: 0.0019745
Valid-->> Epoch [2681/3000], Standardized Loss: 0.0123339, Inverse Loss: 0.0025746
Training -->> Epoch: 2682,(no reg loss)standard loss: 0.0094896, inverse loss: 0.0019808
Valid-->> Epoch [2682/3000], Standardized Loss: 0.0123744, Inverse Loss: 0.0025830
Training -->> Epoch: 2683,(no reg loss)standard loss: 0.0092723, inverse loss: 0.0019355
Valid-->> Epoch [2683/3000], Standardized Loss: 0.0123474, Inverse Loss: 0.0025774
Training -->> Epoch: 2684,(no reg loss)standard loss: 0.0095008, inverse loss: 0.0019832
Valid-->> Epoch [2684/3000], Standardized Loss: 0.0123110, Inverse Loss: 0.0025698
Training -->> Epoch: 2685,(no reg loss)standard loss: 0.0091130, inverse loss: 0.0019022
Valid-->> Epoch [2685/3000], Standardized Loss: 0.0122617, Inverse Loss: 0.0025595
Training -->> Epoch: 2686,(no reg loss)standard loss: 0.0095333, inverse loss: 0.0019900
Valid-->> Epoch [2686/3000], Standardized Loss: 0.0123663, Inverse Loss: 0.0025813
Training -->> Epoch: 2687,(no reg loss)standard loss: 0.0095510, inverse loss: 0.0019937
Valid-->> Epoch [2687/3000], Standardized Loss: 0.0123736, Inverse Loss: 0.0025828
Training -->> Epoch: 2688,(no reg loss)standard loss: 0.0094308, inverse loss: 0.0019686
Valid-->> Epoch [2688/3000], Standardized Loss: 0.0123491, Inverse Loss: 0.0025777
Training -->> Epoch: 2689,(no reg loss)standard loss: 0.0095572, inverse loss: 0.0019950
Valid-->> Epoch [2689/3000], Standardized Loss: 0.0123514, Inverse Loss: 0.0025782
Training -->> Epoch: 2690,(no reg loss)standard loss: 0.0093655, inverse loss: 0.0019549
Valid-->> Epoch [2690/3000], Standardized Loss: 0.0123414, Inverse Loss: 0.0025761
Training -->> Epoch: 2691,(no reg loss)standard loss: 0.0097959, inverse loss: 0.0020448
Valid-->> Epoch [2691/3000], Standardized Loss: 0.0123688, Inverse Loss: 0.0025819
Training -->> Epoch: 2692,(no reg loss)standard loss: 0.0092092, inverse loss: 0.0019223
Valid-->> Epoch [2692/3000], Standardized Loss: 0.0122765, Inverse Loss: 0.0025626
Training -->> Epoch: 2693,(no reg loss)standard loss: 0.0096124, inverse loss: 0.0020065
Valid-->> Epoch [2693/3000], Standardized Loss: 0.0123806, Inverse Loss: 0.0025843
Training -->> Epoch: 2694,(no reg loss)standard loss: 0.0096168, inverse loss: 0.0020074
Valid-->> Epoch [2694/3000], Standardized Loss: 0.0123180, Inverse Loss: 0.0025712
Training -->> Epoch: 2695,(no reg loss)standard loss: 0.0094059, inverse loss: 0.0019634
Valid-->> Epoch [2695/3000], Standardized Loss: 0.0123758, Inverse Loss: 0.0025833
Training -->> Epoch: 2696,(no reg loss)standard loss: 0.0098099, inverse loss: 0.0020477
Valid-->> Epoch [2696/3000], Standardized Loss: 0.0123543, Inverse Loss: 0.0025788
Training -->> Epoch: 2697,(no reg loss)standard loss: 0.0094127, inverse loss: 0.0019648
Valid-->> Epoch [2697/3000], Standardized Loss: 0.0124465, Inverse Loss: 0.0025981
Training -->> Epoch: 2698,(no reg loss)standard loss: 0.0096617, inverse loss: 0.0020168
Valid-->> Epoch [2698/3000], Standardized Loss: 0.0124327, Inverse Loss: 0.0025952
Training -->> Epoch: 2699,(no reg loss)standard loss: 0.0096109, inverse loss: 0.0020062
Valid-->> Epoch [2699/3000], Standardized Loss: 0.0123864, Inverse Loss: 0.0025855
Training -->> Epoch: 2700,(no reg loss)standard loss: 0.0096579, inverse loss: 0.0020160
Valid-->> Epoch [2700/3000], Standardized Loss: 0.0124222, Inverse Loss: 0.0025930
Training -->> Epoch: 2701,(no reg loss)standard loss: 0.0094236, inverse loss: 0.0019671
Valid-->> Epoch [2701/3000], Standardized Loss: 0.0123220, Inverse Loss: 0.0025721
Training -->> Epoch: 2702,(no reg loss)standard loss: 0.0097653, inverse loss: 0.0020384
Valid-->> Epoch [2702/3000], Standardized Loss: 0.0123825, Inverse Loss: 0.0025847
Training -->> Epoch: 2703,(no reg loss)standard loss: 0.0094017, inverse loss: 0.0019625
Valid-->> Epoch [2703/3000], Standardized Loss: 0.0123042, Inverse Loss: 0.0025684
Training -->> Epoch: 2704,(no reg loss)standard loss: 0.0097697, inverse loss: 0.0020393
Valid-->> Epoch [2704/3000], Standardized Loss: 0.0124728, Inverse Loss: 0.0026035
Training -->> Epoch: 2705,(no reg loss)standard loss: 0.0097116, inverse loss: 0.0020272
Valid-->> Epoch [2705/3000], Standardized Loss: 0.0123811, Inverse Loss: 0.0025844
Training -->> Epoch: 2706,(no reg loss)standard loss: 0.0096229, inverse loss: 0.0020087
Valid-->> Epoch [2706/3000], Standardized Loss: 0.0123782, Inverse Loss: 0.0025838
Training -->> Epoch: 2707,(no reg loss)standard loss: 0.0097081, inverse loss: 0.0020265
Valid-->> Epoch [2707/3000], Standardized Loss: 0.0123502, Inverse Loss: 0.0025780
Training -->> Epoch: 2708,(no reg loss)standard loss: 0.0095736, inverse loss: 0.0019984
Valid-->> Epoch [2708/3000], Standardized Loss: 0.0123228, Inverse Loss: 0.0025722
Training -->> Epoch: 2709,(no reg loss)standard loss: 0.0096466, inverse loss: 0.0020136
Valid-->> Epoch [2709/3000], Standardized Loss: 0.0124196, Inverse Loss: 0.0025924
Training -->> Epoch: 2710,(no reg loss)standard loss: 0.0095667, inverse loss: 0.0019969
Valid-->> Epoch [2710/3000], Standardized Loss: 0.0123325, Inverse Loss: 0.0025743
Training -->> Epoch: 2711,(no reg loss)standard loss: 0.0097482, inverse loss: 0.0020348
Valid-->> Epoch [2711/3000], Standardized Loss: 0.0124512, Inverse Loss: 0.0025990
Training -->> Epoch: 2712,(no reg loss)standard loss: 0.0098407, inverse loss: 0.0020541
Valid-->> Epoch [2712/3000], Standardized Loss: 0.0123910, Inverse Loss: 0.0025865
Training -->> Epoch: 2713,(no reg loss)standard loss: 0.0096084, inverse loss: 0.0020056
Valid-->> Epoch [2713/3000], Standardized Loss: 0.0123735, Inverse Loss: 0.0025828
Training -->> Epoch: 2714,(no reg loss)standard loss: 0.0097436, inverse loss: 0.0020339
Valid-->> Epoch [2714/3000], Standardized Loss: 0.0123918, Inverse Loss: 0.0025866
Training -->> Epoch: 2715,(no reg loss)standard loss: 0.0096862, inverse loss: 0.0020219
Valid-->> Epoch [2715/3000], Standardized Loss: 0.0123975, Inverse Loss: 0.0025878
Training -->> Epoch: 2716,(no reg loss)standard loss: 0.0098578, inverse loss: 0.0020577
Valid-->> Epoch [2716/3000], Standardized Loss: 0.0124711, Inverse Loss: 0.0026032
Training -->> Epoch: 2717,(no reg loss)standard loss: 0.0097755, inverse loss: 0.0020405
Valid-->> Epoch [2717/3000], Standardized Loss: 0.0123984, Inverse Loss: 0.0025880
Training -->> Epoch: 2718,(no reg loss)standard loss: 0.0096515, inverse loss: 0.0020146
Valid-->> Epoch [2718/3000], Standardized Loss: 0.0123449, Inverse Loss: 0.0025769
Training -->> Epoch: 2719,(no reg loss)standard loss: 0.0098118, inverse loss: 0.0020481
Valid-->> Epoch [2719/3000], Standardized Loss: 0.0123776, Inverse Loss: 0.0025837
Training -->> Epoch: 2720,(no reg loss)standard loss: 0.0097295, inverse loss: 0.0020309
Valid-->> Epoch [2720/3000], Standardized Loss: 0.0123531, Inverse Loss: 0.0025786
Training -->> Epoch: 2721,(no reg loss)standard loss: 0.0098429, inverse loss: 0.0020546
Valid-->> Epoch [2721/3000], Standardized Loss: 0.0122969, Inverse Loss: 0.0025668
Training -->> Epoch: 2722,(no reg loss)standard loss: 0.0099306, inverse loss: 0.0020729
Valid-->> Epoch [2722/3000], Standardized Loss: 0.0123730, Inverse Loss: 0.0025827
Training -->> Epoch: 2723,(no reg loss)standard loss: 0.0097071, inverse loss: 0.0020262
Valid-->> Epoch [2723/3000], Standardized Loss: 0.0122907, Inverse Loss: 0.0025655
Training -->> Epoch: 2724,(no reg loss)standard loss: 0.0096637, inverse loss: 0.0020172
Valid-->> Epoch [2724/3000], Standardized Loss: 0.0123126, Inverse Loss: 0.0025701
Training -->> Epoch: 2725,(no reg loss)standard loss: 0.0098578, inverse loss: 0.0020577
Valid-->> Epoch [2725/3000], Standardized Loss: 0.0123841, Inverse Loss: 0.0025850
Training -->> Epoch: 2726,(no reg loss)standard loss: 0.0096673, inverse loss: 0.0020179
Valid-->> Epoch [2726/3000], Standardized Loss: 0.0123734, Inverse Loss: 0.0025828
Training -->> Epoch: 2727,(no reg loss)standard loss: 0.0099162, inverse loss: 0.0020699
Valid-->> Epoch [2727/3000], Standardized Loss: 0.0123620, Inverse Loss: 0.0025804
Training -->> Epoch: 2728,(no reg loss)standard loss: 0.0098177, inverse loss: 0.0020493
Valid-->> Epoch [2728/3000], Standardized Loss: 0.0122634, Inverse Loss: 0.0025598
Training -->> Epoch: 2729,(no reg loss)standard loss: 0.0099144, inverse loss: 0.0020695
Valid-->> Epoch [2729/3000], Standardized Loss: 0.0123047, Inverse Loss: 0.0025685
Training -->> Epoch: 2730,(no reg loss)standard loss: 0.0098517, inverse loss: 0.0020564
Valid-->> Epoch [2730/3000], Standardized Loss: 0.0123870, Inverse Loss: 0.0025856
Training -->> Epoch: 2731,(no reg loss)standard loss: 0.0098191, inverse loss: 0.0020496
Valid-->> Epoch [2731/3000], Standardized Loss: 0.0123559, Inverse Loss: 0.0025792
Training -->> Epoch: 2732,(no reg loss)standard loss: 0.0099550, inverse loss: 0.0020780
Valid-->> Epoch [2732/3000], Standardized Loss: 0.0123256, Inverse Loss: 0.0025728
Training -->> Epoch: 2733,(no reg loss)standard loss: 0.0096447, inverse loss: 0.0020132
Valid-->> Epoch [2733/3000], Standardized Loss: 0.0123423, Inverse Loss: 0.0025763
Training -->> Epoch: 2734,(no reg loss)standard loss: 0.0101238, inverse loss: 0.0021132
Valid-->> Epoch [2734/3000], Standardized Loss: 0.0123746, Inverse Loss: 0.0025831
Training -->> Epoch: 2735,(no reg loss)standard loss: 0.0096286, inverse loss: 0.0020099
Valid-->> Epoch [2735/3000], Standardized Loss: 0.0122138, Inverse Loss: 0.0025495
Valid-->> Lowest loss found at epoch 2735, loss: 0.0025495
Epoch 2735, Masked params (inverse standardized): tensor([3.230310e+01, 4.430422e+01, 6.715584e-02, 2.642081e+01, 2.774118e+01,
        1.780917e+01, 2.144090e+01, 1.631163e+00, 1.066150e+02, 2.761642e+01,
        2.621262e+01, 4.461469e+01, 2.445485e+01, 1.504170e+01, 8.461918e+01,
        2.838327e+01, 5.671677e+00, 2.984127e+01, 1.196908e+01, 3.204937e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 2736,(no reg loss)standard loss: 0.0096309, inverse loss: 0.0020103
Valid-->> Epoch [2736/3000], Standardized Loss: 0.0122453, Inverse Loss: 0.0025561
Training -->> Epoch: 2737,(no reg loss)standard loss: 0.0099172, inverse loss: 0.0020701
Valid-->> Epoch [2737/3000], Standardized Loss: 0.0123423, Inverse Loss: 0.0025763
Training -->> Epoch: 2738,(no reg loss)standard loss: 0.0099743, inverse loss: 0.0020820
Valid-->> Epoch [2738/3000], Standardized Loss: 0.0123999, Inverse Loss: 0.0025883
Training -->> Epoch: 2739,(no reg loss)standard loss: 0.0100090, inverse loss: 0.0020893
Valid-->> Epoch [2739/3000], Standardized Loss: 0.0123364, Inverse Loss: 0.0025751
Training -->> Epoch: 2740,(no reg loss)standard loss: 0.0097408, inverse loss: 0.0020333
Valid-->> Epoch [2740/3000], Standardized Loss: 0.0123175, Inverse Loss: 0.0025711
Training -->> Epoch: 2741,(no reg loss)standard loss: 0.0101538, inverse loss: 0.0021195
Valid-->> Epoch [2741/3000], Standardized Loss: 0.0124566, Inverse Loss: 0.0026002
Training -->> Epoch: 2742,(no reg loss)standard loss: 0.0097411, inverse loss: 0.0020333
Valid-->> Epoch [2742/3000], Standardized Loss: 0.0122485, Inverse Loss: 0.0025567
Training -->> Epoch: 2743,(no reg loss)standard loss: 0.0101552, inverse loss: 0.0021198
Valid-->> Epoch [2743/3000], Standardized Loss: 0.0124494, Inverse Loss: 0.0025987
Training -->> Epoch: 2744,(no reg loss)standard loss: 0.0099399, inverse loss: 0.0020748
Valid-->> Epoch [2744/3000], Standardized Loss: 0.0124053, Inverse Loss: 0.0025895
Training -->> Epoch: 2745,(no reg loss)standard loss: 0.0100391, inverse loss: 0.0020956
Valid-->> Epoch [2745/3000], Standardized Loss: 0.0122968, Inverse Loss: 0.0025668
Training -->> Epoch: 2746,(no reg loss)standard loss: 0.0098151, inverse loss: 0.0020488
Valid-->> Epoch [2746/3000], Standardized Loss: 0.0123680, Inverse Loss: 0.0025817
Training -->> Epoch: 2747,(no reg loss)standard loss: 0.0101349, inverse loss: 0.0021155
Valid-->> Epoch [2747/3000], Standardized Loss: 0.0124523, Inverse Loss: 0.0025993
Training -->> Epoch: 2748,(no reg loss)standard loss: 0.0101249, inverse loss: 0.0021135
Valid-->> Epoch [2748/3000], Standardized Loss: 0.0122452, Inverse Loss: 0.0025560
Training -->> Epoch: 2749,(no reg loss)standard loss: 0.0100493, inverse loss: 0.0020977
Valid-->> Epoch [2749/3000], Standardized Loss: 0.0123331, Inverse Loss: 0.0025744
Training -->> Epoch: 2750,(no reg loss)standard loss: 0.0100833, inverse loss: 0.0021048
Valid-->> Epoch [2750/3000], Standardized Loss: 0.0123339, Inverse Loss: 0.0025746
Training -->> Epoch: 2751,(no reg loss)standard loss: 0.0100419, inverse loss: 0.0020961
Valid-->> Epoch [2751/3000], Standardized Loss: 0.0123281, Inverse Loss: 0.0025734
Training -->> Epoch: 2752,(no reg loss)standard loss: 0.0101423, inverse loss: 0.0021171
Valid-->> Epoch [2752/3000], Standardized Loss: 0.0123651, Inverse Loss: 0.0025811
Training -->> Epoch: 2753,(no reg loss)standard loss: 0.0100852, inverse loss: 0.0021052
Valid-->> Epoch [2753/3000], Standardized Loss: 0.0123110, Inverse Loss: 0.0025698
Training -->> Epoch: 2754,(no reg loss)standard loss: 0.0099593, inverse loss: 0.0020789
Valid-->> Epoch [2754/3000], Standardized Loss: 0.0122821, Inverse Loss: 0.0025637
Training -->> Epoch: 2755,(no reg loss)standard loss: 0.0100308, inverse loss: 0.0020938
Valid-->> Epoch [2755/3000], Standardized Loss: 0.0124032, Inverse Loss: 0.0025890
Training -->> Epoch: 2756,(no reg loss)standard loss: 0.0100246, inverse loss: 0.0020925
Valid-->> Epoch [2756/3000], Standardized Loss: 0.0123623, Inverse Loss: 0.0025805
Training -->> Epoch: 2757,(no reg loss)standard loss: 0.0100388, inverse loss: 0.0020955
Valid-->> Epoch [2757/3000], Standardized Loss: 0.0123519, Inverse Loss: 0.0025783
Training -->> Epoch: 2758,(no reg loss)standard loss: 0.0101937, inverse loss: 0.0021278
Valid-->> Epoch [2758/3000], Standardized Loss: 0.0124020, Inverse Loss: 0.0025888
Training -->> Epoch: 2759,(no reg loss)standard loss: 0.0101055, inverse loss: 0.0021094
Valid-->> Epoch [2759/3000], Standardized Loss: 0.0122752, Inverse Loss: 0.0025623
Training -->> Epoch: 2760,(no reg loss)standard loss: 0.0100641, inverse loss: 0.0021008
Valid-->> Epoch [2760/3000], Standardized Loss: 0.0123494, Inverse Loss: 0.0025778
Training -->> Epoch: 2761,(no reg loss)standard loss: 0.0102323, inverse loss: 0.0021359
Valid-->> Epoch [2761/3000], Standardized Loss: 0.0123446, Inverse Loss: 0.0025768
Training -->> Epoch: 2762,(no reg loss)standard loss: 0.0101872, inverse loss: 0.0021265
Valid-->> Epoch [2762/3000], Standardized Loss: 0.0123630, Inverse Loss: 0.0025806
Training -->> Epoch: 2763,(no reg loss)standard loss: 0.0102294, inverse loss: 0.0021353
Valid-->> Epoch [2763/3000], Standardized Loss: 0.0123465, Inverse Loss: 0.0025772
Training -->> Epoch: 2764,(no reg loss)standard loss: 0.0102135, inverse loss: 0.0021319
Valid-->> Epoch [2764/3000], Standardized Loss: 0.0124379, Inverse Loss: 0.0025963
Training -->> Epoch: 2765,(no reg loss)standard loss: 0.0102448, inverse loss: 0.0021385
Valid-->> Epoch [2765/3000], Standardized Loss: 0.0122955, Inverse Loss: 0.0025665
Training -->> Epoch: 2766,(no reg loss)standard loss: 0.0101256, inverse loss: 0.0021136
Valid-->> Epoch [2766/3000], Standardized Loss: 0.0123957, Inverse Loss: 0.0025875
Training -->> Epoch: 2767,(no reg loss)standard loss: 0.0102148, inverse loss: 0.0021322
Valid-->> Epoch [2767/3000], Standardized Loss: 0.0122679, Inverse Loss: 0.0025608
Training -->> Epoch: 2768,(no reg loss)standard loss: 0.0098335, inverse loss: 0.0020526
Valid-->> Epoch [2768/3000], Standardized Loss: 0.0122345, Inverse Loss: 0.0025538
Training -->> Epoch: 2769,(no reg loss)standard loss: 0.0100251, inverse loss: 0.0020926
Valid-->> Epoch [2769/3000], Standardized Loss: 0.0124349, Inverse Loss: 0.0025957
Training -->> Epoch: 2770,(no reg loss)standard loss: 0.0104410, inverse loss: 0.0021794
Valid-->> Epoch [2770/3000], Standardized Loss: 0.0123786, Inverse Loss: 0.0025839
Training -->> Epoch: 2771,(no reg loss)standard loss: 0.0102483, inverse loss: 0.0021392
Valid-->> Epoch [2771/3000], Standardized Loss: 0.0122767, Inverse Loss: 0.0025626
Training -->> Epoch: 2772,(no reg loss)standard loss: 0.0099363, inverse loss: 0.0020741
Valid-->> Epoch [2772/3000], Standardized Loss: 0.0123019, Inverse Loss: 0.0025679
Training -->> Epoch: 2773,(no reg loss)standard loss: 0.0104897, inverse loss: 0.0021896
Valid-->> Epoch [2773/3000], Standardized Loss: 0.0123744, Inverse Loss: 0.0025830
Training -->> Epoch: 2774,(no reg loss)standard loss: 0.0101716, inverse loss: 0.0021232
Valid-->> Epoch [2774/3000], Standardized Loss: 0.0122839, Inverse Loss: 0.0025641
Training -->> Epoch: 2775,(no reg loss)standard loss: 0.0102971, inverse loss: 0.0021494
Valid-->> Epoch [2775/3000], Standardized Loss: 0.0124417, Inverse Loss: 0.0025971
Training -->> Epoch: 2776,(no reg loss)standard loss: 0.0104106, inverse loss: 0.0021731
Valid-->> Epoch [2776/3000], Standardized Loss: 0.0122855, Inverse Loss: 0.0025645
Training -->> Epoch: 2777,(no reg loss)standard loss: 0.0101973, inverse loss: 0.0021286
Valid-->> Epoch [2777/3000], Standardized Loss: 0.0123486, Inverse Loss: 0.0025776
Training -->> Epoch: 2778,(no reg loss)standard loss: 0.0101933, inverse loss: 0.0021277
Valid-->> Epoch [2778/3000], Standardized Loss: 0.0123206, Inverse Loss: 0.0025718
Training -->> Epoch: 2779,(no reg loss)standard loss: 0.0102509, inverse loss: 0.0021398
Valid-->> Epoch [2779/3000], Standardized Loss: 0.0124948, Inverse Loss: 0.0026081
Training -->> Epoch: 2780,(no reg loss)standard loss: 0.0104831, inverse loss: 0.0021882
Valid-->> Epoch [2780/3000], Standardized Loss: 0.0124376, Inverse Loss: 0.0025962
Training -->> Epoch: 2781,(no reg loss)standard loss: 0.0102534, inverse loss: 0.0021403
Valid-->> Epoch [2781/3000], Standardized Loss: 0.0123688, Inverse Loss: 0.0025818
Training -->> Epoch: 2782,(no reg loss)standard loss: 0.0103967, inverse loss: 0.0021702
Valid-->> Epoch [2782/3000], Standardized Loss: 0.0124176, Inverse Loss: 0.0025920
Training -->> Epoch: 2783,(no reg loss)standard loss: 0.0104931, inverse loss: 0.0021903
Valid-->> Epoch [2783/3000], Standardized Loss: 0.0123462, Inverse Loss: 0.0025771
Training -->> Epoch: 2784,(no reg loss)standard loss: 0.0102623, inverse loss: 0.0021421
Valid-->> Epoch [2784/3000], Standardized Loss: 0.0123024, Inverse Loss: 0.0025680
Training -->> Epoch: 2785,(no reg loss)standard loss: 0.0102720, inverse loss: 0.0021442
Valid-->> Epoch [2785/3000], Standardized Loss: 0.0123391, Inverse Loss: 0.0025756
Training -->> Epoch: 2786,(no reg loss)standard loss: 0.0103736, inverse loss: 0.0021654
Valid-->> Epoch [2786/3000], Standardized Loss: 0.0123525, Inverse Loss: 0.0025784
Training -->> Epoch: 2787,(no reg loss)standard loss: 0.0103579, inverse loss: 0.0021621
Valid-->> Epoch [2787/3000], Standardized Loss: 0.0124262, Inverse Loss: 0.0025938
Training -->> Epoch: 2788,(no reg loss)standard loss: 0.0107313, inverse loss: 0.0022400
Valid-->> Epoch [2788/3000], Standardized Loss: 0.0123163, Inverse Loss: 0.0025709
Training -->> Epoch: 2789,(no reg loss)standard loss: 0.0101977, inverse loss: 0.0021286
Valid-->> Epoch [2789/3000], Standardized Loss: 0.0123274, Inverse Loss: 0.0025732
Training -->> Epoch: 2790,(no reg loss)standard loss: 0.0104669, inverse loss: 0.0021849
Valid-->> Epoch [2790/3000], Standardized Loss: 0.0123048, Inverse Loss: 0.0025685
Training -->> Epoch: 2791,(no reg loss)standard loss: 0.0104387, inverse loss: 0.0021790
Valid-->> Epoch [2791/3000], Standardized Loss: 0.0122970, Inverse Loss: 0.0025669
Training -->> Epoch: 2792,(no reg loss)standard loss: 0.0104105, inverse loss: 0.0021731
Valid-->> Epoch [2792/3000], Standardized Loss: 0.0123288, Inverse Loss: 0.0025735
Training -->> Epoch: 2793,(no reg loss)standard loss: 0.0104654, inverse loss: 0.0021845
Valid-->> Epoch [2793/3000], Standardized Loss: 0.0124129, Inverse Loss: 0.0025911
Training -->> Epoch: 2794,(no reg loss)standard loss: 0.0103326, inverse loss: 0.0021568
Valid-->> Epoch [2794/3000], Standardized Loss: 0.0123531, Inverse Loss: 0.0025786
Training -->> Epoch: 2795,(no reg loss)standard loss: 0.0105061, inverse loss: 0.0021930
Valid-->> Epoch [2795/3000], Standardized Loss: 0.0123846, Inverse Loss: 0.0025852
Training -->> Epoch: 2796,(no reg loss)standard loss: 0.0102449, inverse loss: 0.0021385
Valid-->> Epoch [2796/3000], Standardized Loss: 0.0122802, Inverse Loss: 0.0025634
Training -->> Epoch: 2797,(no reg loss)standard loss: 0.0105170, inverse loss: 0.0021953
Valid-->> Epoch [2797/3000], Standardized Loss: 0.0124095, Inverse Loss: 0.0025903
Training -->> Epoch: 2798,(no reg loss)standard loss: 0.0103382, inverse loss: 0.0021580
Valid-->> Epoch [2798/3000], Standardized Loss: 0.0123860, Inverse Loss: 0.0025854
Training -->> Epoch: 2799,(no reg loss)standard loss: 0.0105881, inverse loss: 0.0022101
Valid-->> Epoch [2799/3000], Standardized Loss: 0.0122956, Inverse Loss: 0.0025666
Training -->> Epoch: 2800,(no reg loss)standard loss: 0.0106075, inverse loss: 0.0022142
Valid-->> Epoch [2800/3000], Standardized Loss: 0.0123283, Inverse Loss: 0.0025734
Training -->> Epoch: 2801,(no reg loss)standard loss: 0.0103705, inverse loss: 0.0021647
Valid-->> Epoch [2801/3000], Standardized Loss: 0.0122830, Inverse Loss: 0.0025639
Training -->> Epoch: 2802,(no reg loss)standard loss: 0.0104935, inverse loss: 0.0021904
Valid-->> Epoch [2802/3000], Standardized Loss: 0.0123739, Inverse Loss: 0.0025829
Training -->> Epoch: 2803,(no reg loss)standard loss: 0.0107985, inverse loss: 0.0022541
Valid-->> Epoch [2803/3000], Standardized Loss: 0.0124428, Inverse Loss: 0.0025973
Training -->> Epoch: 2804,(no reg loss)standard loss: 0.0102642, inverse loss: 0.0021425
Valid-->> Epoch [2804/3000], Standardized Loss: 0.0122588, Inverse Loss: 0.0025589
Training -->> Epoch: 2805,(no reg loss)standard loss: 0.0106252, inverse loss: 0.0022179
Valid-->> Epoch [2805/3000], Standardized Loss: 0.0124022, Inverse Loss: 0.0025888
Training -->> Epoch: 2806,(no reg loss)standard loss: 0.0105646, inverse loss: 0.0022052
Valid-->> Epoch [2806/3000], Standardized Loss: 0.0123825, Inverse Loss: 0.0025847
Training -->> Epoch: 2807,(no reg loss)standard loss: 0.0106489, inverse loss: 0.0022228
Valid-->> Epoch [2807/3000], Standardized Loss: 0.0123132, Inverse Loss: 0.0025702
Training -->> Epoch: 2808,(no reg loss)standard loss: 0.0104881, inverse loss: 0.0021893
Valid-->> Epoch [2808/3000], Standardized Loss: 0.0122930, Inverse Loss: 0.0025660
Training -->> Epoch: 2809,(no reg loss)standard loss: 0.0105836, inverse loss: 0.0022092
Valid-->> Epoch [2809/3000], Standardized Loss: 0.0123408, Inverse Loss: 0.0025760
Training -->> Epoch: 2810,(no reg loss)standard loss: 0.0107249, inverse loss: 0.0022387
Valid-->> Epoch [2810/3000], Standardized Loss: 0.0123987, Inverse Loss: 0.0025881
Training -->> Epoch: 2811,(no reg loss)standard loss: 0.0104193, inverse loss: 0.0021749
Valid-->> Epoch [2811/3000], Standardized Loss: 0.0123197, Inverse Loss: 0.0025716
Training -->> Epoch: 2812,(no reg loss)standard loss: 0.0105757, inverse loss: 0.0022075
Valid-->> Epoch [2812/3000], Standardized Loss: 0.0124295, Inverse Loss: 0.0025945
Training -->> Epoch: 2813,(no reg loss)standard loss: 0.0108255, inverse loss: 0.0022597
Valid-->> Epoch [2813/3000], Standardized Loss: 0.0123332, Inverse Loss: 0.0025744
Training -->> Epoch: 2814,(no reg loss)standard loss: 0.0104920, inverse loss: 0.0021901
Valid-->> Epoch [2814/3000], Standardized Loss: 0.0124241, Inverse Loss: 0.0025934
Training -->> Epoch: 2815,(no reg loss)standard loss: 0.0107684, inverse loss: 0.0022478
Valid-->> Epoch [2815/3000], Standardized Loss: 0.0123734, Inverse Loss: 0.0025828
Training -->> Epoch: 2816,(no reg loss)standard loss: 0.0106442, inverse loss: 0.0022218
Valid-->> Epoch [2816/3000], Standardized Loss: 0.0123834, Inverse Loss: 0.0025849
Training -->> Epoch: 2817,(no reg loss)standard loss: 0.0107477, inverse loss: 0.0022435
Valid-->> Epoch [2817/3000], Standardized Loss: 0.0122919, Inverse Loss: 0.0025658
Training -->> Epoch: 2818,(no reg loss)standard loss: 0.0108788, inverse loss: 0.0022708
Valid-->> Epoch [2818/3000], Standardized Loss: 0.0124352, Inverse Loss: 0.0025957
Training -->> Epoch: 2819,(no reg loss)standard loss: 0.0105469, inverse loss: 0.0022015
Valid-->> Epoch [2819/3000], Standardized Loss: 0.0122974, Inverse Loss: 0.0025670
Training -->> Epoch: 2820,(no reg loss)standard loss: 0.0107307, inverse loss: 0.0022399
Valid-->> Epoch [2820/3000], Standardized Loss: 0.0123606, Inverse Loss: 0.0025801
Training -->> Epoch: 2821,(no reg loss)standard loss: 0.0105732, inverse loss: 0.0022070
Valid-->> Epoch [2821/3000], Standardized Loss: 0.0123049, Inverse Loss: 0.0025685
Training -->> Epoch: 2822,(no reg loss)standard loss: 0.0108364, inverse loss: 0.0022620
Valid-->> Epoch [2822/3000], Standardized Loss: 0.0124146, Inverse Loss: 0.0025914
Training -->> Epoch: 2823,(no reg loss)standard loss: 0.0105750, inverse loss: 0.0022074
Valid-->> Epoch [2823/3000], Standardized Loss: 0.0122360, Inverse Loss: 0.0025541
Training -->> Epoch: 2824,(no reg loss)standard loss: 0.0104999, inverse loss: 0.0021917
Valid-->> Epoch [2824/3000], Standardized Loss: 0.0124005, Inverse Loss: 0.0025885
Training -->> Epoch: 2825,(no reg loss)standard loss: 0.0107849, inverse loss: 0.0022512
Valid-->> Epoch [2825/3000], Standardized Loss: 0.0124234, Inverse Loss: 0.0025933
Training -->> Epoch: 2826,(no reg loss)standard loss: 0.0109595, inverse loss: 0.0022877
Valid-->> Epoch [2826/3000], Standardized Loss: 0.0124160, Inverse Loss: 0.0025917
Training -->> Epoch: 2827,(no reg loss)standard loss: 0.0106597, inverse loss: 0.0022251
Valid-->> Epoch [2827/3000], Standardized Loss: 0.0122531, Inverse Loss: 0.0025577
Training -->> Epoch: 2828,(no reg loss)standard loss: 0.0105160, inverse loss: 0.0021951
Valid-->> Epoch [2828/3000], Standardized Loss: 0.0122999, Inverse Loss: 0.0025675
Training -->> Epoch: 2829,(no reg loss)standard loss: 0.0108907, inverse loss: 0.0022733
Valid-->> Epoch [2829/3000], Standardized Loss: 0.0124382, Inverse Loss: 0.0025963
Training -->> Epoch: 2830,(no reg loss)standard loss: 0.0107519, inverse loss: 0.0022443
Valid-->> Epoch [2830/3000], Standardized Loss: 0.0122911, Inverse Loss: 0.0025656
Training -->> Epoch: 2831,(no reg loss)standard loss: 0.0104621, inverse loss: 0.0021838
Valid-->> Epoch [2831/3000], Standardized Loss: 0.0122630, Inverse Loss: 0.0025598
Training -->> Epoch: 2832,(no reg loss)standard loss: 0.0110692, inverse loss: 0.0023106
Valid-->> Epoch [2832/3000], Standardized Loss: 0.0123341, Inverse Loss: 0.0025746
Training -->> Epoch: 2833,(no reg loss)standard loss: 0.0106933, inverse loss: 0.0022321
Valid-->> Epoch [2833/3000], Standardized Loss: 0.0123092, Inverse Loss: 0.0025694
Training -->> Epoch: 2834,(no reg loss)standard loss: 0.0107748, inverse loss: 0.0022491
Valid-->> Epoch [2834/3000], Standardized Loss: 0.0123340, Inverse Loss: 0.0025746
Training -->> Epoch: 2835,(no reg loss)standard loss: 0.0109113, inverse loss: 0.0022776
Valid-->> Epoch [2835/3000], Standardized Loss: 0.0124198, Inverse Loss: 0.0025925
Training -->> Epoch: 2836,(no reg loss)standard loss: 0.0108086, inverse loss: 0.0022562
Valid-->> Epoch [2836/3000], Standardized Loss: 0.0122761, Inverse Loss: 0.0025625
Training -->> Epoch: 2837,(no reg loss)standard loss: 0.0107709, inverse loss: 0.0022483
Valid-->> Epoch [2837/3000], Standardized Loss: 0.0123703, Inverse Loss: 0.0025822
Training -->> Epoch: 2838,(no reg loss)standard loss: 0.0108037, inverse loss: 0.0022551
Valid-->> Epoch [2838/3000], Standardized Loss: 0.0123260, Inverse Loss: 0.0025729
Training -->> Epoch: 2839,(no reg loss)standard loss: 0.0108059, inverse loss: 0.0022556
Valid-->> Epoch [2839/3000], Standardized Loss: 0.0123995, Inverse Loss: 0.0025883
Training -->> Epoch: 2840,(no reg loss)standard loss: 0.0108148, inverse loss: 0.0022575
Valid-->> Epoch [2840/3000], Standardized Loss: 0.0123513, Inverse Loss: 0.0025782
Training -->> Epoch: 2841,(no reg loss)standard loss: 0.0109830, inverse loss: 0.0022926
Valid-->> Epoch [2841/3000], Standardized Loss: 0.0123603, Inverse Loss: 0.0025801
Training -->> Epoch: 2842,(no reg loss)standard loss: 0.0109117, inverse loss: 0.0022777
Valid-->> Epoch [2842/3000], Standardized Loss: 0.0123089, Inverse Loss: 0.0025693
Training -->> Epoch: 2843,(no reg loss)standard loss: 0.0108149, inverse loss: 0.0022575
Valid-->> Epoch [2843/3000], Standardized Loss: 0.0122852, Inverse Loss: 0.0025644
Training -->> Epoch: 2844,(no reg loss)standard loss: 0.0108549, inverse loss: 0.0022658
Valid-->> Epoch [2844/3000], Standardized Loss: 0.0123813, Inverse Loss: 0.0025845
Training -->> Epoch: 2845,(no reg loss)standard loss: 0.0109586, inverse loss: 0.0022875
Valid-->> Epoch [2845/3000], Standardized Loss: 0.0123278, Inverse Loss: 0.0025733
Training -->> Epoch: 2846,(no reg loss)standard loss: 0.0110714, inverse loss: 0.0023110
Valid-->> Epoch [2846/3000], Standardized Loss: 0.0123003, Inverse Loss: 0.0025675
Training -->> Epoch: 2847,(no reg loss)standard loss: 0.0107692, inverse loss: 0.0022479
Valid-->> Epoch [2847/3000], Standardized Loss: 0.0123000, Inverse Loss: 0.0025675
Training -->> Epoch: 2848,(no reg loss)standard loss: 0.0109751, inverse loss: 0.0022909
Valid-->> Epoch [2848/3000], Standardized Loss: 0.0122287, Inverse Loss: 0.0025526
Training -->> Epoch: 2849,(no reg loss)standard loss: 0.0110634, inverse loss: 0.0023094
Valid-->> Epoch [2849/3000], Standardized Loss: 0.0124175, Inverse Loss: 0.0025920
Training -->> Epoch: 2850,(no reg loss)standard loss: 0.0107027, inverse loss: 0.0022341
Valid-->> Epoch [2850/3000], Standardized Loss: 0.0122373, Inverse Loss: 0.0025544
Training -->> Epoch: 2851,(no reg loss)standard loss: 0.0112276, inverse loss: 0.0023436
Valid-->> Epoch [2851/3000], Standardized Loss: 0.0122780, Inverse Loss: 0.0025629
Training -->> Epoch: 2852,(no reg loss)standard loss: 0.0109170, inverse loss: 0.0022788
Valid-->> Epoch [2852/3000], Standardized Loss: 0.0123170, Inverse Loss: 0.0025710
Training -->> Epoch: 2853,(no reg loss)standard loss: 0.0109408, inverse loss: 0.0022838
Valid-->> Epoch [2853/3000], Standardized Loss: 0.0123637, Inverse Loss: 0.0025808
Training -->> Epoch: 2854,(no reg loss)standard loss: 0.0109637, inverse loss: 0.0022885
Valid-->> Epoch [2854/3000], Standardized Loss: 0.0123626, Inverse Loss: 0.0025806
Training -->> Epoch: 2855,(no reg loss)standard loss: 0.0110219, inverse loss: 0.0023007
Valid-->> Epoch [2855/3000], Standardized Loss: 0.0123265, Inverse Loss: 0.0025730
Training -->> Epoch: 2856,(no reg loss)standard loss: 0.0108877, inverse loss: 0.0022727
Valid-->> Epoch [2856/3000], Standardized Loss: 0.0124043, Inverse Loss: 0.0025893
Training -->> Epoch: 2857,(no reg loss)standard loss: 0.0113479, inverse loss: 0.0023687
Valid-->> Epoch [2857/3000], Standardized Loss: 0.0123726, Inverse Loss: 0.0025826
Training -->> Epoch: 2858,(no reg loss)standard loss: 0.0109600, inverse loss: 0.0022878
Valid-->> Epoch [2858/3000], Standardized Loss: 0.0123045, Inverse Loss: 0.0025684
Training -->> Epoch: 2859,(no reg loss)standard loss: 0.0109990, inverse loss: 0.0022959
Valid-->> Epoch [2859/3000], Standardized Loss: 0.0123188, Inverse Loss: 0.0025714
Training -->> Epoch: 2860,(no reg loss)standard loss: 0.0112420, inverse loss: 0.0023466
Valid-->> Epoch [2860/3000], Standardized Loss: 0.0123485, Inverse Loss: 0.0025776
Training -->> Epoch: 2861,(no reg loss)standard loss: 0.0108667, inverse loss: 0.0022683
Valid-->> Epoch [2861/3000], Standardized Loss: 0.0122394, Inverse Loss: 0.0025548
Training -->> Epoch: 2862,(no reg loss)standard loss: 0.0109574, inverse loss: 0.0022872
Valid-->> Epoch [2862/3000], Standardized Loss: 0.0123048, Inverse Loss: 0.0025685
Training -->> Epoch: 2863,(no reg loss)standard loss: 0.0110366, inverse loss: 0.0023038
Valid-->> Epoch [2863/3000], Standardized Loss: 0.0124106, Inverse Loss: 0.0025906
Training -->> Epoch: 2864,(no reg loss)standard loss: 0.0110338, inverse loss: 0.0023032
Valid-->> Epoch [2864/3000], Standardized Loss: 0.0122813, Inverse Loss: 0.0025636
Training -->> Epoch: 2865,(no reg loss)standard loss: 0.0111049, inverse loss: 0.0023180
Valid-->> Epoch [2865/3000], Standardized Loss: 0.0124024, Inverse Loss: 0.0025889
Training -->> Epoch: 2866,(no reg loss)standard loss: 0.0110744, inverse loss: 0.0023116
Valid-->> Epoch [2866/3000], Standardized Loss: 0.0124157, Inverse Loss: 0.0025916
Training -->> Epoch: 2867,(no reg loss)standard loss: 0.0111698, inverse loss: 0.0023316
Valid-->> Epoch [2867/3000], Standardized Loss: 0.0123896, Inverse Loss: 0.0025862
Training -->> Epoch: 2868,(no reg loss)standard loss: 0.0107602, inverse loss: 0.0022461
Valid-->> Epoch [2868/3000], Standardized Loss: 0.0121826, Inverse Loss: 0.0025430
Valid-->> Lowest loss found at epoch 2868, loss: 0.0025430
Epoch 2868, Masked params (inverse standardized): tensor([3.230221e+01, 4.430546e+01, 6.722641e-02, 2.642614e+01, 2.774574e+01,
        1.781179e+01, 2.144085e+01, 1.633162e+00, 1.066165e+02, 2.762215e+01,
        2.621675e+01, 4.461500e+01, 2.445592e+01, 1.504107e+01, 8.461694e+01,
        2.838589e+01, 5.670176e+00, 2.984084e+01, 1.196944e+01, 3.204924e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 2869,(no reg loss)standard loss: 0.0112814, inverse loss: 0.0023549
Valid-->> Epoch [2869/3000], Standardized Loss: 0.0125746, Inverse Loss: 0.0026248
Training -->> Epoch: 2870,(no reg loss)standard loss: 0.0113276, inverse loss: 0.0023645
Valid-->> Epoch [2870/3000], Standardized Loss: 0.0122822, Inverse Loss: 0.0025638
Training -->> Epoch: 2871,(no reg loss)standard loss: 0.0109333, inverse loss: 0.0022822
Valid-->> Epoch [2871/3000], Standardized Loss: 0.0122228, Inverse Loss: 0.0025514
Training -->> Epoch: 2872,(no reg loss)standard loss: 0.0107936, inverse loss: 0.0022530
Valid-->> Epoch [2872/3000], Standardized Loss: 0.0122144, Inverse Loss: 0.0025496
Training -->> Epoch: 2873,(no reg loss)standard loss: 0.0111727, inverse loss: 0.0023322
Valid-->> Epoch [2873/3000], Standardized Loss: 0.0123577, Inverse Loss: 0.0025795
Training -->> Epoch: 2874,(no reg loss)standard loss: 0.0112091, inverse loss: 0.0023398
Valid-->> Epoch [2874/3000], Standardized Loss: 0.0123724, Inverse Loss: 0.0025826
Training -->> Epoch: 2875,(no reg loss)standard loss: 0.0112458, inverse loss: 0.0023474
Valid-->> Epoch [2875/3000], Standardized Loss: 0.0123218, Inverse Loss: 0.0025720
Training -->> Epoch: 2876,(no reg loss)standard loss: 0.0110253, inverse loss: 0.0023014
Valid-->> Epoch [2876/3000], Standardized Loss: 0.0123075, Inverse Loss: 0.0025690
Training -->> Epoch: 2877,(no reg loss)standard loss: 0.0111198, inverse loss: 0.0023211
Valid-->> Epoch [2877/3000], Standardized Loss: 0.0123906, Inverse Loss: 0.0025864
Training -->> Epoch: 2878,(no reg loss)standard loss: 0.0113903, inverse loss: 0.0023776
Valid-->> Epoch [2878/3000], Standardized Loss: 0.0123170, Inverse Loss: 0.0025710
Training -->> Epoch: 2879,(no reg loss)standard loss: 0.0112385, inverse loss: 0.0023459
Valid-->> Epoch [2879/3000], Standardized Loss: 0.0123442, Inverse Loss: 0.0025767
Training -->> Epoch: 2880,(no reg loss)standard loss: 0.0111923, inverse loss: 0.0023363
Valid-->> Epoch [2880/3000], Standardized Loss: 0.0123159, Inverse Loss: 0.0025708
Training -->> Epoch: 2881,(no reg loss)standard loss: 0.0115115, inverse loss: 0.0024029
Valid-->> Epoch [2881/3000], Standardized Loss: 0.0124340, Inverse Loss: 0.0025954
Training -->> Epoch: 2882,(no reg loss)standard loss: 0.0112857, inverse loss: 0.0023558
Valid-->> Epoch [2882/3000], Standardized Loss: 0.0123664, Inverse Loss: 0.0025813
Training -->> Epoch: 2883,(no reg loss)standard loss: 0.0112427, inverse loss: 0.0023468
Valid-->> Epoch [2883/3000], Standardized Loss: 0.0122865, Inverse Loss: 0.0025647
Training -->> Epoch: 2884,(no reg loss)standard loss: 0.0112349, inverse loss: 0.0023452
Valid-->> Epoch [2884/3000], Standardized Loss: 0.0123937, Inverse Loss: 0.0025870
Training -->> Epoch: 2885,(no reg loss)standard loss: 0.0113926, inverse loss: 0.0023781
Valid-->> Epoch [2885/3000], Standardized Loss: 0.0122477, Inverse Loss: 0.0025566
Training -->> Epoch: 2886,(no reg loss)standard loss: 0.0112246, inverse loss: 0.0023430
Valid-->> Epoch [2886/3000], Standardized Loss: 0.0122853, Inverse Loss: 0.0025644
Training -->> Epoch: 2887,(no reg loss)standard loss: 0.0113205, inverse loss: 0.0023630
Valid-->> Epoch [2887/3000], Standardized Loss: 0.0123406, Inverse Loss: 0.0025760
Training -->> Epoch: 2888,(no reg loss)standard loss: 0.0111799, inverse loss: 0.0023337
Valid-->> Epoch [2888/3000], Standardized Loss: 0.0123052, Inverse Loss: 0.0025686
Training -->> Epoch: 2889,(no reg loss)standard loss: 0.0112852, inverse loss: 0.0023556
Valid-->> Epoch [2889/3000], Standardized Loss: 0.0125408, Inverse Loss: 0.0026177
Training -->> Epoch: 2890,(no reg loss)standard loss: 0.0117423, inverse loss: 0.0024511
Valid-->> Epoch [2890/3000], Standardized Loss: 0.0122855, Inverse Loss: 0.0025645
Training -->> Epoch: 2891,(no reg loss)standard loss: 0.0114913, inverse loss: 0.0023987
Valid-->> Epoch [2891/3000], Standardized Loss: 0.0123411, Inverse Loss: 0.0025761
Training -->> Epoch: 2892,(no reg loss)standard loss: 0.0112877, inverse loss: 0.0023562
Valid-->> Epoch [2892/3000], Standardized Loss: 0.0123024, Inverse Loss: 0.0025680
Training -->> Epoch: 2893,(no reg loss)standard loss: 0.0112973, inverse loss: 0.0023582
Valid-->> Epoch [2893/3000], Standardized Loss: 0.0123955, Inverse Loss: 0.0025874
Training -->> Epoch: 2894,(no reg loss)standard loss: 0.0116230, inverse loss: 0.0024262
Valid-->> Epoch [2894/3000], Standardized Loss: 0.0122636, Inverse Loss: 0.0025599
Training -->> Epoch: 2895,(no reg loss)standard loss: 0.0112795, inverse loss: 0.0023545
Valid-->> Epoch [2895/3000], Standardized Loss: 0.0123483, Inverse Loss: 0.0025776
Training -->> Epoch: 2896,(no reg loss)standard loss: 0.0114961, inverse loss: 0.0023997
Valid-->> Epoch [2896/3000], Standardized Loss: 0.0122827, Inverse Loss: 0.0025639
Training -->> Epoch: 2897,(no reg loss)standard loss: 0.0114884, inverse loss: 0.0023981
Valid-->> Epoch [2897/3000], Standardized Loss: 0.0123212, Inverse Loss: 0.0025719
Training -->> Epoch: 2898,(no reg loss)standard loss: 0.0113293, inverse loss: 0.0023649
Valid-->> Epoch [2898/3000], Standardized Loss: 0.0122704, Inverse Loss: 0.0025613
Training -->> Epoch: 2899,(no reg loss)standard loss: 0.0114107, inverse loss: 0.0023819
Valid-->> Epoch [2899/3000], Standardized Loss: 0.0123441, Inverse Loss: 0.0025767
Training -->> Epoch: 2900,(no reg loss)standard loss: 0.0115959, inverse loss: 0.0024205
Valid-->> Epoch [2900/3000], Standardized Loss: 0.0124021, Inverse Loss: 0.0025888
Training -->> Epoch: 2901,(no reg loss)standard loss: 0.0112527, inverse loss: 0.0023489
Valid-->> Epoch [2901/3000], Standardized Loss: 0.0122541, Inverse Loss: 0.0025579
Training -->> Epoch: 2902,(no reg loss)standard loss: 0.0117089, inverse loss: 0.0024441
Valid-->> Epoch [2902/3000], Standardized Loss: 0.0125047, Inverse Loss: 0.0026102
Training -->> Epoch: 2903,(no reg loss)standard loss: 0.0114743, inverse loss: 0.0023951
Valid-->> Epoch [2903/3000], Standardized Loss: 0.0122897, Inverse Loss: 0.0025653
Training -->> Epoch: 2904,(no reg loss)standard loss: 0.0114039, inverse loss: 0.0023804
Valid-->> Epoch [2904/3000], Standardized Loss: 0.0123472, Inverse Loss: 0.0025773
Training -->> Epoch: 2905,(no reg loss)standard loss: 0.0114073, inverse loss: 0.0023811
Valid-->> Epoch [2905/3000], Standardized Loss: 0.0123671, Inverse Loss: 0.0025815
Training -->> Epoch: 2906,(no reg loss)standard loss: 0.0118432, inverse loss: 0.0024721
Valid-->> Epoch [2906/3000], Standardized Loss: 0.0123921, Inverse Loss: 0.0025867
Training -->> Epoch: 2907,(no reg loss)standard loss: 0.0113537, inverse loss: 0.0023700
Valid-->> Epoch [2907/3000], Standardized Loss: 0.0122297, Inverse Loss: 0.0025528
Training -->> Epoch: 2908,(no reg loss)standard loss: 0.0115139, inverse loss: 0.0024034
Valid-->> Epoch [2908/3000], Standardized Loss: 0.0122716, Inverse Loss: 0.0025615
Training -->> Epoch: 2909,(no reg loss)standard loss: 0.0114224, inverse loss: 0.0023843
Valid-->> Epoch [2909/3000], Standardized Loss: 0.0122795, Inverse Loss: 0.0025632
Training -->> Epoch: 2910,(no reg loss)standard loss: 0.0116370, inverse loss: 0.0024291
Valid-->> Epoch [2910/3000], Standardized Loss: 0.0124568, Inverse Loss: 0.0026002
Training -->> Epoch: 2911,(no reg loss)standard loss: 0.0114482, inverse loss: 0.0023897
Valid-->> Epoch [2911/3000], Standardized Loss: 0.0122704, Inverse Loss: 0.0025613
Training -->> Epoch: 2912,(no reg loss)standard loss: 0.0116348, inverse loss: 0.0024286
Valid-->> Epoch [2912/3000], Standardized Loss: 0.0122702, Inverse Loss: 0.0025613
Training -->> Epoch: 2913,(no reg loss)standard loss: 0.0113085, inverse loss: 0.0023605
Valid-->> Epoch [2913/3000], Standardized Loss: 0.0122336, Inverse Loss: 0.0025536
Training -->> Epoch: 2914,(no reg loss)standard loss: 0.0114585, inverse loss: 0.0023918
Valid-->> Epoch [2914/3000], Standardized Loss: 0.0124063, Inverse Loss: 0.0025897
Training -->> Epoch: 2915,(no reg loss)standard loss: 0.0116880, inverse loss: 0.0024397
Valid-->> Epoch [2915/3000], Standardized Loss: 0.0123191, Inverse Loss: 0.0025715
Training -->> Epoch: 2916,(no reg loss)standard loss: 0.0115108, inverse loss: 0.0024028
Valid-->> Epoch [2916/3000], Standardized Loss: 0.0123500, Inverse Loss: 0.0025779
Training -->> Epoch: 2917,(no reg loss)standard loss: 0.0116883, inverse loss: 0.0024398
Valid-->> Epoch [2917/3000], Standardized Loss: 0.0122926, Inverse Loss: 0.0025659
Training -->> Epoch: 2918,(no reg loss)standard loss: 0.0114951, inverse loss: 0.0023995
Valid-->> Epoch [2918/3000], Standardized Loss: 0.0124425, Inverse Loss: 0.0025972
Training -->> Epoch: 2919,(no reg loss)standard loss: 0.0118527, inverse loss: 0.0024741
Valid-->> Epoch [2919/3000], Standardized Loss: 0.0123136, Inverse Loss: 0.0025703
Training -->> Epoch: 2920,(no reg loss)standard loss: 0.0116091, inverse loss: 0.0024233
Valid-->> Epoch [2920/3000], Standardized Loss: 0.0122912, Inverse Loss: 0.0025656
Training -->> Epoch: 2921,(no reg loss)standard loss: 0.0114741, inverse loss: 0.0023951
Valid-->> Epoch [2921/3000], Standardized Loss: 0.0122360, Inverse Loss: 0.0025541
Training -->> Epoch: 2922,(no reg loss)standard loss: 0.0116484, inverse loss: 0.0024315
Valid-->> Epoch [2922/3000], Standardized Loss: 0.0123122, Inverse Loss: 0.0025700
Training -->> Epoch: 2923,(no reg loss)standard loss: 0.0117445, inverse loss: 0.0024515
Valid-->> Epoch [2923/3000], Standardized Loss: 0.0123043, Inverse Loss: 0.0025684
Training -->> Epoch: 2924,(no reg loss)standard loss: 0.0115808, inverse loss: 0.0024174
Valid-->> Epoch [2924/3000], Standardized Loss: 0.0123958, Inverse Loss: 0.0025875
Training -->> Epoch: 2925,(no reg loss)standard loss: 0.0121002, inverse loss: 0.0025258
Valid-->> Epoch [2925/3000], Standardized Loss: 0.0122681, Inverse Loss: 0.0025608
Training -->> Epoch: 2926,(no reg loss)standard loss: 0.0116811, inverse loss: 0.0024383
Valid-->> Epoch [2926/3000], Standardized Loss: 0.0124151, Inverse Loss: 0.0025915
Training -->> Epoch: 2927,(no reg loss)standard loss: 0.0116932, inverse loss: 0.0024408
Valid-->> Epoch [2927/3000], Standardized Loss: 0.0122428, Inverse Loss: 0.0025555
Training -->> Epoch: 2928,(no reg loss)standard loss: 0.0115338, inverse loss: 0.0024076
Valid-->> Epoch [2928/3000], Standardized Loss: 0.0123009, Inverse Loss: 0.0025677
Training -->> Epoch: 2929,(no reg loss)standard loss: 0.0116253, inverse loss: 0.0024266
Valid-->> Epoch [2929/3000], Standardized Loss: 0.0123915, Inverse Loss: 0.0025866
Training -->> Epoch: 2930,(no reg loss)standard loss: 0.0118723, inverse loss: 0.0024782
Valid-->> Epoch [2930/3000], Standardized Loss: 0.0123604, Inverse Loss: 0.0025801
Training -->> Epoch: 2931,(no reg loss)standard loss: 0.0117581, inverse loss: 0.0024544
Valid-->> Epoch [2931/3000], Standardized Loss: 0.0122481, Inverse Loss: 0.0025566
Training -->> Epoch: 2932,(no reg loss)standard loss: 0.0117811, inverse loss: 0.0024592
Valid-->> Epoch [2932/3000], Standardized Loss: 0.0123078, Inverse Loss: 0.0025691
Training -->> Epoch: 2933,(no reg loss)standard loss: 0.0116285, inverse loss: 0.0024273
Valid-->> Epoch [2933/3000], Standardized Loss: 0.0123545, Inverse Loss: 0.0025789
Training -->> Epoch: 2934,(no reg loss)standard loss: 0.0117321, inverse loss: 0.0024489
Valid-->> Epoch [2934/3000], Standardized Loss: 0.0124021, Inverse Loss: 0.0025888
Training -->> Epoch: 2935,(no reg loss)standard loss: 0.0118772, inverse loss: 0.0024792
Valid-->> Epoch [2935/3000], Standardized Loss: 0.0123901, Inverse Loss: 0.0025863
Training -->> Epoch: 2936,(no reg loss)standard loss: 0.0118339, inverse loss: 0.0024702
Valid-->> Epoch [2936/3000], Standardized Loss: 0.0122931, Inverse Loss: 0.0025660
Training -->> Epoch: 2937,(no reg loss)standard loss: 0.0117322, inverse loss: 0.0024490
Valid-->> Epoch [2937/3000], Standardized Loss: 0.0123644, Inverse Loss: 0.0025809
Training -->> Epoch: 2938,(no reg loss)standard loss: 0.0119930, inverse loss: 0.0025034
Valid-->> Epoch [2938/3000], Standardized Loss: 0.0124165, Inverse Loss: 0.0025918
Training -->> Epoch: 2939,(no reg loss)standard loss: 0.0116517, inverse loss: 0.0024322
Valid-->> Epoch [2939/3000], Standardized Loss: 0.0122457, Inverse Loss: 0.0025562
Training -->> Epoch: 2940,(no reg loss)standard loss: 0.0120957, inverse loss: 0.0025248
Valid-->> Epoch [2940/3000], Standardized Loss: 0.0123037, Inverse Loss: 0.0025683
Training -->> Epoch: 2941,(no reg loss)standard loss: 0.0117949, inverse loss: 0.0024621
Valid-->> Epoch [2941/3000], Standardized Loss: 0.0122392, Inverse Loss: 0.0025548
Training -->> Epoch: 2942,(no reg loss)standard loss: 0.0118162, inverse loss: 0.0024665
Valid-->> Epoch [2942/3000], Standardized Loss: 0.0124408, Inverse Loss: 0.0025969
Training -->> Epoch: 2943,(no reg loss)standard loss: 0.0117999, inverse loss: 0.0024631
Valid-->> Epoch [2943/3000], Standardized Loss: 0.0123502, Inverse Loss: 0.0025780
Training -->> Epoch: 2944,(no reg loss)standard loss: 0.0121710, inverse loss: 0.0025406
Valid-->> Epoch [2944/3000], Standardized Loss: 0.0122739, Inverse Loss: 0.0025620
Training -->> Epoch: 2945,(no reg loss)standard loss: 0.0116689, inverse loss: 0.0024358
Valid-->> Epoch [2945/3000], Standardized Loss: 0.0122940, Inverse Loss: 0.0025662
Training -->> Epoch: 2946,(no reg loss)standard loss: 0.0119809, inverse loss: 0.0025009
Valid-->> Epoch [2946/3000], Standardized Loss: 0.0123041, Inverse Loss: 0.0025683
Training -->> Epoch: 2947,(no reg loss)standard loss: 0.0115992, inverse loss: 0.0024212
Valid-->> Epoch [2947/3000], Standardized Loss: 0.0121750, Inverse Loss: 0.0025414
Valid-->> Lowest loss found at epoch 2947, loss: 0.0025414
Epoch 2947, Masked params (inverse standardized): tensor([3.230323e+01, 4.430465e+01, 6.874466e-02, 2.642873e+01, 2.774780e+01,
        1.781377e+01, 2.144199e+01, 1.634768e+00, 1.066154e+02, 2.762463e+01,
        2.621903e+01, 4.461675e+01, 2.445689e+01, 1.504304e+01, 8.461938e+01,
        2.838720e+01, 5.671244e+00, 2.984173e+01, 1.197141e+01, 3.205020e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 2948,(no reg loss)standard loss: 0.0118232, inverse loss: 0.0024679
Valid-->> Epoch [2948/3000], Standardized Loss: 0.0123894, Inverse Loss: 0.0025861
Training -->> Epoch: 2949,(no reg loss)standard loss: 0.0119390, inverse loss: 0.0024921
Valid-->> Epoch [2949/3000], Standardized Loss: 0.0123201, Inverse Loss: 0.0025717
Training -->> Epoch: 2950,(no reg loss)standard loss: 0.0118527, inverse loss: 0.0024741
Valid-->> Epoch [2950/3000], Standardized Loss: 0.0123182, Inverse Loss: 0.0025713
Training -->> Epoch: 2951,(no reg loss)standard loss: 0.0117998, inverse loss: 0.0024631
Valid-->> Epoch [2951/3000], Standardized Loss: 0.0122544, Inverse Loss: 0.0025580
Training -->> Epoch: 2952,(no reg loss)standard loss: 0.0117892, inverse loss: 0.0024609
Valid-->> Epoch [2952/3000], Standardized Loss: 0.0123457, Inverse Loss: 0.0025770
Training -->> Epoch: 2953,(no reg loss)standard loss: 0.0122527, inverse loss: 0.0025576
Valid-->> Epoch [2953/3000], Standardized Loss: 0.0124688, Inverse Loss: 0.0026027
Training -->> Epoch: 2954,(no reg loss)standard loss: 0.0116475, inverse loss: 0.0024313
Valid-->> Epoch [2954/3000], Standardized Loss: 0.0121615, Inverse Loss: 0.0025386
Valid-->> Lowest loss found at epoch 2954, loss: 0.0025386
Epoch 2954, Masked params (inverse standardized): tensor([3.230514e+01, 4.430330e+01, 6.732368e-02, 2.642958e+01, 2.774806e+01,
        1.781120e+01, 2.144300e+01, 1.633005e+00, 1.066139e+02, 2.762479e+01,
        2.622026e+01, 4.461565e+01, 2.445808e+01, 1.504284e+01, 8.461516e+01,
        2.838735e+01, 5.671671e+00, 2.984442e+01, 1.197018e+01, 3.205148e+01],
       device='cuda:0', grad_fn=<AddBackward0>)
Training -->> Epoch: 2955,(no reg loss)standard loss: 0.0118410, inverse loss: 0.0024717
Valid-->> Epoch [2955/3000], Standardized Loss: 0.0123931, Inverse Loss: 0.0025869
Training -->> Epoch: 2956,(no reg loss)standard loss: 0.0119699, inverse loss: 0.0024986
Valid-->> Epoch [2956/3000], Standardized Loss: 0.0123056, Inverse Loss: 0.0025686
Training -->> Epoch: 2957,(no reg loss)standard loss: 0.0120165, inverse loss: 0.0025083
Valid-->> Epoch [2957/3000], Standardized Loss: 0.0124766, Inverse Loss: 0.0026044
Training -->> Epoch: 2958,(no reg loss)standard loss: 0.0121918, inverse loss: 0.0025449
Valid-->> Epoch [2958/3000], Standardized Loss: 0.0123214, Inverse Loss: 0.0025720
Training -->> Epoch: 2959,(no reg loss)standard loss: 0.0119709, inverse loss: 0.0024988
Valid-->> Epoch [2959/3000], Standardized Loss: 0.0123977, Inverse Loss: 0.0025879
Training -->> Epoch: 2960,(no reg loss)standard loss: 0.0120147, inverse loss: 0.0025079
Valid-->> Epoch [2960/3000], Standardized Loss: 0.0123136, Inverse Loss: 0.0025703
Training -->> Epoch: 2961,(no reg loss)standard loss: 0.0120546, inverse loss: 0.0025163
Valid-->> Epoch [2961/3000], Standardized Loss: 0.0124538, Inverse Loss: 0.0025996
Training -->> Epoch: 2962,(no reg loss)standard loss: 0.0121328, inverse loss: 0.0025326
Valid-->> Epoch [2962/3000], Standardized Loss: 0.0123624, Inverse Loss: 0.0025805
Training -->> Epoch: 2963,(no reg loss)standard loss: 0.0120749, inverse loss: 0.0025205
Valid-->> Epoch [2963/3000], Standardized Loss: 0.0123796, Inverse Loss: 0.0025841
Training -->> Epoch: 2964,(no reg loss)standard loss: 0.0121521, inverse loss: 0.0025366
Valid-->> Epoch [2964/3000], Standardized Loss: 0.0123962, Inverse Loss: 0.0025876
Training -->> Epoch: 2965,(no reg loss)standard loss: 0.0119250, inverse loss: 0.0024892
Valid-->> Epoch [2965/3000], Standardized Loss: 0.0122872, Inverse Loss: 0.0025648
Training -->> Epoch: 2966,(no reg loss)standard loss: 0.0120666, inverse loss: 0.0025188
Valid-->> Epoch [2966/3000], Standardized Loss: 0.0124335, Inverse Loss: 0.0025954
Training -->> Epoch: 2967,(no reg loss)standard loss: 0.0122350, inverse loss: 0.0025539
Valid-->> Epoch [2967/3000], Standardized Loss: 0.0123377, Inverse Loss: 0.0025754
Training -->> Epoch: 2968,(no reg loss)standard loss: 0.0120302, inverse loss: 0.0025112
Valid-->> Epoch [2968/3000], Standardized Loss: 0.0123662, Inverse Loss: 0.0025813
Training -->> Epoch: 2969,(no reg loss)standard loss: 0.0122148, inverse loss: 0.0025497
Valid-->> Epoch [2969/3000], Standardized Loss: 0.0123150, Inverse Loss: 0.0025706
Training -->> Epoch: 2970,(no reg loss)standard loss: 0.0120452, inverse loss: 0.0025143
Valid-->> Epoch [2970/3000], Standardized Loss: 0.0125086, Inverse Loss: 0.0026110
Training -->> Epoch: 2971,(no reg loss)standard loss: 0.0123019, inverse loss: 0.0025679
Valid-->> Epoch [2971/3000], Standardized Loss: 0.0122840, Inverse Loss: 0.0025642
Training -->> Epoch: 2972,(no reg loss)standard loss: 0.0122443, inverse loss: 0.0025558
Valid-->> Epoch [2972/3000], Standardized Loss: 0.0124228, Inverse Loss: 0.0025931
Training -->> Epoch: 2973,(no reg loss)standard loss: 0.0122087, inverse loss: 0.0025484
Valid-->> Epoch [2973/3000], Standardized Loss: 0.0123329, Inverse Loss: 0.0025744
Training -->> Epoch: 2974,(no reg loss)standard loss: 0.0120915, inverse loss: 0.0025240
Valid-->> Epoch [2974/3000], Standardized Loss: 0.0123336, Inverse Loss: 0.0025745
Training -->> Epoch: 2975,(no reg loss)standard loss: 0.0121245, inverse loss: 0.0025308
Valid-->> Epoch [2975/3000], Standardized Loss: 0.0123280, Inverse Loss: 0.0025733
Training -->> Epoch: 2976,(no reg loss)standard loss: 0.0124100, inverse loss: 0.0025904
Valid-->> Epoch [2976/3000], Standardized Loss: 0.0123255, Inverse Loss: 0.0025728
Training -->> Epoch: 2977,(no reg loss)standard loss: 0.0122482, inverse loss: 0.0025567
Valid-->> Epoch [2977/3000], Standardized Loss: 0.0122610, Inverse Loss: 0.0025594
Training -->> Epoch: 2978,(no reg loss)standard loss: 0.0119235, inverse loss: 0.0024889
Valid-->> Epoch [2978/3000], Standardized Loss: 0.0123104, Inverse Loss: 0.0025697
Training -->> Epoch: 2979,(no reg loss)standard loss: 0.0124238, inverse loss: 0.0025933
Valid-->> Epoch [2979/3000], Standardized Loss: 0.0123988, Inverse Loss: 0.0025881
Training -->> Epoch: 2980,(no reg loss)standard loss: 0.0122637, inverse loss: 0.0025599
Valid-->> Epoch [2980/3000], Standardized Loss: 0.0124019, Inverse Loss: 0.0025887
Training -->> Epoch: 2981,(no reg loss)standard loss: 0.0122439, inverse loss: 0.0025558
Valid-->> Epoch [2981/3000], Standardized Loss: 0.0122909, Inverse Loss: 0.0025656
Training -->> Epoch: 2982,(no reg loss)standard loss: 0.0120431, inverse loss: 0.0025139
Valid-->> Epoch [2982/3000], Standardized Loss: 0.0122895, Inverse Loss: 0.0025653
Training -->> Epoch: 2983,(no reg loss)standard loss: 0.0122806, inverse loss: 0.0025634
Valid-->> Epoch [2983/3000], Standardized Loss: 0.0123421, Inverse Loss: 0.0025763
Training -->> Epoch: 2984,(no reg loss)standard loss: 0.0124383, inverse loss: 0.0025964
Valid-->> Epoch [2984/3000], Standardized Loss: 0.0124777, Inverse Loss: 0.0026046
Training -->> Epoch: 2985,(no reg loss)standard loss: 0.0124623, inverse loss: 0.0026014
Valid-->> Epoch [2985/3000], Standardized Loss: 0.0122277, Inverse Loss: 0.0025524
Training -->> Epoch: 2986,(no reg loss)standard loss: 0.0119317, inverse loss: 0.0024906
Valid-->> Epoch [2986/3000], Standardized Loss: 0.0123314, Inverse Loss: 0.0025740
Training -->> Epoch: 2987,(no reg loss)standard loss: 0.0126608, inverse loss: 0.0026428
Valid-->> Epoch [2987/3000], Standardized Loss: 0.0124245, Inverse Loss: 0.0025935
Training -->> Epoch: 2988,(no reg loss)standard loss: 0.0125224, inverse loss: 0.0026139
Valid-->> Epoch [2988/3000], Standardized Loss: 0.0123360, Inverse Loss: 0.0025750
Training -->> Epoch: 2989,(no reg loss)standard loss: 0.0120621, inverse loss: 0.0025178
Valid-->> Epoch [2989/3000], Standardized Loss: 0.0122174, Inverse Loss: 0.0025502
Training -->> Epoch: 2990,(no reg loss)standard loss: 0.0121439, inverse loss: 0.0025349
Valid-->> Epoch [2990/3000], Standardized Loss: 0.0122476, Inverse Loss: 0.0025565
Training -->> Epoch: 2991,(no reg loss)standard loss: 0.0122725, inverse loss: 0.0025618
Valid-->> Epoch [2991/3000], Standardized Loss: 0.0123863, Inverse Loss: 0.0025855
Training -->> Epoch: 2992,(no reg loss)standard loss: 0.0123459, inverse loss: 0.0025771
Valid-->> Epoch [2992/3000], Standardized Loss: 0.0122934, Inverse Loss: 0.0025661
Training -->> Epoch: 2993,(no reg loss)standard loss: 0.0124784, inverse loss: 0.0026047
Valid-->> Epoch [2993/3000], Standardized Loss: 0.0122754, Inverse Loss: 0.0025623
Training -->> Epoch: 2994,(no reg loss)standard loss: 0.0122883, inverse loss: 0.0025650
Valid-->> Epoch [2994/3000], Standardized Loss: 0.0123374, Inverse Loss: 0.0025753
Training -->> Epoch: 2995,(no reg loss)standard loss: 0.0124932, inverse loss: 0.0026078
Valid-->> Epoch [2995/3000], Standardized Loss: 0.0123538, Inverse Loss: 0.0025787
Training -->> Epoch: 2996,(no reg loss)standard loss: 0.0120642, inverse loss: 0.0025183
Valid-->> Epoch [2996/3000], Standardized Loss: 0.0121713, Inverse Loss: 0.0025406
Training -->> Epoch: 2997,(no reg loss)standard loss: 0.0126876, inverse loss: 0.0026484
Valid-->> Epoch [2997/3000], Standardized Loss: 0.0123357, Inverse Loss: 0.0025749
Training -->> Epoch: 2998,(no reg loss)standard loss: 0.0121984, inverse loss: 0.0025463
Valid-->> Epoch [2998/3000], Standardized Loss: 0.0123107, Inverse Loss: 0.0025697
Training -->> Epoch: 2999,(no reg loss)standard loss: 0.0127502, inverse loss: 0.0026615
Valid-->> Epoch [2999/3000], Standardized Loss: 0.0124245, Inverse Loss: 0.0025935
Training -->> Epoch: 3000,(no reg loss)standard loss: 0.0124035, inverse loss: 0.0025891
Valid-->> Epoch [3000/3000], Standardized Loss: 0.0122664, Inverse Loss: 0.0025605
Total prediction time: 491.19 seconds

 0
